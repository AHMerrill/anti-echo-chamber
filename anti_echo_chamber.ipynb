{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb0d9c13-7874-47ea-8c6e-c2003d5c6762",
   "metadata": {},
   "source": [
    "# Anti Echo Chamber\n",
    "\n",
    "This notebook does the following things:\n",
    "- Load topic and stance embeddings from Hugging Face\n",
    "- Load transformer models for topic, stance, and summarization\n",
    "- Upload and analyze a news article\n",
    "- Retrieve similar topics with opposing viewpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b420c6-8c50-4968-ae9f-9d8052200484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Setup and Configuration\n",
    "# ====================================================\n",
    "!pip install -q chromadb sentence-transformers transformers huggingface-hub pymupdf beautifulsoup4 scikit-learn\n",
    "\n",
    "import os, json, gc, tempfile, requests\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from huggingface_hub import hf_hub_download\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from bs4 import BeautifulSoup\n",
    "import fitz  # PyMuPDF for PDFs\n",
    "import chromadb\n",
    "from google.colab import files\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Disable telemetry and tokenizer parallelism\n",
    "os.environ[\"CHROMA_TELEMETRY_ENABLED\"] = \"false\"\n",
    "os.environ[\"ANONYMIZED_TELEMETRY\"] = \"false\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "HF_DATASET_ID = \"zanimal/anti-echo-artifacts\"\n",
    "REPO_OWNER = \"AHMerrill\"\n",
    "REPO_NAME = \"anti-echo-chamber\"\n",
    "BRANCH = \"main\"\n",
    "\n",
    "TOPIC_MODEL_NAME = \"intfloat/e5-base-v2\"\n",
    "STANCE_MODEL_NAME = \"intfloat/e5-base-v2\"\n",
    "SUMMARIZER_MODEL_NAME = \"facebook/bart-large-cnn\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f6eb96-5875-4c73-aba7-4d23b225bb56",
   "metadata": {},
   "source": [
    "## Load or Rebuild Chroma from Hugging Face Dataset\n",
    "This step fetches topic and stance embeddings from your Hugging Face dataset and constructs local Chroma collections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06f6fe1-ca11-49b4-8d35-0f9dd1e7dbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMA_DIR = Path(\"chroma_db\")\n",
    "if CHROMA_DIR.exists():\n",
    "    print(\"Existing Chroma found. Using cached version.\")\n",
    "    client = chromadb.PersistentClient(path=str(CHROMA_DIR))\n",
    "else:\n",
    "    print(\"Building Chroma collections from Hugging Face dataset...\")\n",
    "    CHROMA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    client = chromadb.PersistentClient(path=str(CHROMA_DIR))\n",
    "    topic_coll = client.get_or_create_collection(\"news_topic\", metadata={\"hnsw:space\": \"cosine\"})\n",
    "    stance_coll = client.get_or_create_collection(\"news_stance\", metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "    REGISTRY_URL = f\"https://raw.githubusercontent.com/{REPO_OWNER}/{REPO_NAME}/{BRANCH}/artifacts/artifacts_registry.json\"\n",
    "    REGISTRY = requests.get(REGISTRY_URL, timeout=30).json()\n",
    "\n",
    "    for b in REGISTRY.get(\"batches\", []):\n",
    "        paths = b.get(\"paths\") or {}\n",
    "        if not all(k in paths for k in [\"embeddings_topic\", \"embeddings_stance\", \"metadata_topic\", \"metadata_stance\"]):\n",
    "            continue\n",
    "\n",
    "        t_vecs = np.load(hf_hub_download(HF_DATASET_ID, paths[\"embeddings_topic\"], repo_type=\"dataset\"))[\"arr_0\"]\n",
    "        s_vecs = np.load(hf_hub_download(HF_DATASET_ID, paths[\"embeddings_stance\"], repo_type=\"dataset\"))[\"arr_0\"]\n",
    "        t_meta = [json.loads(l) for l in open(hf_hub_download(HF_DATASET_ID, paths[\"metadata_topic\"], repo_type=\"dataset\"), encoding=\"utf-8\")]\n",
    "        s_meta = [json.loads(l) for l in open(hf_hub_download(HF_DATASET_ID, paths[\"metadata_stance\"], repo_type=\"dataset\"), encoding=\"utf-8\")]\n",
    "\n",
    "        topic_coll.upsert(ids=[m.get(\"id\", f\"topic::{i}\") for i, m in enumerate(t_meta)], embeddings=t_vecs.tolist(), metadatas=t_meta)\n",
    "        stance_coll.upsert(ids=[m.get(\"id\", f\"stance::{i}\") for i, m in enumerate(s_meta)], embeddings=s_vecs.tolist(), metadatas=s_meta)\n",
    "\n",
    "topic_coll = client.get_collection(\"news_topic\")\n",
    "stance_coll = client.get_collection(\"news_stance\")\n",
    "print(\"Chroma ready with:\", topic_coll.count(), \"topic vectors\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf04491a-972d-4031-b6f5-0b5b8f5b32ea",
   "metadata": {},
   "source": [
    "## Load Embedding and Summarization Models\n",
    "We will use the following:\n",
    "- intfloat/e5-base-v2 for topic and stance embeddings\n",
    "- facebook/bart-large-cnn for summarization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8de0dd9-3903-414a-9b62-fef076cdc7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = SentenceTransformer(TOPIC_MODEL_NAME)\n",
    "stance_model = SentenceTransformer(STANCE_MODEL_NAME)\n",
    "tok_sum = AutoTokenizer.from_pretrained(SUMMARIZER_MODEL_NAME)\n",
    "model_sum = AutoModelForSeq2SeqLM.from_pretrained(SUMMARIZER_MODEL_NAME)\n",
    "\n",
    "print(\"Models loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35a3a82-e939-4acd-aa79-72b9d2f8d128",
   "metadata": {},
   "source": [
    "## Upload an Article\n",
    "Upload a .txt, .pdf, or .html file for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cd0f63-868b-494b-a655-177813c2829f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded = files.upload()\n",
    "filename = list(uploaded.keys())[0]\n",
    "ext = Path(filename).suffix.lower()\n",
    "\n",
    "def extract_text(file_path):\n",
    "    if ext == \".txt\":\n",
    "        return open(file_path, encoding=\"utf-8\", errors=\"ignore\").read()\n",
    "    elif ext == \".pdf\":\n",
    "        text = \"\"\n",
    "        with fitz.open(file_path) as doc:\n",
    "            for page in doc:\n",
    "                text += page.get_text()\n",
    "        return text\n",
    "    elif ext == \".html\":\n",
    "        html = open(file_path, encoding=\"utf-8\", errors=\"ignore\").read()\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        return soup.get_text(separator=\"\\n\")\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type\")\n",
    "\n",
    "text = extract_text(filename)\n",
    "display(Markdown(f\"**Extracted first 2,000 characters:**\\n\\n{text[:2000]}...\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdbe1cb-63fb-4f08-a8ea-f2324703c18b",
   "metadata": {},
   "source": [
    "## Summarize and Compute Embeddings\n",
    "This step summarizes the article for stance analysis, then embeds both the summary and full text for topic analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e965da0-87f5-465e-95c2-036e7ee34155",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tok_sum([text], return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "summary_ids = model_sum.generate(**inputs, max_length=150, num_beams=4, early_stopping=True)\n",
    "summary = tok_sum.batch_decode(summary_ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "stance_vec = stance_model.encode([summary], normalize_embeddings=True)[0]\n",
    "topic_vecs = topic_model.encode([summary, text[:3000]], normalize_embeddings=True)\n",
    "topic_vec_mean = topic_vecs.mean(axis=0)\n",
    "\n",
    "display(Markdown(f\"### Summary for Stance Analysis\\n> {summary}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb94bce-0c44-452a-bef2-56feed9c8924",
   "metadata": {},
   "source": [
    "## Query for Similar Topics\n",
    "Retrieve the 100 most similar articles by topic embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b4bddb-a764-4152-9980-bc44840c3a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = topic_coll.query(\n",
    "    query_embeddings=[topic_vec_mean.tolist()],\n",
    "    n_results=100,\n",
    "    include=[\"metadatas\"]\n",
    ")\n",
    "flat_results = [m for batch in results[\"metadatas\"] for m in batch]\n",
    "print(f\"Found {len(flat_results)} potential topic matches.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a04d60-f69d-4e52-9b8c-1d466ded896e",
   "metadata": {},
   "source": [
    "## Rank by Opposing Stance\n",
    "We compute cosine similarity between stance embeddings and display opposing viewpoints first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024a09cb-a683-491f-9078-53e01977e4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not flat_results:\n",
    "    display(Markdown(\"No topic matches found. The database may still be updating.\"))\n",
    "else:\n",
    "    stance_summaries = [m.get(\"stance_summary\", m.get(\"summary\", m.get(\"title\", \"\"))) for m in flat_results]\n",
    "    stance_embeddings = np.array([stance_model.encode([s], normalize_embeddings=True)[0] for s in stance_summaries])\n",
    "    stance_sims = cosine_similarity([stance_vec], stance_embeddings)[0]\n",
    "\n",
    "    ranked = sorted(zip(flat_results, stance_sims), key=lambda x: x[1])\n",
    "\n",
    "    def sim_label(s):\n",
    "        if s < 0.2: return \"Very Dissimilar\"\n",
    "        elif s < 0.4: return \"Dissimilar\"\n",
    "        elif s < 0.6: return \"Somewhat Similar\"\n",
    "        elif s < 0.8: return \"Similar\"\n",
    "        else: return \"Very Similar\"\n",
    "\n",
    "    display(Markdown(\"### Results: Similar Topics, Contrasting Perspectives\"))\n",
    "    for meta, sim in ranked[:10]:\n",
    "        topic_display = meta.get(\"topic_label\") or meta.get(\"inferred_topic\") or \"(topic unknown)\"\n",
    "        md = f\"\"\"\n",
    "**{meta.get('title','(untitled)')}**  \n",
    "Source: {meta.get('domain','unknown')}  \n",
    "Topic: *{topic_display}*  \n",
    "Stance Similarity: {sim:.2f} ({sim_label(sim)})  \n",
    "[Read original article]({meta.get('url','#')})\n",
    "\"\"\"\n",
    "        display(Markdown(md))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9af150-b65e-433e-ac61-42a5fcca2acf",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "Free GPU and CPU memory after analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bb1c51-29fb-44e6-a312-7857644d8a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "del text, summary, stance_vec, topic_vec_mean, topic_vecs\n",
    "gc.collect()\n",
    "print(\"Memory cleared.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
