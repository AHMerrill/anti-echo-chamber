{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Setup 1 of N — environment and workspace\n",
        "\n",
        "Purpose\n",
        "Install core dependencies, set up a clean workspace at /content/anti_echo, and print diagnostics so collaborators can debug quickly.\n",
        "\n",
        "Why this matters\n",
        "Pinned installs reduce breakage. A consistent folder layout keeps artifacts predictable. Diagnostics help when the runtime changes.\n",
        "\n",
        "Outputs\n",
        "Directories created under /content/anti_echo, environment flags set, package versions printed."
      ],
      "metadata": {
        "id": "z3lyMabwG0TZ"
      },
      "id": "z3lyMabwG0TZ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup 1 of N: environment and workspace\n",
        "# Colab safe. No Drive mount. Heavy comments for clarity.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import textwrap\n",
        "from pathlib import Path\n",
        "\n",
        "def pip_install(pkgs):\n",
        "    # Quiet installs but still show what is being installed for reproducibility\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + pkgs\n",
        "    print(\"Installing:\", \" \".join(pkgs))\n",
        "    subprocess.check_call(cmd)\n",
        "\n",
        "# Core dependencies with sane pins or upper bounds\n",
        "pip_install([\n",
        "    \"feedparser==6.0.10\",                    # RSS parsing\n",
        "    \"trafilatura>=1.6.2,<2.0\",               # robust article extraction\n",
        "    \"sentence-transformers>=2.6.1,<3.0\",     # embeddings\n",
        "    \"chromadb>=0.5.5,<0.6.0\",                # local vector store\n",
        "    \"huggingface_hub>=0.24.0,<0.28.0\",       # HF dataset and file ops\n",
        "    \"pyyaml>=6.0.1,<7.0\",                    # config parsing\n",
        "    \"numpy>=1.26.4,<3.0\",                    # arrays\n",
        "    \"tqdm>=4.66.0,<5.0\",                     # progress\n",
        "    \"requests>=2.31.0,<3.0\",                 # HTTP\n",
        "    \"rapidfuzz>=3.6.0,<4.0\",                 # dedupe or fuzzy utils\n",
        "    \"scikit-learn>=1.4.0,<2.0\",              # clustering for topics\n",
        "    \"transformers>=4.43.0,<5.0\",             # summarization model\n",
        "    \"nltk>=3.8.1,<4.0\"                       # sentence splitting\n",
        "])\n",
        "\n",
        "# Optional accelerator. If import fails, install a compatible torch\n",
        "try:\n",
        "    import torch\n",
        "except Exception:\n",
        "    pip_install([\"torch>=2.2.0,<3.0\"])\n",
        "    import torch\n",
        "\n",
        "# Define a single project root for all artifacts in this session\n",
        "PROJECT_ROOT = Path(\"/content/anti_echo\").resolve()\n",
        "SUBDIRS = [\n",
        "    \"raw\",        # scraped article text and sidecar metadata\n",
        "    \"batches\",    # packaged embeddings and manifest before HF upload\n",
        "    \"chroma_db\",  # local persistent Chroma store\n",
        "    \"logs\",       # run logs\n",
        "    \"feeds\",      # index and feed state\n",
        "    \"tmp\"         # scratch space\n",
        "]\n",
        "for d in SUBDIRS:\n",
        "    (PROJECT_ROOT / d).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Set environment flags to reduce noise and avoid accidental multithreading bugs\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ[\"CHROMA_TELEMETRY_ENABLED\"] = \"false\"\n",
        "os.environ[\"ANONYMIZED_TELEMETRY\"] = \"false\"\n",
        "\n",
        "# Print environment diagnostics for reproducibility and easy debugging\n",
        "import platform, json\n",
        "from importlib.metadata import version, PackageNotFoundError\n",
        "\n",
        "def v(name):\n",
        "    try:\n",
        "        return version(name)\n",
        "    except PackageNotFoundError:\n",
        "        return \"not-installed\"\n",
        "\n",
        "info = {\n",
        "    \"python\": sys.version.split()[0],\n",
        "    \"platform\": platform.platform(),\n",
        "    \"packages\": {\n",
        "        \"feedparser\": v(\"feedparser\"),\n",
        "        \"trafilatura\": v(\"trafilatura\"),\n",
        "        \"sentence-transformers\": v(\"sentence-transformers\"),\n",
        "        \"chromadb\": v(\"chromadb\"),\n",
        "        \"huggingface_hub\": v(\"huggingface-hub\"),\n",
        "        \"PyYAML\": v(\"PyYAML\"),\n",
        "        \"numpy\": v(\"numpy\"),\n",
        "        \"rapidfuzz\": v(\"rapidfuzz\"),\n",
        "        \"torch\": v(\"torch\"),\n",
        "        \"tqdm\": v(\"tqdm\"),\n",
        "        \"requests\": v(\"requests\"),\n",
        "        \"scikit-learn\": v(\"scikit-learn\"),\n",
        "        \"transformers\": v(\"transformers\"),\n",
        "        \"nltk\": v(\"nltk\"),\n",
        "    },\n",
        "    \"paths\": {\n",
        "        \"project_root\": str(PROJECT_ROOT),\n",
        "        \"raw\": str(PROJECT_ROOT / \"raw\"),\n",
        "        \"batches\": str(PROJECT_ROOT / \"batches\"),\n",
        "        \"chroma_db\": str(PROJECT_ROOT / \"chroma_db\"),\n",
        "        \"logs\": str(PROJECT_ROOT / \"logs\"),\n",
        "        \"feeds\": str(PROJECT_ROOT / \"feeds\"),\n",
        "        \"tmp\": str(PROJECT_ROOT / \"tmp\"),\n",
        "    }\n",
        "}\n",
        "\n",
        "# CUDA info is helpful to know if summarization can use GPU\n",
        "info[\"cuda_available\"] = bool(torch.cuda.is_available())\n",
        "if info[\"cuda_available\"]:\n",
        "    info[\"cuda_device_name\"] = torch.cuda.get_device_name(0)\n",
        "\n",
        "print(json.dumps(info, indent=2))\n",
        "\n",
        "# A tiny workspace readme helps collaborators quickly orient\n",
        "workspace_readme = PROJECT_ROOT / \"README_WORKSPACE.txt\"\n",
        "if not workspace_readme.exists():\n",
        "    workspace_readme.write_text(textwrap.dedent(\"\"\"\n",
        "        anti echo chamber - Colab workspace\n",
        "        This directory is ephemeral per session.\n",
        "        Do not commit files from here directly.\n",
        "        Subdirs:\n",
        "          raw        - local scraped texts and meta for this session\n",
        "          batches    - locally packaged batches before HF upload\n",
        "          chroma_db  - local Chroma rebuild target\n",
        "          logs       - run logs\n",
        "          feeds      - runtime feed artifacts\n",
        "          tmp        - scratch space\n",
        "    \"\"\").strip() + \"\\n\", encoding=\"utf-8\")\n",
        "print(f\"Workspace ready at {PROJECT_ROOT}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zffpD2rNG0Ax",
        "outputId": "0660ffc5-ae11-485a-a0be-9fb8b2cd77b4"
      },
      "id": "zffpD2rNG0Ax",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing: feedparser==6.0.10 trafilatura>=1.6.2,<2.0 sentence-transformers>=2.6.1,<3.0 chromadb>=0.5.5,<0.6.0 huggingface_hub>=0.24.0,<0.28.0 pyyaml>=6.0.1,<7.0 numpy>=1.26.4,<3.0 tqdm>=4.66.0,<5.0 requests>=2.31.0,<3.0 rapidfuzz>=3.6.0,<4.0\n",
            "{\n",
            "  \"python\": \"3.12.11\",\n",
            "  \"platform\": \"Linux-6.6.97+-x86_64-with-glibc2.35\",\n",
            "  \"cuda_available\": true,\n",
            "  \"packages\": {\n",
            "    \"feedparser\": \"6.0.10\",\n",
            "    \"trafilatura\": \"1.12.2\",\n",
            "    \"sentence-transformers\": \"2.7.0\",\n",
            "    \"chromadb\": \"0.5.23\",\n",
            "    \"huggingface_hub\": \"0.27.1\",\n",
            "    \"pyyaml\": \"6.0.3\",\n",
            "    \"numpy\": \"2.0.2\",\n",
            "    \"rapidfuzz\": \"3.14.1\",\n",
            "    \"torch\": \"2.8.0+cu126\",\n",
            "    \"tqdm\": \"4.67.1\",\n",
            "    \"requests\": \"2.32.4\"\n",
            "  },\n",
            "  \"paths\": {\n",
            "    \"project_root\": \"/content/anti_echo\",\n",
            "    \"raw\": \"/content/anti_echo/raw\",\n",
            "    \"batches\": \"/content/anti_echo/batches\",\n",
            "    \"chroma_db\": \"/content/anti_echo/chroma_db\",\n",
            "    \"logs\": \"/content/anti_echo/logs\",\n",
            "    \"feeds\": \"/content/anti_echo/feeds\",\n",
            "    \"tmp\": \"/content/anti_echo/tmp\"\n",
            "  },\n",
            "  \"cuda_device_name\": \"NVIDIA A100-SXM4-40GB\"\n",
            "}\n",
            "Workspace ready at /content/anti_echo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup 2 of N — config and paths bootstrap\n",
        "\n",
        "Purpose\n",
        "Pull config and label maps from your GitHub repo, cache them locally in the Colab session, validate required keys, and create runtime dirs from config.\n",
        "\n",
        "Why this matters\n",
        "Single source of truth for model names, dims, batch filenames, and collection names. Keeps the notebook aligned with the repo.\n",
        "\n",
        "Outputs\n",
        "CONFIG, STANCE_AXES, TOPIC_LABELS in memory, directories created, and HF_DATASET_ID exported to env.\n"
      ],
      "metadata": {
        "id": "rJEp6xNjHjAz"
      },
      "id": "rJEp6xNjHjAz"
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup 2 of N: config and paths bootstrap\n",
        "# Reads config files from your GitHub repo and prepares runtime paths.\n",
        "\n",
        "import os\n",
        "import json\n",
        "import yaml\n",
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "PROJECT_ROOT = Path(\"/content/anti_echo\").resolve()\n",
        "CONFIG_CACHE = PROJECT_ROOT / \"config_cache\"\n",
        "CONFIG_CACHE.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Your repo location for configuration files\n",
        "REPO_OWNER = \"AHMerrill\"\n",
        "REPO_NAME = \"anti-echo-chamber\"\n",
        "BRANCH = \"main\"\n",
        "\n",
        "def raw_url(path: str) -> str:\n",
        "    # Compose a raw GitHub URL for a given path in the repo\n",
        "    return f\"https://raw.githubusercontent.com/{REPO_OWNER}/{REPO_NAME}/{BRANCH}/{path.lstrip('/')}\"\n",
        "\n",
        "def fetch_text_first(paths):\n",
        "    # Try multiple candidate filenames and return the first that exists\n",
        "    last_err = None\n",
        "    for p in paths:\n",
        "        url = raw_url(p)\n",
        "        try:\n",
        "            r = requests.get(url, timeout=20)\n",
        "            if r.status_code == 200 and r.text.strip():\n",
        "                return r.text, p, url\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "    raise RuntimeError(f\"Could not fetch any of {paths}. Last error: {last_err}\")\n",
        "\n",
        "# Candidate paths allow json or yaml variants without editing the notebook\n",
        "CFG_CANDIDATES = [\"config/config.yaml\",\"config/config.yml\",\"config/config.json\"]\n",
        "STANCE_CANDIDATES = [\"config/stance_axes.json\",\"config/stance_axes.yaml\",\"config/stance_axes.yml\"]\n",
        "TOPIC_CANDIDATES = [\"config/topic_labels.json\",\"config/topic_labels.yaml\",\"config/topic_labels.yml\"]\n",
        "\n",
        "# Fetch and cache a local copy in the Colab session\n",
        "cfg_txt, cfg_path, cfg_url = fetch_text_first(CFG_CANDIDATES)\n",
        "stance_txt, stance_path, stance_url = fetch_text_first(STANCE_CANDIDATES)\n",
        "topic_txt, topic_path, topic_url = fetch_text_first(TOPIC_CANDIDATES)\n",
        "\n",
        "(CONFIG_CACHE / Path(cfg_path).name).write_text(cfg_txt, encoding=\"utf-8\")\n",
        "(CONFIG_CACHE / Path(stance_path).name).write_text(stance_txt, encoding=\"utf-8\")\n",
        "(CONFIG_CACHE / Path(topic_path).name).write_text(topic_txt, encoding=\"utf-8\")\n",
        "\n",
        "def parse_json_or_yaml(txt: str):\n",
        "    # Parse as JSON first, fall back to YAML\n",
        "    txt = txt.strip()\n",
        "    try:\n",
        "        return json.loads(txt)\n",
        "    except Exception:\n",
        "        return yaml.safe_load(txt)\n",
        "\n",
        "# Parse into Python structures\n",
        "CONFIG = yaml.safe_load(cfg_txt) if cfg_path.endswith((\".yaml\",\".yml\")) else json.loads(cfg_txt)\n",
        "STANCE_AXES = parse_json_or_yaml(stance_txt)\n",
        "TOPIC_LABELS = parse_json_or_yaml(topic_txt)\n",
        "\n",
        "# Validate that essential keys exist to avoid surprises later\n",
        "required_cfg_keys = [\"hf_dataset_id\",\"chroma_collections\",\"embeddings\",\"batch\",\"ids\",\"chroma\"]\n",
        "missing = [k for k in required_cfg_keys if k not in CONFIG]\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing required config keys: {missing}\")\n",
        "\n",
        "# Create directories from config for batches, chroma, and logs\n",
        "(Path(PROJECT_ROOT / CONFIG[\"batch\"][\"base_dir\"])).mkdir(parents=True, exist_ok=True)\n",
        "(Path(PROJECT_ROOT / CONFIG[\"chroma\"][\"dir\"])).mkdir(parents=True, exist_ok=True)\n",
        "(Path(PROJECT_ROOT / CONFIG.get(\"logging\", {}).get(\"save_dir\", \"logs\"))).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Print a short summary and export dataset id into the environment\n",
        "print(json.dumps({\n",
        "    \"hf_dataset_id\": CONFIG[\"hf_dataset_id\"],\n",
        "    \"collections\": CONFIG[\"chroma_collections\"],\n",
        "    \"embeddings\": CONFIG[\"embeddings\"],\n",
        "    \"paths\": {\n",
        "        \"batches\": str(PROJECT_ROOT / CONFIG[\"batch\"][\"base_dir\"]),\n",
        "        \"chroma_db\": str(PROJECT_ROOT / CONFIG[\"chroma\"][\"dir\"]),\n",
        "    },\n",
        "    \"source_urls\": {\"config\": cfg_url, \"stance_axes\": stance_url, \"topic_labels\": topic_url}\n",
        "}, indent=2))\n",
        "\n",
        "os.environ[\"HF_DATASET_ID\"] = CONFIG[\"hf_dataset_id\"]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhAwiP7LHjgu",
        "outputId": "dfed6ce6-fe62-4b03-da57-ed7b0365d864"
      },
      "id": "vhAwiP7LHjgu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"hf_dataset_id\": \"zanimal/anti-echo-artifacts\",\n",
            "  \"collections\": {\n",
            "    \"topic\": \"news_topic\",\n",
            "    \"stance\": \"news_stance\"\n",
            "  },\n",
            "  \"embeddings\": {\n",
            "    \"topic_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
            "    \"stance_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
            "    \"dim\": 384,\n",
            "    \"dtype\": \"float16\",\n",
            "    \"pooling\": \"mean\",\n",
            "    \"chunk_tokens\": 512\n",
            "  },\n",
            "  \"summarizer\": {\n",
            "    \"model\": \"facebook/bart-large-cnn\",\n",
            "    \"target_sentences\": 5,\n",
            "    \"truncation\": 2048\n",
            "  },\n",
            "  \"batch_files\": {\n",
            "    \"topic_file\": \"embeddings_topic.npz\",\n",
            "    \"stance_file\": \"embeddings_stance.npz\",\n",
            "    \"metadata_file\": \"metadata.jsonl\",\n",
            "    \"manifest_name\": \"manifest.json\",\n",
            "    \"base_dir\": \"batches\"\n",
            "  },\n",
            "  \"id_policy\": {\n",
            "    \"scheme\": \"domain-slug-sha12\",\n",
            "    \"hash\": \"sha256\",\n",
            "    \"normalize_whitespace\": true,\n",
            "    \"lowercase\": true\n",
            "  },\n",
            "  \"paths\": {\n",
            "    \"project_root\": \"/content/anti_echo\",\n",
            "    \"config_cache\": \"/content/anti_echo/config_cache\",\n",
            "    \"raw\": \"/content/anti_echo/raw\",\n",
            "    \"batches\": \"/content/anti_echo/batches\",\n",
            "    \"chroma_db\": \"/content/anti_echo/chroma_db\",\n",
            "    \"logs\": \"/content/anti_echo/logs\",\n",
            "    \"tmp\": \"/content/anti_echo/tmp\"\n",
            "  },\n",
            "  \"loaded\": {\n",
            "    \"stance_axes_count\": 49,\n",
            "    \"topic_labels_count\": 24\n",
            "  },\n",
            "  \"source_urls\": {\n",
            "    \"config\": \"https://raw.githubusercontent.com/AHMerrill/anti-echo-chamber/main/config/config.yaml\",\n",
            "    \"stance_axes\": \"https://raw.githubusercontent.com/AHMerrill/anti-echo-chamber/main/config/stance_axes.json\",\n",
            "    \"topic_labels\": \"https://raw.githubusercontent.com/AHMerrill/anti-echo-chamber/main/config/topic_labels.json\"\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup 3 of N — authentication for Hugging Face and GitHub\n",
        "\n",
        "Purpose\n",
        "Load tokens into environment and verify access. Later cells use these tokens to push to HF and update your GitHub registry.\n",
        "\n",
        "Why this matters\n",
        "Catching auth issues early prevents failing halfway through a run.\n",
        "\n",
        "Outputs\n",
        "Logged in to HF, GitHub token validated."
      ],
      "metadata": {
        "id": "7FuW4A7ELmrx"
      },
      "id": "7FuW4A7ELmrx"
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup 3 of N: auth for Hugging Face and GitHub\n",
        "# Prompts only if tokens are not already in the environment.\n",
        "\n",
        "import os\n",
        "import requests\n",
        "from getpass import getpass\n",
        "from huggingface_hub import login, whoami\n",
        "\n",
        "def need(envvar, prompt):\n",
        "    # Request once per session if missing\n",
        "    if not os.environ.get(envvar, \"\").strip():\n",
        "        os.environ[envvar] = getpass(prompt)\n",
        "    print(f\"{envvar} loaded:\", bool(os.environ.get(envvar)))\n",
        "\n",
        "# Gather tokens\n",
        "need(\"HF_TOKEN\", \"Enter your Hugging Face token: \")\n",
        "need(\"GITHUB_TOKEN\", \"Enter your GitHub Personal Access Token: \")\n",
        "\n",
        "# Sign in to HF so upload_file works later\n",
        "try:\n",
        "    login(token=os.environ[\"HF_TOKEN\"], add_to_git_credential=False)\n",
        "    print(\"HF login OK:\", whoami(token=os.environ[\"HF_TOKEN\"]).get(\"name\",\"(ok)\"))\n",
        "except Exception as e:\n",
        "    print(\"HF login failed:\", e)\n",
        "\n",
        "# Quick GitHub check to confirm token scopes\n",
        "try:\n",
        "    r = requests.get(\n",
        "        \"https://api.github.com/user\",\n",
        "        headers={\"Authorization\": f\"Bearer {os.environ['GITHUB_TOKEN']}\"},\n",
        "        timeout=15\n",
        "    )\n",
        "    print(\"GitHub auth status:\", r.status_code)\n",
        "except Exception as e:\n",
        "    print(\"GitHub auth check failed:\", e)\n"
      ],
      "metadata": {
        "id": "D0Si5GInpuhV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f370b5a8-89f2-461b-822e-6f003c40c4b9"
      },
      "id": "D0Si5GInpuhV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Hugging Face token: ··········\n",
            "HF_TOKEN loaded: True\n",
            "Enter your GitHub Personal Access Token: ··········\n",
            "GITHUB_TOKEN loaded: True\n",
            "GitHub auth status: 200\n",
            "HF user: zanimal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup 4 of N — restore feed index and feed state\n",
        "\n",
        "Purpose\n",
        "Restore feeds/index.json and feeds/feeds_state.json from HF latest pointers, fallback to GitHub if missing, or reconstruct from HF batch metadata if neither exists.\n",
        "\n",
        "Why this matters\n",
        "Prevents re scraping the same URLs, and keeps numbering and batching consistent across runs.\n",
        "\n",
        "Outputs\n",
        "feeds/index.json and feeds/feeds_state.json present locally with a quick summary."
      ],
      "metadata": {
        "id": "PcIvKe4m4v9r"
      },
      "id": "PcIvKe4m4v9r"
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup 4 of N: restore feed index and state\n",
        "# Guarantees local index and state exist before scraping.\n",
        "\n",
        "import os, json, shutil, requests, datetime as dt, re, hashlib\n",
        "from pathlib import Path\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "PROJECT_ROOT = Path(\"/content/anti_echo\").resolve()\n",
        "FEEDS_DIR = PROJECT_ROOT / \"feeds\"\n",
        "FEEDS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "INDEX_PATH = FEEDS_DIR / \"index.json\"\n",
        "STATE_PATH = FEEDS_DIR / \"feeds_state.json\"\n",
        "HF_DATASET_ID = os.environ[\"HF_DATASET_ID\"]\n",
        "\n",
        "REPO_OWNER = \"AHMerrill\"\n",
        "REPO_NAME = \"anti-echo-chamber\"\n",
        "BRANCH = \"main\"\n",
        "\n",
        "def try_hf_restore() -> bool:\n",
        "    # Prefer HF because it is the single source of truth in this design\n",
        "    try:\n",
        "        st = hf_hub_download(HF_DATASET_ID, \"feeds/feeds_state_latest.json\", repo_type=\"dataset\")\n",
        "        ix = hf_hub_download(HF_DATASET_ID, \"feeds/feed_index_latest.json\", repo_type=\"dataset\")\n",
        "        shutil.copy(st, STATE_PATH)\n",
        "        shutil.copy(ix, INDEX_PATH)\n",
        "        print(\"Restored feed state from HF latest\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(\"HF latest not found:\", e)\n",
        "        return False\n",
        "\n",
        "def try_github_restore() -> bool:\n",
        "    # Fallback if HF latest pointers are not present yet\n",
        "    try:\n",
        "        base = f\"https://raw.githubusercontent.com/{REPO_OWNER}/{REPO_NAME}/{BRANCH}/feeds\"\n",
        "        got = False\n",
        "        for src, dst in [(\"feeds_state_latest.json\", STATE_PATH), (\"feed_index_latest.json\", INDEX_PATH)]:\n",
        "            r = requests.get(f\"{base}/{src}\", timeout=20)\n",
        "            if r.status_code == 200 and r.text.strip():\n",
        "                dst.write_text(r.text, encoding=\"utf-8\")\n",
        "                got = True\n",
        "        if got:\n",
        "            print(\"Restored feed state from GitHub latest\")\n",
        "        return got\n",
        "    except Exception as e:\n",
        "        print(\"GitHub restore failed:\", e)\n",
        "        return False\n",
        "\n",
        "restored = try_hf_restore() or try_github_restore()\n",
        "\n",
        "if not restored:\n",
        "    # Reconstruct from HF batch metadata listed in artifacts_registry.json\n",
        "    print(\"No latest feed state found. Attempting reconstruction from HF batches...\")\n",
        "    REGISTRY_URL = f\"https://raw.githubusercontent.com/{REPO_OWNER}/{REPO_NAME}/{BRANCH}/artifacts/artifacts_registry.json\"\n",
        "    REGISTRY = requests.get(REGISTRY_URL, timeout=20).json()\n",
        "\n",
        "    # Collect metadata jsonl from each batch\n",
        "    metas = []\n",
        "    for b in REGISTRY.get(\"batches\", []):\n",
        "        meta_rel = (b.get(\"hf_paths\") or b.get(\"paths\") or {}).get(\"metadata\")\n",
        "        if not meta_rel:\n",
        "            continue\n",
        "        try:\n",
        "            metas.append(Path(hf_hub_download(HF_DATASET_ID, meta_rel, repo_type=\"dataset\")))\n",
        "        except Exception as e:\n",
        "            print(\"Skip meta fetch:\", e)\n",
        "\n",
        "    # Build a minimal index of known URLs to prevent re scraping\n",
        "    items = {}\n",
        "    def norm(txt): return re.sub(r\"\\s+\",\" \", txt.strip().lower())\n",
        "    def sha256_text(txt): return hashlib.sha256(norm(txt).encode()).hexdigest()\n",
        "\n",
        "    for fp in metas:\n",
        "        for line in fp.read_text(encoding=\"utf-8\").splitlines():\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            try:\n",
        "                obj = json.loads(line)\n",
        "            except Exception:\n",
        "                continue\n",
        "            u = obj.get(\"url\")\n",
        "            if u and u not in items:\n",
        "                items[u] = {\"status\": \"ok\", \"fetched_at\": dt.datetime.now(dt.timezone.utc).isoformat()}\n",
        "\n",
        "    INDEX_PATH.write_text(json.dumps({\"last_updated\": dt.datetime.now(dt.timezone.utc).isoformat(),\"items\": items}, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "    # Create a simple ring buffer structure for each feed\n",
        "    url_hashes = [sha256_text(u)[:12] for u in items.keys()]\n",
        "    feeds_block = {\n",
        "        \"commentisfree\": {\"feed_url\": None, \"recent_url_hashes\": url_hashes[-1000:], \"recent_url_hashes_max\": 1000},\n",
        "        \"theguardian\": {\"feed_url\": None, \"recent_url_hashes\": url_hashes[-500:], \"recent_url_hashes_max\": 500},\n",
        "    }\n",
        "    STATE_PATH.write_text(json.dumps({\"version\":1,\"updated_at\": dt.datetime.now(dt.timezone.utc).isoformat(),\"feeds\": feeds_block}, indent=2), encoding=\"utf-8\")\n",
        "    print(\"Reconstruction complete\")\n",
        "\n",
        "# Echo a small summary so you can verify\n",
        "ix = json.loads(INDEX_PATH.read_text(encoding=\"utf-8\"))\n",
        "st = json.loads(STATE_PATH.read_text(encoding=\"utf-8\"))\n",
        "print({\"index_items\": len(ix.get(\"items\",{})), \"feeds\": list(st.get(\"feeds\",{}).keys())})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZlphFLgMJWk",
        "outputId": "9253db25-e85a-402d-e740-f1c3c595627f"
      },
      "id": "8ZlphFLgMJWk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Hugging Face token: ··········\n",
            "HF_TOKEN set in environment for this session (will reset when runtime restarts).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup 5 of N — rebuild Chroma from HF batches\n",
        "\n",
        "Purpose\n",
        "Create or refresh local Chroma collections from the HF dataset using artifacts/artifacts_registry.json as the ledger. HF is the source of truth. Local Chroma is a cache.\n",
        "\n",
        "Why this matters\n",
        "Ensures your retrieval state is consistent before adding new data. No duplicated rows. Clean numbering follows registry order.\n",
        "\n",
        "Outputs\n",
        "Two Chroma collections present with counts: topic and stance."
      ],
      "metadata": {
        "id": "dTRylrXg44OE"
      },
      "id": "dTRylrXg44OE"
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup 5 of N: rebuild Chroma from HF\n",
        "# Uses HF-hosted batches to fully restore local Chroma before any new run.\n",
        "\n",
        "import os, json, numpy as np\n",
        "from pathlib import Path\n",
        "from huggingface_hub import hf_hub_download\n",
        "import chromadb, requests\n",
        "\n",
        "PROJECT_ROOT = Path(\"/content/anti_echo\").resolve()\n",
        "CHROMA_DIR = PROJECT_ROOT / CONFIG[\"chroma\"][\"dir\"]\n",
        "HF_DATASET_ID = CONFIG[\"hf_dataset_id\"]\n",
        "COLL_TOPIC = CONFIG[\"chroma_collections\"][\"topic\"]\n",
        "COLL_STANCE = CONFIG[\"chroma_collections\"][\"stance\"]\n",
        "EMB_DIM = int(CONFIG[\"embeddings\"][\"dim\"])\n",
        "\n",
        "def ensure_chroma():\n",
        "    # Persistent Chroma so later cells see the same state\n",
        "    client = chromadb.PersistentClient(path=str(CHROMA_DIR))\n",
        "    t = client.get_or_create_collection(name=COLL_TOPIC, metadata={\"hnsw:space\": \"cosine\"})\n",
        "    s = client.get_or_create_collection(name=COLL_STANCE, metadata={\"hnsw:space\": \"cosine\"})\n",
        "    return client, t, s\n",
        "\n",
        "def read_metadata_jsonl(fp: Path):\n",
        "    # Metadata drives ids and retrieval context later\n",
        "    ids, metas = [], []\n",
        "    with fp.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            obj = json.loads(line)\n",
        "            ids.append(obj[\"id\"])\n",
        "            metas.append(obj)\n",
        "    return ids, metas\n",
        "\n",
        "def load_npz(fp: Path, dim: int):\n",
        "    # Supports compressed npz with a single array inside\n",
        "    arr = np.load(fp, allow_pickle=False)\n",
        "    if isinstance(arr, np.lib.npyio.NpzFile):\n",
        "        keys = list(arr.files)\n",
        "        arr = arr[keys[0]]\n",
        "    vecs = np.asarray(arr)\n",
        "    if vecs.ndim != 2 or vecs.shape[1] != dim:\n",
        "        raise ValueError(f\"Bad embedding shape in {fp.name}. Got {vecs.shape}, expected [N,{dim}]\")\n",
        "    if not np.isfinite(vecs).all():\n",
        "        raise ValueError(f\"Non finite values in {fp.name}\")\n",
        "    return vecs\n",
        "\n",
        "def upsert_chunks(coll, ids, vecs, metas, chunk=2048):\n",
        "    # Bulk upsert in chunks to avoid payload size limits\n",
        "    for i in range(0, len(ids), chunk):\n",
        "        j = min(i+chunk, len(ids))\n",
        "        coll.upsert(ids=ids[i:j], embeddings=vecs[i:j].tolist(), metadatas=metas[i:j])\n",
        "\n",
        "# Pull the registry from GitHub to know which batches exist\n",
        "REGISTRY_URL = f\"https://raw.githubusercontent.com/AHMerrill/anti-echo-chamber/main/artifacts/artifacts_registry.json\"\n",
        "REGISTRY = requests.get(REGISTRY_URL, timeout=20).json()\n",
        "\n",
        "client, topic_coll, stance_coll = ensure_chroma()\n",
        "added = 0\n",
        "\n",
        "# Replay all batches in order\n",
        "for b in REGISTRY.get(\"batches\", []):\n",
        "    paths = b.get(\"hf_paths\") or b.get(\"paths\") or {}\n",
        "    if not all(k in paths for k in [\"embeddings_topic\",\"embeddings_stance\",\"metadata\",\"manifest\"]):\n",
        "        continue\n",
        "\n",
        "    # Download artifacts for this batch\n",
        "    t_local = Path(hf_hub_download(HF_DATASET_ID, paths[\"embeddings_topic\"], repo_type=\"dataset\"))\n",
        "    s_local = Path(hf_hub_download(HF_DATASET_ID, paths[\"embeddings_stance\"], repo_type=\"dataset\"))\n",
        "    m_local = Path(hf_hub_download(HF_DATASET_ID, paths[\"metadata\"], repo_type=\"dataset\"))\n",
        "\n",
        "    # Load ids, metadata, and vectors with shape checks\n",
        "    ids, metas = read_metadata_jsonl(m_local)\n",
        "    t_vecs = load_npz(t_local, EMB_DIM)\n",
        "    s_vecs = load_npz(s_local, EMB_DIM)\n",
        "    if t_vecs.shape[0] != len(ids) or s_vecs.shape[0] != len(ids):\n",
        "        raise ValueError(f\"Row count mismatch in batch {b.get('batch_id')}\")\n",
        "\n",
        "    # Upsert both collections\n",
        "    upsert_chunks(topic_coll, ids, t_vecs, metas)\n",
        "    upsert_chunks(stance_coll, ids, s_vecs, metas)\n",
        "    added += len(ids)\n",
        "    print(f\"Ingested {b.get('batch_id')} +{len(ids)}\")\n",
        "\n",
        "print({\"topic_count\": topic_coll.count(), \"stance_count\": stance_coll.count(), \"docs_added\": added})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8hxn43ELo1G",
        "outputId": "064c4417-7f26-4352-ebdf-d997ae8e653d"
      },
      "id": "w8hxn43ELo1G",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
            "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hugging Face login: OK\n",
            "HF dataset found: zanimal/anti-echo-artifacts\n",
            "{\n",
            "  \"registry_version\": 2,\n",
            "  \"models\": {\n",
            "    \"topic\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
            "    \"stance\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
            "    \"dim\": 384\n",
            "  },\n",
            "  \"batch_count\": 1\n",
            "}\n",
            "Batches overview:\n",
            "- batch_20251011T232938Z_283ca40f | docs=None | created_at=20251011T232938Z\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup 6 of N — scraping tunables and Guardian feeds\n",
        "\n",
        "Purpose\n",
        "Define scrape quotas, date floor, and feed list. Export to environment so the scraper reads configuration without edits.\n",
        "\n",
        "Why this matters\n",
        "Allows you to adjust scrape size and distribution per run from a single cell.\n",
        "\n",
        "Outputs\n",
        "Environment variables set and a printed summary."
      ],
      "metadata": {
        "id": "B8nvAQxhNAcH"
      },
      "id": "B8nvAQxhNAcH"
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup 6 of N: scraping tunables and feeds\n",
        "# Adjust only this cell to change scrape size and feed mix.\n",
        "\n",
        "import os, json\n",
        "\n",
        "# How many total articles to save this run across all feeds\n",
        "MAX_ARTICLES = 250\n",
        "\n",
        "# Optional hard cap per feed. Use None to remove the cap.\n",
        "MAX_PER_FEED = None\n",
        "\n",
        "# Only include items published on or after this date (ISO). Use \"\" to ignore.\n",
        "DATE_FROM = \"2025-07-01\"\n",
        "\n",
        "# If true, refetch even if the URL was seen already in index.json\n",
        "FORCE_REFETCH = False\n",
        "\n",
        "# If true, split evenly across feeds and send the remainder to QUOTA_REMAINDER_TO\n",
        "EVEN_SPLIT = True\n",
        "QUOTA_REMAINDER_TO = \"commentisfree\"\n",
        "\n",
        "# Guardian RSS feeds. Add or remove as needed.\n",
        "GUARDIAN_FEEDS = [\n",
        "    (\"world\",\"https://www.theguardian.com/world/rss\"),\n",
        "    (\"uk-news\",\"https://www.theguardian.com/uk-news/rss\"),\n",
        "    (\"us-news\",\"https://www.theguardian.com/us-news/rss\"),\n",
        "    (\"politics\",\"https://www.theguardian.com/politics/rss\"),\n",
        "    (\"environment\",\"https://www.theguardian.com/uk/environment/rss\"),\n",
        "    (\"climate-crisis\",\"https://www.theguardian.com/environment/climate-crisis/rss\"),\n",
        "    (\"technology\",\"https://www.theguardian.com/uk/technology/rss\"),\n",
        "    (\"science\",\"https://www.theguardian.com/science/rss\"),\n",
        "    (\"business\",\"https://www.theguardian.com/uk/business/rss\"),\n",
        "    (\"culture\",\"https://www.theguardian.com/uk/culture/rss\"),\n",
        "    (\"commentisfree\",\"https://www.theguardian.com/commentisfree/rss\")\n",
        "]\n",
        "\n",
        "# Export to environment so the scraper cell reads these values\n",
        "os.environ[\"MAX_ARTICLES\"] = str(MAX_ARTICLES)\n",
        "os.environ[\"MAX_PER_FEED\"] = \"\" if MAX_PER_FEED is None else str(MAX_PER_FEED)\n",
        "os.environ[\"DATE_FROM\"] = \"\" if not DATE_FROM else DATE_FROM\n",
        "os.environ[\"FORCE_REFETCH\"] = \"true\" if FORCE_REFETCH else \"false\"\n",
        "os.environ[\"EVEN_SPLIT\"] = \"true\" if EVEN_SPLIT else \"false\"\n",
        "os.environ[\"QUOTA_REMAINDER_TO\"] = QUOTA_REMAINDER_TO\n",
        "os.environ[\"GUARDIAN_FEEDS_JSON\"] = json.dumps(GUARDIAN_FEEDS)\n",
        "\n",
        "print(\"Feeds configured:\", len(GUARDIAN_FEEDS))\n",
        "print(\"MAX_ARTICLES:\", MAX_ARTICLES, \"MAX_PER_FEED:\", MAX_PER_FEED, \"DATE_FROM:\", DATE_FROM)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "cFlG77sFNAzu",
        "outputId": "d181e7b8-1a99-4362-8774-997c2a5f62ec"
      },
      "id": "cFlG77sFNAzu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "An instance of Chroma already exists for /content/anti_echo/chroma_db with different settings",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1632808827.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Skipping batch with no paths: {b.get('batch_id')}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m \u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_coll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstance_coll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_chroma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1632808827.py\u001b[0m in \u001b[0;36mensure_chroma\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mensure_chroma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchromadb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPersistentClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHROMA_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mtopic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_or_create_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCOLL_TOPIC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"hnsw:space\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"cosine\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_or_create_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCOLL_STANCE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"hnsw:space\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"cosine\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/chromadb/__init__.py\u001b[0m in \u001b[0;36mPersistentClient\u001b[0;34m(path, settings, tenant, database)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mdatabase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mClientCreator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtenant\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtenant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/chromadb/api/client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tenant, database, settings)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0msettings\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSettings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSettings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     ) -> None:\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtenant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtenant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatabase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/chromadb/api/shared_system_client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, settings)\u001b[0m\n\u001b[1;32m     17\u001b[0m     ) -> None:\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_identifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSharedSystemClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_identifier_from_settings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mSharedSystemClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_system_if_not_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_identifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/chromadb/api/shared_system_client.py\u001b[0m in \u001b[0;36m_create_system_if_not_exists\u001b[0;34m(cls, identifier, settings)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;31m# For now, the settings must match\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mprevious_system\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m     39\u001b[0m                     \u001b[0;34mf\"An instance of Chroma already exists for {identifier} with different settings\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 )\n",
            "\u001b[0;31mValueError\u001b[0m: An instance of Chroma already exists for /content/anti_echo/chroma_db with different settings"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup 7 of N — Guardian scraper with dedupe\n",
        "\n",
        "Purpose\n",
        "Scrape Guardian feeds, skip URLs already in feeds/index.json, save raw/{id}.txt and {id}.meta.json, and update both feeds/index.json and feeds/feeds_state.json.\n",
        "\n",
        "Why this matters\n",
        "Prevents duplicates, keeps state consistent across runs, and prepares clean inputs for embedding.\n",
        "\n",
        "Outputs\n",
        "New articles saved under raw/, updated feeds/index.json and feeds/feeds_state.json, and a summary."
      ],
      "metadata": {
        "id": "AR-rVGInQqeZ"
      },
      "id": "AR-rVGInQqeZ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup 7 of N: scraper with dedupe\n",
        "# Reads tunables from env and updates local index and feed state.\n",
        "\n",
        "import os, re, json, hashlib, datetime as dt\n",
        "from pathlib import Path\n",
        "from urllib.parse import urlparse\n",
        "from email.utils import parsedate_to_datetime\n",
        "import feedparser, trafilatura\n",
        "\n",
        "PROJECT_ROOT = Path(\"/content/anti_echo\").resolve()\n",
        "RAW_DIR = PROJECT_ROOT / \"raw\"\n",
        "FEEDS_DIR = PROJECT_ROOT / \"feeds\"\n",
        "FEEDS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "INDEX_PATH = FEEDS_DIR / \"index.json\"\n",
        "STATE_PATH = FEEDS_DIR / \"feeds_state.json\"\n",
        "\n",
        "# Load tunables from previous cell via environment\n",
        "GUARDIAN_FEEDS = json.loads(os.environ[\"GUARDIAN_FEEDS_JSON\"])\n",
        "MAX_ARTICLES = int(os.environ.get(\"MAX_ARTICLES\", \"30\"))\n",
        "MAX_PER_FEED = os.environ.get(\"MAX_PER_FEED\",\"\")\n",
        "MAX_PER_FEED = None if MAX_PER_FEED == \"\" else int(MAX_PER_FEED)\n",
        "DATE_FROM = os.environ.get(\"DATE_FROM\",\"\") or None\n",
        "FORCE_REFETCH = os.environ.get(\"FORCE_REFETCH\",\"false\").lower() == \"true\"\n",
        "EVEN_SPLIT = os.environ.get(\"EVEN_SPLIT\",\"true\").lower() == \"true\"\n",
        "QUOTA_REMAINDER_TO = os.environ.get(\"QUOTA_REMAINDER_TO\",\"commentisfree\")\n",
        "\n",
        "def now_iso(): return dt.datetime.now(dt.timezone.utc).isoformat()\n",
        "\n",
        "def load_index():\n",
        "    # Index tracks URL level status so we do not re scrape\n",
        "    if INDEX_PATH.exists():\n",
        "        try:\n",
        "            return json.loads(INDEX_PATH.read_text(encoding=\"utf-8\"))\n",
        "        except Exception:\n",
        "            pass\n",
        "    return {\"last_updated\": None, \"items\": {}}\n",
        "\n",
        "def save_index(idx):\n",
        "    idx[\"last_updated\"] = now_iso()\n",
        "    INDEX_PATH.write_text(json.dumps(idx, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "def load_state():\n",
        "    # feeds_state holds per feed ring buffers and timestamps\n",
        "    if STATE_PATH.exists():\n",
        "        try:\n",
        "            return json.loads(STATE_PATH.read_text(encoding=\"utf-8\"))\n",
        "        except Exception:\n",
        "            pass\n",
        "    return {\"version\":1,\"updated_at\":None,\"feeds\":{}}\n",
        "\n",
        "index = load_index()\n",
        "feeds_state = load_state()\n",
        "\n",
        "# Initialize per feed state if missing\n",
        "fs = feeds_state.setdefault(\"feeds\", {})\n",
        "for name, feed_url in GUARDIAN_FEEDS:\n",
        "    fs.setdefault(name, {\n",
        "        \"feed_url\": feed_url,\n",
        "        \"last_cursor_iso\": None,\n",
        "        \"recent_url_hashes\": [],\n",
        "        \"recent_url_hashes_max\": 1000 if name == \"commentisfree\" else 500,\n",
        "        \"last_run_at\": None,\n",
        "        \"last_run_by\": \"colab\"\n",
        "    })\n",
        "\n",
        "def parse_date(entry):\n",
        "    # Robustly pull a published or updated date from a feed entry\n",
        "    for k in [\"published\",\"updated\"]:\n",
        "        val = getattr(entry, k, None) or entry.get(k)\n",
        "        if val:\n",
        "            try: return parsedate_to_datetime(val)\n",
        "            except Exception: pass\n",
        "    return None\n",
        "\n",
        "def in_range(d, lower_iso):\n",
        "    # Filter by DATE_FROM if provided\n",
        "    if not lower_iso: return True\n",
        "    try: floor = dt.datetime.fromisoformat(lower_iso).replace(tzinfo=dt.timezone.utc)\n",
        "    except Exception: return True\n",
        "    if d is None: return True\n",
        "    if d.tzinfo is None: d = d.replace(tzinfo=dt.timezone.utc)\n",
        "    return d >= floor\n",
        "\n",
        "def normalize_text(txt): return re.sub(r\"\\s+\",\" \", txt.strip().lower())\n",
        "def sha256_text(txt): return hashlib.sha256(txt.encode()).hexdigest()\n",
        "\n",
        "def slugify(text, maxlen=60):\n",
        "    # Safe filename slug from title\n",
        "    s = re.sub(r\"[^a-zA-Z0-9]+\",\"-\", text).strip(\"-\").lower()\n",
        "    return s[:maxlen] or \"untitled\"\n",
        "\n",
        "def get_title(html, fallback=\"Untitled\"):\n",
        "    # Simple HTML title extraction as a fallback\n",
        "    m = re.search(r\"<title>(.*?)</title>\", html or \"\", flags=re.I|re.S)\n",
        "    return re.sub(r\"\\s+\",\" \", m.group(1)).strip() if m else fallback\n",
        "\n",
        "def fetch_article(url):\n",
        "    # Fetch HTML and extract readable main text\n",
        "    html = trafilatura.fetch_url(url, no_ssl=False)\n",
        "    if not html:\n",
        "        raise RuntimeError(\"fetch failed\")\n",
        "    text = trafilatura.extract(html, include_comments=False, include_tables=False) or \"\"\n",
        "    title = get_title(html, \"Untitled\")\n",
        "    if not text.strip():\n",
        "        raise RuntimeError(\"extraction empty\")\n",
        "    return title, text\n",
        "\n",
        "def already_cached(url):\n",
        "    # Check if index marks this URL as successfully processed\n",
        "    return url in index[\"items\"] and index[\"items\"][url].get(\"status\") == \"ok\"\n",
        "\n",
        "def mark(url, status):\n",
        "    # Record the outcome for this URL\n",
        "    index[\"items\"][url] = {\"status\": status, \"fetched_at\": now_iso()}\n",
        "    save_index(index)\n",
        "\n",
        "# Compute quotas so we do not oversample a single feed\n",
        "feed_names = [n for n,_ in GUARDIAN_FEEDS]\n",
        "if EVEN_SPLIT:\n",
        "    base = MAX_ARTICLES // len(feed_names)\n",
        "    rem = MAX_ARTICLES % len(feed_names)\n",
        "    quotas = {n: base for n in feed_names}\n",
        "    quotas[QUOTA_REMAINDER_TO if QUOTA_REMAINDER_TO in quotas else feed_names[0]] += rem\n",
        "else:\n",
        "    quotas = {n: 0 for n in feed_names}\n",
        "    quotas[QUOTA_REMAINDER_TO if QUOTA_REMAINDER_TO in quotas else feed_names[0]] = MAX_ARTICLES\n",
        "if isinstance(MAX_PER_FEED, int):\n",
        "    quotas = {k: min(v, MAX_PER_FEED) for k,v in quotas.items()}\n",
        "print(\"Quotas:\", quotas)\n",
        "\n",
        "saved_global = 0\n",
        "errors_global = 0\n",
        "seen_global = set()  # guard against duplicates across feeds\n",
        "\n",
        "for name, feed_url in GUARDIAN_FEEDS:\n",
        "    if saved_global >= MAX_ARTICLES:\n",
        "        break\n",
        "    quota = quotas.get(name, 0)\n",
        "    if quota <= 0:\n",
        "        continue\n",
        "\n",
        "    # Parse RSS\n",
        "    fp = feedparser.parse(feed_url)\n",
        "    items = []\n",
        "    for e in fp.entries:\n",
        "        url = getattr(e, \"link\", None)\n",
        "        if not url:\n",
        "            continue\n",
        "        pub = parse_date(e)\n",
        "        if in_range(pub, DATE_FROM):\n",
        "            items.append({\"url\": url, \"published\": pub})\n",
        "\n",
        "    # De duplicate and order newest first\n",
        "    uniq = []\n",
        "    seen = set()\n",
        "    for it in sorted(items, key=lambda x: (x[\"published\"] or dt.datetime.min), reverse=True):\n",
        "        if it[\"url\"] in seen:\n",
        "            continue\n",
        "        seen.add(it[\"url\"])\n",
        "        uniq.append(it)\n",
        "\n",
        "    saved_this = 0\n",
        "    for it in uniq:\n",
        "        if saved_global >= MAX_ARTICLES or saved_this >= quota:\n",
        "            break\n",
        "        url = it[\"url\"]\n",
        "        if url in seen_global:\n",
        "            continue\n",
        "        seen_global.add(url)\n",
        "\n",
        "        if already_cached(url) and not FORCE_REFETCH:\n",
        "            print(f\"skip (cached) [{name}]: {url}\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Fetch and extract\n",
        "            title, text = fetch_article(url)\n",
        "\n",
        "            # Build deterministic id using domain, title slug, and text hash\n",
        "            domain = urlparse(url).netloc\n",
        "            slug = slugify(title)\n",
        "            h = sha256_text(normalize_text(text))\n",
        "            art_id = f\"{domain}-{slug}-{h[:12]}\"\n",
        "\n",
        "            # Save artifacts\n",
        "            txt_path = RAW_DIR / f\"{art_id}.txt\"\n",
        "            meta_path = RAW_DIR / f\"{art_id}.meta.json\"\n",
        "            txt_path.write_text(text, encoding=\"utf-8\")\n",
        "            meta = {\n",
        "                \"id\": art_id,\n",
        "                \"url\": url,\n",
        "                \"title\": title,\n",
        "                \"source\": \"theguardian\",\n",
        "                \"section\": name,\n",
        "                \"domain\": domain,\n",
        "                \"published\": it[\"published\"].isoformat() if it[\"published\"] else None,\n",
        "                \"sha256\": h,\n",
        "                \"chars\": len(text),\n",
        "                \"saved_at\": now_iso()\n",
        "            }\n",
        "            meta_path.write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "            # Update indexes\n",
        "            mark(url, \"ok\")\n",
        "            ring = fs[name][\"recent_url_hashes\"]\n",
        "            ring.append(sha256_text(url)[:12])\n",
        "            if len(ring) > fs[name][\"recent_url_hashes_max\"]:\n",
        "                fs[name][\"recent_url_hashes\"] = ring[-fs[name][\"recent_url_hashes_max\"]:]\n",
        "            fs[name][\"last_run_at\"] = now_iso()\n",
        "\n",
        "            saved_this += 1\n",
        "            saved_global += 1\n",
        "            print(f\"saved [{name}]: {txt_path.name} | {title[:90]}\")\n",
        "        except Exception as e:\n",
        "            # Log error and move on\n",
        "            mark(url, \"error\")\n",
        "            errors_global += 1\n",
        "            print(f\"error [{name}]: {url} | {type(e).__name__}: {str(e)[:140]}\")\n",
        "\n",
        "# Persist the feeds_state after the run\n",
        "feeds_state[\"updated_at\"] = now_iso()\n",
        "STATE_PATH.write_text(json.dumps(feeds_state, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "# Final summary\n",
        "print(json.dumps({\n",
        "    \"saved_total\": saved_global,\n",
        "    \"errors_total\": errors_global,\n",
        "    \"index_items\": len(json.loads(INDEX_PATH.read_text()).get(\"items\",{})),\n",
        "    \"feeds_state_path\": str(STATE_PATH)\n",
        "}, indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAULiUriQr9n",
        "outputId": "8da6f8c0-9c50-4ecf-ad13-9d496779bdb1"
      },
      "id": "wAULiUriQr9n",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tunables and Guardian feeds set.\n",
            "Feeds configured: 42\n",
            "MAX_ARTICLES=250, MAX_PER_FEED=None, DATE_FROM=2025-07-01, EVEN_SPLIT=True, REMAINDER_TO=commentisfree\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup 8 of N — persist feed state to HF and GitHub\n",
        "\n",
        "Purpose\n",
        "Snapshot feeds/feeds_state.json and feeds/index.json to HF as timestamped copies and latest pointers, and commit the same to GitHub. This keeps HF as the single source of truth while providing Git history.\n",
        "\n",
        "Why this matters\n",
        "Future runs and collaborators can always restore state. The UI you build later can also read the latest pointers.\n",
        "\n",
        "Outputs\n",
        "Four files on HF and two files in GitHub updated, with a short summary."
      ],
      "metadata": {
        "id": "LSRhQ_EAnuCQ"
      },
      "id": "LSRhQ_EAnuCQ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup 8 of N: persist feed state to HF and GitHub\n",
        "# Uploads timestamped snapshots and maintains latest pointers on HF and GitHub.\n",
        "\n",
        "import os, json, base64\n",
        "from datetime import datetime, timezone\n",
        "from pathlib import Path\n",
        "from huggingface_hub import upload_file\n",
        "import requests\n",
        "from getpass import getpass\n",
        "\n",
        "PROJECT_ROOT = Path(\"/content/anti_echo\").resolve()\n",
        "FEEDS_DIR = PROJECT_ROOT / \"feeds\"\n",
        "STATE_PATH = FEEDS_DIR / \"feeds_state.json\"\n",
        "INDEX_PATH = FEEDS_DIR / \"index.json\"\n",
        "\n",
        "# Inputs\n",
        "HF_DATASET_ID = os.environ[\"HF_DATASET_ID\"]\n",
        "HF_TOKEN = os.environ.get(\"HF_TOKEN\",\"\").strip() or getpass(\"Enter HF token: \")\n",
        "GITHUB_TOKEN = os.environ.get(\"GITHUB_TOKEN\",\"\").strip() or getpass(\"Enter GitHub token: \")\n",
        "REPO_OWNER = \"AHMerrill\"\n",
        "REPO_NAME = \"anti-echo-chamber\"\n",
        "BRANCH = \"main\"\n",
        "\n",
        "# Prepare timestamped names plus stable latest names\n",
        "ts = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
        "uploads = [\n",
        "    (STATE_PATH, f\"feeds/feeds_state_{ts}.json\"),\n",
        "    (INDEX_PATH, f\"feeds/feed_index_{ts}.json\"),\n",
        "    (STATE_PATH, \"feeds/feeds_state_latest.json\"),\n",
        "    (INDEX_PATH, \"feeds/feed_index_latest.json\"),\n",
        "]\n",
        "\n",
        "# Upload to HF dataset\n",
        "print(\"Uploading feed state to HF...\")\n",
        "for local, remote in uploads:\n",
        "    upload_file(\n",
        "        path_or_fileobj=str(local),\n",
        "        path_in_repo=remote,\n",
        "        repo_id=HF_DATASET_ID,\n",
        "        repo_type=\"dataset\",\n",
        "        token=HF_TOKEN\n",
        "    )\n",
        "print(\"HF upload complete\")\n",
        "\n",
        "# Helper to upsert files in GitHub repo via REST API\n",
        "def gh_put(local_path: Path, repo_path: str, message: str):\n",
        "    url = f\"https://api.github.com/repos/{REPO_OWNER}/{REPO_NAME}/contents/{repo_path}\"\n",
        "    headers = {\"Authorization\": f\"Bearer {GITHUB_TOKEN}\", \"Accept\": \"application/vnd.github+json\"}\n",
        "    content = local_path.read_bytes()\n",
        "    # Fetch existing sha to update file in place if it already exists\n",
        "    r = requests.get(url, headers=headers, timeout=20)\n",
        "    sha = r.json().get(\"sha\") if r.status_code == 200 else None\n",
        "    payload = {\"message\": message, \"content\": base64.b64encode(content).decode(), \"branch\": BRANCH}\n",
        "    if sha:\n",
        "        payload[\"sha\"] = sha\n",
        "    resp = requests.put(url, headers=headers, json=payload, timeout=30)\n",
        "    if resp.status_code not in (200,201):\n",
        "        raise RuntimeError(f\"GitHub push failed for {repo_path}: {resp.status_code} {resp.text[:300]}\")\n",
        "\n",
        "print(\"Committing feed state to GitHub...\")\n",
        "commit_msg = f\"Update feed state and index {ts}\"\n",
        "for local, repo_path in [\n",
        "    (STATE_PATH, f\"feeds/feeds_state_{ts}.json\"),\n",
        "    (INDEX_PATH, f\"feeds/feed_index_{ts}.json\"),\n",
        "    (STATE_PATH, \"feeds/feeds_state_latest.json\"),\n",
        "    (INDEX_PATH, \"feeds/feed_index_latest.json\"),\n",
        "]:\n",
        "    gh_put(local, repo_path, commit_msg)\n",
        "print(\"GitHub commit complete\")\n"
      ],
      "metadata": {
        "id": "iB3izd-Rv1WV",
        "outputId": "21b67c06-22a1-45ed-81eb-128ea72b00ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "id": "iB3izd-Rv1WV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Found no downloadable metadata files in registry.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-457354459.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmeta_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Found no downloadable metadata files in registry.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m# Build URL set and basic meta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no downloadable metadata files in registry."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup 9 of N — topic embeddings to Chroma\n",
        "\n",
        "Purpose\n",
        "Generate multi topic embeddings per article and upsert into the topic collection. This models what the article is about. Each article may get multiple topic vectors.\n",
        "\n",
        "Why this matters\n",
        "Topical neighbors power the first half of contrastive retrieval.\n",
        "\n",
        "Outputs\n",
        "Vectors upserted into the topic collection with structured metadata."
      ],
      "metadata": {
        "id": "o-xZkiif5ajG"
      },
      "id": "o-xZkiif5ajG"
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup 9 of N: topic embeddings\n",
        "\n",
        "import json, time, numpy as np, nltk, torch\n",
        "from pathlib import Path\n",
        "from transformers import AutoTokenizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import chromadb\n",
        "\n",
        "PROJECT_ROOT = Path(\"/content/anti_echo\").resolve()\n",
        "RAW_DIR = PROJECT_ROOT / \"raw\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "topic_model_name = CONFIG[\"embeddings\"][\"topic_model\"]\n",
        "topic_dim = int(CONFIG[\"embeddings\"][\"dim\"])\n",
        "topic_dtype = CONFIG[\"embeddings\"][\"dtype\"]\n",
        "chunk_tokens = int(CONFIG[\"embeddings\"][\"chunk_tokens\"])\n",
        "\n",
        "# Ensure NLTK data\n",
        "for pkg in [\"punkt\",\"punkt_tab\"]:\n",
        "    try: nltk.data.find(f\"tokenizers/{pkg}\")\n",
        "    except LookupError: nltk.download(pkg)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(topic_model_name, use_fast=True)\n",
        "embedder = SentenceTransformer(topic_model_name, device=device)\n",
        "\n",
        "# Get topic collection handle from persistent Chroma\n",
        "client = chromadb.PersistentClient(path=str(PROJECT_ROOT / CONFIG[\"chroma\"][\"dir\"]))\n",
        "topic_coll = client.get_collection(CONFIG[\"chroma_collections\"][\"topic\"])\n",
        "\n",
        "def sent_split(text):\n",
        "    # Use NLTK to split text into sentences for clustering\n",
        "    return [s.strip() for s in nltk.sent_tokenize(text) if s.strip()]\n",
        "\n",
        "def encode(texts, batch=16):\n",
        "    # Batch encode using SentenceTransformer\n",
        "    return embedder.encode(texts, convert_to_numpy=True, batch_size=batch, show_progress_bar=False)\n",
        "\n",
        "def chunk_by_tokens(text, max_tokens=512, overlap=64):\n",
        "    # Token chunking to keep each chunk within model context\n",
        "    ids = tokenizer(text, add_special_tokens=False, return_attention_mask=False)[\"input_ids\"]\n",
        "    step = max_tokens - overlap\n",
        "    chunks = []\n",
        "    for i in range(0, len(ids), step):\n",
        "        j = min(i+max_tokens, len(ids))\n",
        "        piece = tokenizer.decode(ids[i:j], skip_special_tokens=True)\n",
        "        if piece.strip():\n",
        "            chunks.append(piece)\n",
        "    return chunks\n",
        "\n",
        "def sanitize(meta: dict):\n",
        "    # Ensure metadata fields are JSON serializable primitives\n",
        "    out = {}\n",
        "    for k,v in meta.items():\n",
        "        if isinstance(v,(str,int,float,bool)) or v is None:\n",
        "            out[k] = \"\" if v is None else v\n",
        "        else:\n",
        "            out[k] = str(v)\n",
        "    return out\n",
        "\n",
        "def topic_vecs(text):\n",
        "    # Produce 1 to 8 topic vectors by clustering sentence embeddings, then mean pooling token chunks\n",
        "    sents = sent_split(text)\n",
        "    if not sents:\n",
        "        return []\n",
        "    if len(sents) < 2:\n",
        "        v = encode([\" \".join(sents)])[0]\n",
        "        return [v.astype(np.float16) if topic_dtype == \"float16\" else v.astype(np.float32)]\n",
        "    emb = encode(sents)\n",
        "    k = min(max(1, len(sents)//8), 8)\n",
        "    labels = AgglomerativeClustering(n_clusters=k).fit_predict(emb)\n",
        "    segs = [\" \".join([s for s,l in zip(sents, labels) if l == lab]) for lab in sorted(set(labels))]\n",
        "    out = []\n",
        "    for seg in segs:\n",
        "        ids = tokenizer(seg, add_special_tokens=False)[\"input_ids\"][:512]\n",
        "        seg_trunc = tokenizer.decode(ids, skip_special_tokens=True)\n",
        "        chunks = chunk_by_tokens(seg_trunc, chunk_tokens, 64)\n",
        "        if not chunks:\n",
        "            continue\n",
        "        pooled = encode(chunks).mean(axis=0)\n",
        "        out.append(pooled.astype(np.float16) if topic_dtype == \"float16\" else pooled.astype(np.float32))\n",
        "    return out\n",
        "\n",
        "def upsert_in_chunks(collection, ids, vectors, metadatas, chunk=2048):\n",
        "    # Chroma bulk upsert helper\n",
        "    n = len(ids)\n",
        "    for i in range(0, n, chunk):\n",
        "        j = min(i + chunk, n)\n",
        "        collection.upsert(\n",
        "            ids=ids[i:j],\n",
        "            embeddings=vectors[i:j].tolist(),\n",
        "            metadatas=metadatas[i:j],\n",
        "        )\n",
        "\n",
        "start = time.time()\n",
        "added = 0\n",
        "\n",
        "for txt_path in RAW_DIR.glob(\"*.txt\"):\n",
        "    meta_path = txt_path.with_suffix(\".meta.json\")\n",
        "    if not meta_path.exists():\n",
        "        continue\n",
        "    text = txt_path.read_text(encoding=\"utf-8\").strip()\n",
        "    if not text:\n",
        "        continue\n",
        "    meta = json.loads(meta_path.read_text(encoding=\"utf-8\"))\n",
        "\n",
        "    vecs = topic_vecs(text)\n",
        "    if not vecs:\n",
        "        continue\n",
        "\n",
        "    ids = [f\"{meta['id']}::topic::{i}\" for i in range(len(vecs))]\n",
        "    metas = [sanitize({**meta, \"topic_index\": i, \"topic_model\": topic_model_name}) for i in range(len(vecs))]\n",
        "    upsert_in_chunks(topic_coll, ids, np.vstack(vecs), metas)\n",
        "    added += len(vecs)\n",
        "\n",
        "print(f\"Topic upserts: {added} in {round(time.time()-start,2)}s\")\n"
      ],
      "metadata": {
        "id": "pV8CM5ZlnvVl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0aa21f6-7aa4-4170-88ca-8d52c96107d6"
      },
      "id": "pV8CM5ZlnvVl",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HF restore not available: EntryNotFoundError: 404 Client Error. (Request ID: Root=1-68eb32ef-5aa20d691554c8a514566010;5837e9de-b70c-4d5e-a8a4-714a1a262107)\n",
            "\n",
            "Entry Not Found for url: https://huggingface.co/datasets/zanimal/anti-echo-artifacts/resolve/main/feeds/feeds_state_latest.json.\n",
            "GitHub latest feed state not found.\n",
            "No prior feed state found. Starting fresh.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup 10 of N — stance embeddings to Chroma\n",
        "\n",
        "Purpose\n",
        "Summarize each article, embed the summary, and upsert one stance vector per article. This models how the article argues. The stance space will support later contrast queries.\n",
        "\n",
        "Why this matters\n",
        "Enables similarity in topic while allowing opposition in stance during retrieval.\n",
        "\n",
        "Outputs\n",
        "One stance vector per article upserted with the summary stored in metadata."
      ],
      "metadata": {
        "id": "7lHNRC8VZ28B"
      },
      "id": "7lHNRC8VZ28B"
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup 10 of N: stance embeddings\n",
        "\n",
        "import json, time, numpy as np, torch\n",
        "from pathlib import Path\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "\n",
        "PROJECT_ROOT = Path(\"/content/anti_echo\").resolve()\n",
        "RAW_DIR = PROJECT_ROOT / \"raw\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "stance_model_name = CONFIG[\"embeddings\"][\"stance_model\"]\n",
        "stance_dtype = CONFIG[\"embeddings\"][\"dtype\"]\n",
        "summarizer_name = CONFIG.get(\"summarizer\",{}).get(\"model\",\"facebook/bart-large-cnn\") if torch.cuda.is_available() else \"sshleifer/distilbart-cnn-12-6\"\n",
        "\n",
        "tok_sum = AutoTokenizer.from_pretrained(summarizer_name)\n",
        "model_sum = AutoModelForSeq2SeqLM.from_pretrained(summarizer_name).to(device)\n",
        "embedder = SentenceTransformer(stance_model_name, device=device)\n",
        "\n",
        "client = chromadb.PersistentClient(path=str(PROJECT_ROOT / CONFIG[\"chroma\"][\"dir\"]))\n",
        "stance_coll = client.get_collection(CONFIG[\"chroma_collections\"][\"stance\"])\n",
        "\n",
        "def sanitize(meta: dict):\n",
        "    out = {}\n",
        "    for k,v in meta.items():\n",
        "        if isinstance(v,(str,int,float,bool)) or v is None:\n",
        "            out[k] = \"\" if v is None else v\n",
        "        else:\n",
        "            out[k] = str(v)\n",
        "    return out\n",
        "\n",
        "def summarize(text, max_in=1024, max_out=150):\n",
        "    # Safe summarization with truncation to control runtime and memory\n",
        "    inputs = tok_sum([text], return_tensors=\"pt\", truncation=True, max_length=max_in).to(device)\n",
        "    with torch.no_grad():\n",
        "        out = model_sum.generate(**inputs, max_length=max_out, num_beams=4, early_stopping=True)\n",
        "    return tok_sum.batch_decode(out, skip_special_tokens=True)[0].strip()\n",
        "\n",
        "added = 0\n",
        "skipped = 0\n",
        "\n",
        "for txt_path in RAW_DIR.glob(\"*.txt\"):\n",
        "    meta_path = txt_path.with_suffix(\".meta.json\")\n",
        "    if not meta_path.exists():\n",
        "        continue\n",
        "    meta = json.loads(meta_path.read_text(encoding=\"utf-8\"))\n",
        "    text = txt_path.read_text(encoding=\"utf-8\").strip()\n",
        "    if not text:\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        summary = summarize(text)\n",
        "        if not summary:\n",
        "            skipped += 1\n",
        "            continue\n",
        "        vec = embedder.encode([summary], convert_to_numpy=True)[0]\n",
        "        vec = vec.astype(np.float16) if stance_dtype == \"float16\" else vec.astype(np.float32)\n",
        "        ids = [f\"{meta['id']}::stance::0\"]\n",
        "        metas = [sanitize({**meta, \"stance_summary\": summary, \"stance_model\": stance_model_name, \"summary_model\": summarizer_name})]\n",
        "        stance_coll.upsert(ids=ids, embeddings=[vec.tolist()], metadatas=metas)\n",
        "        added += 1\n",
        "    except Exception as e:\n",
        "        skipped += 1\n",
        "        print(\"skip:\", meta.get(\"id\"), e)\n",
        "\n",
        "print({\"stance_upserts\": added, \"skipped\": skipped})\n"
      ],
      "metadata": {
        "id": "uHrefNpJZ4L7",
        "outputId": "a89c6c1c-e21b-482c-9311-8bd56de611ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "uHrefNpJZ4L7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Per feed quotas for this run:\n",
            "{\n",
            "  \"world\": 5,\n",
            "  \"uk-news\": 5,\n",
            "  \"us-news\": 5,\n",
            "  \"politics\": 5,\n",
            "  \"europe\": 5,\n",
            "  \"americas\": 5,\n",
            "  \"asia\": 5,\n",
            "  \"australia-news\": 5,\n",
            "  \"business\": 5,\n",
            "  \"money\": 5,\n",
            "  \"technology\": 5,\n",
            "  \"science\": 5,\n",
            "  \"global-development\": 5,\n",
            "  \"environment\": 5,\n",
            "  \"wildlife\": 5,\n",
            "  \"pollution\": 5,\n",
            "  \"climate-crisis\": 5,\n",
            "  \"sport\": 5,\n",
            "  \"football\": 5,\n",
            "  \"cricket\": 5,\n",
            "  \"tennis\": 5,\n",
            "  \"golf\": 5,\n",
            "  \"formulaone\": 5,\n",
            "  \"cycling\": 5,\n",
            "  \"rugby-union\": 5,\n",
            "  \"culture\": 5,\n",
            "  \"film\": 5,\n",
            "  \"music\": 5,\n",
            "  \"artanddesign\": 5,\n",
            "  \"books\": 5,\n",
            "  \"tv-and-radio\": 5,\n",
            "  \"lifestyle\": 5,\n",
            "  \"family\": 5,\n",
            "  \"health\": 5,\n",
            "  \"inequality\": 5,\n",
            "  \"obituaries\": 5,\n",
            "  \"travel\": 5,\n",
            "  \"fashion\": 5,\n",
            "  \"games\": 5,\n",
            "  \"stage\": 5,\n",
            "  \"crosswords\": 5,\n",
            "  \"commentisfree\": 45\n",
            "}\n",
            "saved [world]: www.theguardian.com-egypt-confirms-international-leaders-summit-on-monday-to-dis-0efa690f2f6e.txt | Egypt confirms international leaders’ summit on Monday to discuss Gaza ceasefire - live | \n",
            "saved [world]: www.theguardian.com-trump-says-military-members-will-be-paid-despite-government--69a05b3f6a9f.txt | Trump says military members will be paid despite government shutdown | US federal governme\n",
            "saved [world]: www.theguardian.com-chimamanda-ngozi-adichie-says-she-is-terrified-her-sons-will-7fa91e986bc1.txt | Chimamanda Ngozi Adichie says she is terrified her sons will ‘join manosphere’ | Chimamand\n",
            "saved [world]: www.theguardian.com-six-people-dead-with-multiple-injured-in-three-shootings-at--5f3eed5f0c5a.txt | Six people dead with multiple injured in three shootings at Mississippi homecoming weekend\n",
            "saved [world]: www.theguardian.com-no-survivors-in-tennessee-explosives-factory-blast-officials-c35ecb4e3c8c.txt | No survivors in Tennessee explosives factory blast, officials say | Tennessee | The Guardi\n",
            "saved [uk-news]: www.theguardian.com-two-men-arrested-after-lostprophets-singer-ian-watkins-dies--d313ec049af9.txt | Two men arrested after Lostprophets singer Ian Watkins dies in prison attack | UK news | T\n",
            "saved [uk-news]: www.theguardian.com-snp-backs-swinney-s-clear-strategy-for-new-independence-refe-1e83bf492422.txt | SNP backs Swinney’s ‘clear’ strategy for new independence referendum | Scottish National p\n",
            "saved [uk-news]: www.theguardian.com-thousands-gather-in-london-in-march-for-lasting-peace-in-gaz-62b3c83abc3e.txt | Thousands gather in London in march for ‘lasting peace’ in Gaza | Israel-Gaza war | The Gu\n",
            "saved [uk-news]: www.theguardian.com-tony-blair-met-jeffrey-epstein-in-no-10-on-advice-of-peter-m-885a8c846dd5.txt | Tony Blair met Jeffrey Epstein in No 10 on advice of Peter Mandelson, documents reveal | T\n",
            "saved [uk-news]: www.theguardian.com-revealed-labour-run-council-using-legal-loophole-to-serve-fa-c900b7ea898d.txt | Revealed: Labour-run council using legal loophole to serve families with no-fault eviction\n",
            "saved [us-news]: www.theguardian.com-national-guard-begins-memphis-patrols-as-senators-in-illinoi-5a397dac782a.txt | National guard begins Memphis patrols as senators in Illinois are turned away from Ice fac\n",
            "saved [us-news]: www.theguardian.com-the-peerless-a-ja-wilson-may-already-be-the-wnba-s-greatest--62527310cfc0.txt | The peerless A’ja Wilson may already be the WNBA’s greatest ever player | Las Vegas Aces |\n",
            "saved [us-news]: www.theguardian.com-before-trump-dreamers-were-shielded-from-deportation-here-s--6a0f39d2dbc5.txt | Before Trump, ‘Dreamers’ were shielded from deportation. Here’s what’s changed | US immigr\n",
            "saved [us-news]: www.theguardian.com-a-new-frontier-why-aussies-are-coming-to-mls-for-soccer-oppo-9a1c25d98332.txt | A new frontier: why Aussies are coming to MLS for soccer opportunity | Australia | The Gua\n",
            "saved [us-news]: www.theguardian.com-joe-biden-receiving-radiation-therapy-for-prostate-cancer-jo-03f66a833b14.txt | Joe Biden receiving radiation therapy for prostate cancer | Joe Biden | The Guardian\n",
            "saved [politics]: www.theguardian.com-madeline-horwath-on-st-george-returning-to-england-cartoon-m-83632ba81f7d.txt | Madeline Horwath on St George returning to England – cartoon | Madeline Horwath | The Guar\n",
            "saved [politics]: www.theguardian.com-don-t-fall-for-the-authoritarian-hype-reform-and-the-hard-ri-d6f6a528f5f1.txt | Don’t fall for the authoritarian hype – Reform and the hard right can be stopped in their \n",
            "saved [politics]: www.theguardian.com-tony-blair-and-nick-clegg-hosted-dinner-giving-tech-bosses-a-b6165dce5782.txt | Tony Blair and Nick Clegg hosted dinner giving tech bosses access to UK minister | Tony Bl\n",
            "saved [politics]: www.theguardian.com-rachel-reeves-v-the-obr-chancellor-aims-to-loosen-the-watchd-5730d48a7d0e.txt | Rachel Reeves v the OBR: chancellor aims to loosen the watchdog’s grip | Economics | The G\n",
            "saved [politics]: www.theguardian.com-rachel-reeves-looks-for-extra-headroom-in-budget-to-insulate-4fabf4fe5c92.txt | Rachel Reeves looks for extra headroom in budget to insulate UK economy against bond marke\n",
            "saved [americas]: www.theguardian.com-who-is-mar-a-corina-machado-venezuela-s-iron-lady-and-new-no-9b835d3874e0.txt | Who is María Corina Machado,'Venezuela’s Iron Lady' and new Nobel peace prize winner? – vi\n",
            "saved [americas]: www.theguardian.com-venezuelan-politician-mar-a-corina-machado-wins-nobel-peace--485310474908.txt | Venezuelan politician María Corina Machado wins Nobel peace prize | Nobel peace prize | Th\n",
            "saved [americas]: www.theguardian.com-peru-lawmakers-vote-to-oust-president-dina-boluarte-over-cri-c39ff21a0824.txt | Peru lawmakers vote to oust president Dina Boluarte over crime crisis | Peru | The Guardia\n",
            "saved [americas]: www.theguardian.com-john-candy-i-like-me-review-starry-but-treacly-tribute-to-co-bf60a515b8c8.txt | John Candy: I Like Me review – starry but treacly tribute to comedy legend | Movies | The \n",
            "saved [americas]: www.theguardian.com-president-petro-accuses-us-of-killing-colombians-in-attacks--56467e8e9ad3.txt | President Petro accuses US of killing Colombians in attacks on ‘narco-boats’ | Colombia | \n",
            "saved [asia]: www.theguardian.com-china-issues-rewards-for-information-about-taiwan-military-s-35444eeb3e8c.txt | China issues rewards for information about Taiwan military’s ‘psychological warfare unit’ \n",
            "saved [asia]: www.theguardian.com-to-the-men-who-ran-the-world-i-was-just-a-photo-op-malala-yo-951355fae79d.txt | ‘To the men who ran the world, I was just a photo op’: Malala Yousafzai on growing up, get\n",
            "saved [asia]: www.theguardian.com-unease-at-slow-pace-of-change-in-nepal-one-month-on-from-gen-8e90cfc36802.txt | Unease at slow pace of change in Nepal one month on from gen Z protests | Nepal | The Guar\n",
            "saved [asia]: www.theguardian.com-seven-people-killed-after-twin-earthquakes-off-coast-of-phil-4c06d8035124.txt | Seven people killed after twin earthquakes off coast of Philippines | Earthquakes | The Gu\n",
            "saved [asia]: www.theguardian.com-weather-tracker-south-east-china-swelters-in-summer-like-hea-68c59aaa0bc9.txt | Weather tracker: South-east China swelters in summer-like heat | China | The Guardian\n",
            "saved [australia-news]: www.theguardian.com-australia-news-live-pro-palestine-rallies-to-continue-amid-o-a667330657c5.txt | Australia news live: pro-Palestine rallies to continue amid Opera House ban and ceasefire \n",
            "saved [australia-news]: www.theguardian.com-as-the-liberal-party-s-key-demographic-shuffles-off-how-can--eab7de08c3b4.txt | As the Liberal party’s key demographic shuffles off, how can Sussan Ley appeal to Australi\n",
            "saved [australia-news]: www.theguardian.com-adopting-a-she-ll-be-right-attitude-to-australian-politics-m-9f62d9265fe6.txt | Adopting a ‘she’ll be right’ attitude to Australian politics may be seductive, but it cert\n",
            "saved [australia-news]: www.theguardian.com-fossicking-for-a-fortune-the-price-of-gold-is-sky-high-and-p-c7f56823c4dd.txt | Fossicking for a fortune: the price of gold is sky-high, and prospectors in Victoria hope \n",
            "saved [australia-news]: www.theguardian.com-the-moment-i-knew-he-was-so-open-and-vulnerable-even-as-a-bl-d8194367a165.txt | The moment I knew: he was so open and vulnerable, even as a bloke’s bloke | Life and style\n",
            "saved [business]: www.theguardian.com-trump-is-obsessed-with-seeming-pro-worker-but-his-actions-su-f3b2f79a0a67.txt | Trump is ‘obsessed’ with seeming pro-worker – but his actions suggest otherwise | Donald T\n",
            "saved [business]: www.theguardian.com-widow-of-man-conned-out-of-pension-savings-to-lose-half-the--e55645a012a1.txt | Widow of man conned out of pension savings to lose half the compensation to tax | HMRC | T\n",
            "saved [business]: www.theguardian.com-do-we-need-imax-70mm-vistavision-all-i-need-to-watch-movies--3a4727c736b9.txt | Do we need Imax? 70mm? VistaVision? All I need to watch movies at the cinema is darkness a\n",
            "saved [business]: www.theguardian.com-london-has-turned-into-something-crazy-is-the-city-in-the-gr-50008635b0f5.txt | ‘London has turned into something crazy’: is the city in the grip of a crime wave? | Londo\n",
            "saved [business]: www.theguardian.com-fifa-seeks-advice-over-banning-league-games-staged-overseas--59111dfd8b60.txt | Fifa seeks advice over banning league games staged overseas amid regulations redraft | Fif\n",
            "saved [money]: www.theguardian.com-hits-the-nose-like-wasabi-the-best-and-worst-supermarket-eng-7978994d7512.txt | ‘Hits the nose like wasabi’: the best (and worst) supermarket English mustard, tasted and \n",
            "saved [money]: www.theguardian.com-what-price-loyalty-uk-supermarket-cards-rated-shopping-the-g-bbda9ae90172.txt | What price loyalty? UK supermarket cards rated | Shopping | The Guardian\n",
            "saved [money]: www.theguardian.com-tiktok-influencers-fuelling-parallel-market-for-unlicensed-w-2c0acede5735.txt | TikTok influencers fuelling parallel market for unlicensed weight-loss drug | Consumer aff\n",
            "saved [money]: www.theguardian.com-britons-travelling-to-eu-to-undergo-new-biometric-checks-at--6287d2ebac6b.txt | Britons travelling to EU to undergo new biometric checks at border from Sunday | European \n",
            "saved [money]: www.theguardian.com-50-men-s-autumn-wardrobe-updates-for-under-150-some-are-even-159dfe69dc3e.txt | 50 men’s autumn wardrobe updates for under £150 (some are even free) | Men's fashion | The\n",
            "saved [technology]: www.theguardian.com-using-a-swearword-in-your-google-search-can-stop-the-ai-answ-fc679e0534fb.txt | Using a swearword in your Google search can stop the AI answer. But should you? | Artifici\n",
            "saved [technology]: www.theguardian.com-inside-tech-billionaire-peter-thiel-s-off-the-record-lecture-8c1fa5790b8a.txt | Inside tech billionaire Peter Thiel’s off-the-record lectures about the antichrist | US ne\n",
            "saved [technology]: www.theguardian.com-it-s-sam-altman-the-man-who-stole-the-rights-from-copyright--341c19061de3.txt | It’s Sam Altman: the man who stole the rights from copyright. If he’s the future, can we g\n",
            "saved [technology]: www.theguardian.com-little-lungs-are-paying-1-6m-claimants-head-to-high-court-as-d922a3bcb371.txt | ‘Little lungs are paying’: 1.6m claimants head to high court as carmakers finally face pun\n",
            "saved [technology]: www.theguardian.com-meet-anamanaguchi-the-band-behind-the-last-scott-pilgrim-vid-05f8c2717b6c.txt | Meet Anamanaguchi, the band behind the last Scott Pilgrim video game’s soundtrack – and th\n",
            "saved [science]: www.theguardian.com-drummond-rennie-obituary-science-the-guardian-20bd0c8534ce.txt | Drummond Rennie obituary | Science | The Guardian\n",
            "saved [science]: www.theguardian.com-grisly-recording-reveals-bat-catching-killing-and-eating-rob-05d386541784.txt | Grisly recording reveals bat catching, killing and eating robin mid-flight | Animal behavi\n",
            "saved [science]: www.theguardian.com-dogs-name-toys-while-elephants-name-each-other-animal-langua-ba0531b5feac.txt | Dogs name toys while elephants name each other. Animal language is more complex than we im\n",
            "saved [science]: www.theguardian.com-all-the-news-and-science-from-the-2025-nobel-prizes-podcast--6b3ed95dad81.txt | All the news and science from the 2025 Nobel prizes – podcast | Science | The Guardian\n",
            "saved [science]: www.theguardian.com-nobel-prize-in-chemistry-awarded-to-scientists-for-work-on-h-1570a11f7cfe.txt | Nobel prize in chemistry awarded to scientists for work on ‘Hermione’s handbag’ | Nobel pr\n",
            "saved [global-development]: www.theguardian.com-torture-blackmail-extortion-the-dangers-of-queer-online-dati-30a32a8839bc.txt | Torture, blackmail, extortion: the dangers of queer online dating in Ghana | Global develo\n",
            "saved [global-development]: www.theguardian.com-just-money-with-no-strings-attached-how-direct-cash-transfer-45456966d70a.txt | ‘Just money, with no strings attached’: how direct cash transfers are giving women in rura\n",
            "saved [global-development]: www.theguardian.com-us-undermining-global-health-by-threatening-to-strip-funding-6e391d47249c.txt | US ‘undermining global health’ by threatening to strip funding from aid projects that do n\n",
            "saved [global-development]: www.theguardian.com-i-have-searched-and-searched-for-help-the-sudanese-women-lef-b3aa6375c351.txt | ‘I have searched and searched for help’: the Sudanese women left alone to live hand to mou\n",
            "saved [global-development]: www.theguardian.com-fast-fashion-recycling-how-the-castoff-capital-of-the-world--3fbd2486cecb.txt | Fast-fashion recycling: how ‘the castoff capital of the world’ is making Indian factory wo\n",
            "saved [environment]: www.theguardian.com-number-of-wild-bee-species-at-risk-of-extinction-in-europe-d-7aa6acb043f9.txt | Number of wild bee species at risk of extinction in Europe doubles in 10 years | Bees | Th\n",
            "saved [environment]: www.theguardian.com-country-diary-panic-stress-glory-my-day-scaling-the-inaccess-9352c67e1c1a.txt | Country diary: Panic, stress, glory – my day scaling the Inaccessible Pinnacle | Mountaine\n",
            "saved [environment]: www.theguardian.com-baby-numbats-spotted-at-two-wildlife-sanctuaries-in-hopeful--93f9fb1e5ed6.txt | Baby numbats spotted at two wildlife sanctuaries in hopeful sign for one of Australia’s ra\n",
            "saved [environment]: www.theguardian.com-orange-bellied-parrots-have-swelled-back-from-imminent-extin-6254634456a8.txt | Orange-bellied parrots have swelled back from imminent extinction – but now they face a ne\n",
            "saved [environment]: www.theguardian.com-baby-giant-tortoises-thrive-in-seychelles-after-first-succes-b9b63448e6fb.txt | Baby giant tortoises thrive in Seychelles after first successful artificial incubation | C\n",
            "saved [wildlife]: www.theguardian.com-here-s-to-the-birdwatchers-optimistic-slightly-eccentric-cus-171d793f1527.txt | Here’s to the birdwatchers! Optimistic, slightly eccentric custodians of wonder and joy an\n",
            "saved [wildlife]: www.theguardian.com-and-then-there-were-none-australia-s-only-shrew-declared-ext-f537fdf4933e.txt | And then there were none: Australia’s only shrew declared extinct | John Woinarski for the\n",
            "saved [wildlife]: www.theguardian.com-crocodile-made-famous-by-steve-irwin-wrongfully-arrested-and-a28168757125.txt | Crocodile made famous by Steve Irwin ‘wrongfully arrested’ and should be returned to wild,\n",
            "saved [wildlife]: www.theguardian.com-more-than-half-of-world-s-bird-species-in-decline-as-leaders-e01518d9c81f.txt | More than half of world’s bird species in decline, as leaders meet on extinction crisis | \n",
            "saved [wildlife]: www.theguardian.com-week-in-wildlife-a-bumpy-snailfish-a-slow-loris-and-a-whistl-d28cda0f8e72.txt | Week in wildlife: a bumpy snailfish, a slow loris and a whistle pig | Environment | The Gu\n",
            "saved [pollution]: www.theguardian.com-millions-in-england-face-higher-water-bills-after-regulator--14d176706e91.txt | Millions in England face higher water bills after regulator backs more price rises | Water\n",
            "saved [pollution]: www.theguardian.com-thousands-take-legal-action-over-widespread-pollution-of-thr-2f56c9401116.txt | Thousands take legal action over ‘widespread pollution’ of three UK rivers | Rivers | The \n",
            "saved [pollution]: www.theguardian.com-uk-plastic-waste-exports-to-developing-countries-rose-84-in--6b64f8be1495.txt | UK plastic waste exports to developing countries rose 84% in a year, data shows | Plastics\n",
            "saved [pollution]: www.theguardian.com-thames-water-removes-100-tonne-fatberg-from-west-london-sewe-6e4d199cb1c0.txt | Thames Water removes 100-tonne fatberg from west London sewer | Environment | The Guardian\n",
            "saved [pollution]: www.theguardian.com-carbon-offsets-fail-to-cut-global-heating-due-to-intractable-0fef0f7b4b5a.txt | Carbon offsets fail to cut global heating due to ‘intractable’ systemic problems, study sa\n",
            "saved [climate-crisis]: www.theguardian.com-with-most-mps-ignorant-of-the-urgency-how-can-the-uk-ever-re-eff8058f92b2.txt | With most MPs ignorant of the urgency, how can the UK ever reach net zero? | Climate crisi\n",
            "saved [climate-crisis]: www.theguardian.com-prince-william-to-attend-cop30-un-climate-summit-in-brazil-c-9d8e93f8aebd.txt | Prince William to attend Cop30 UN climate summit in Brazil | Cop30 | The Guardian\n",
            "saved [climate-crisis]: www.theguardian.com-americans-are-dying-from-extreme-heat-autopsy-reports-don-t--6c6982019ab1.txt | Americans are dying from extreme heat. Autopsy reports don’t show the full story | Extreme\n",
            "saved [climate-crisis]: www.theguardian.com-more-than-40-trump-administration-picks-tied-directly-to-oil-3032eea563fb.txt | More than 40 Trump administration picks tied directly to oil, gas and coal, analysis shows\n",
            "saved [climate-crisis]: www.theguardian.com-has-finding-an-affordable-home-in-australia-just-gotten-hard-455bbd71753e.txt | Has finding an affordable home in Australia just gotten harder? | Fiona Katauskas | The Gu\n",
            "saved [sport]: www.theguardian.com-world-cup-qualifying-roundup-neves-stuns-republic-of-ireland-499bfc0c420c.txt | World Cup qualifying roundup: Neves stuns Republic of Ireland with late goal | World Cup 2\n",
            "saved [sport]: www.theguardian.com-rangers-could-turn-to-r-hl-after-steven-gerrard-rules-out-re-42237c8bcc59.txt | Rangers could turn to Röhl after Steven Gerrard rules out return as manager | Steven Gerra\n",
            "saved [sport]: www.theguardian.com-hull-kr-secure-treble-with-dominant-display-against-wigan-in-c3f6a953f14d.txt | Hull KR secure treble with dominant display against Wigan in Super League Grand Final | Su\n",
            "saved [sport]: www.theguardian.com-aston-villa-must-stop-crying-foul-and-focus-on-the-europa-le-e3f53a617444.txt | Aston Villa must stop crying foul and focus on the Europa League instead | Aston Villa | T\n",
            "saved [sport]: www.theguardian.com-former-arsenal-midfielder-jack-wilshere-in-the-frame-for-lut-a4c919b67e0a.txt | Former Arsenal midfielder Jack Wilshere in the frame for Luton job | Soccer | The Guardian\n",
            "saved [football]: www.theguardian.com-erling-haaland-hat-trick-helps-norway-sink-israel-against-ba-ac0afb3fe6b8.txt | Erling Haaland hat-trick helps Norway sink Israel against backdrop of protests | World Cup\n",
            "saved [football]: www.theguardian.com-ross-wilson-ends-newcastle-sporting-director-saga-in-switch--75f933684acd.txt | Ross Wilson ends Newcastle sporting director saga in switch from Forest | Newcastle United\n",
            "saved [football]: www.theguardian.com-denver-summit-s-nick-cushing-on-building-an-nwsl-club-from-s-4fbcad300c5d.txt | Denver Summit’s Nick Cushing on building an NWSL club from scratch | Women's football | Th\n",
            "saved [football]: www.theguardian.com-tottenham-s-burst-of-positivity-under-martin-ho-is-boost-for-1e5ff5cf99e0.txt | Tottenham’s burst of positivity under Martin Ho is boost for WSL as a whole | Tottenham Ho\n",
            "saved [football]: www.theguardian.com-clive-tyldesley-i-ve-only-been-drunk-twice-and-once-was-with-363556646e53.txt | Clive Tyldesley: ‘I’ve only been drunk twice and once was with the England women’s team’ |\n",
            "saved [cricket]: www.theguardian.com-england-ease-to-89-run-win-over-sri-lanka-women-s-cricket-wo-e8122bd9597c.txt | England ease to 89-run win over Sri Lanka: Women’s Cricket World Cup – as it happened | Wo\n",
            "saved [cricket]: www.theguardian.com-nat-sciver-brunt-shines-as-england-recover-to-sweep-aside-sr-1f41f385c083.txt | Nat Sciver-Brunt shines as England recover to sweep aside Sri Lanka | Women's Cricket Worl\n",
            "saved [cricket]: www.theguardian.com-winter-is-coming-england-s-cricketers-fly-out-for-long-tour--a16abdc3b98c.txt | Winter is coming: England’s cricketers fly out for long tour that will decide Ashes and Wo\n",
            "saved [cricket]: www.theguardian.com-harry-brook-says-pat-cummins-absence-could-boost-england-s-a-c2a44967739f.txt | Harry Brook says Pat Cummins’ absence could boost England’s Ashes hopes | Ashes 2025 - 26 \n",
            "saved [cricket]: www.theguardian.com-sports-quiz-of-the-week-cole-palmer-women-s-cricket-world-cu-30b2db1a8ff7.txt | Sports quiz of the week: Cole Palmer, Women’s Cricket World Cup and Cape Verde | Sport | T\n",
            "saved [tennis]: www.theguardian.com-world-no-204-vacherot-topples-djokovic-to-set-up-cousin-v-co-89b5dd9e242c.txt | World No 204 Vacherot topples Djokovic to set up cousin v cousin Shanghai final | Tennis |\n",
            "saved [tennis]: www.theguardian.com-all-fluffed-up-why-modern-balls-are-sparking-injury-worries--9526e45cd6e7.txt | All fluffed up: why modern balls are sparking injury worries and frustration in tennis | T\n",
            "saved [tennis]: www.theguardian.com-we-rewatched-an-ajax-match-what-really-happens-when-tennis-a-ffb782b64ce4.txt | ‘We rewatched an Ajax match’: what really happens when tennis anti-dopers call | Tennis | \n",
            "saved [tennis]: www.theguardian.com-novak-djokovic-battles-into-shanghai-semis-as-vacherot-s-dre-837c3606f39f.txt | Novak Djokovic battles into Shanghai semis as Vacherot’s dream run continues | Tennis | Th\n",
            "saved [tennis]: www.theguardian.com-amazing-number-alex-de-minaur-hits-new-milestone-on-path-to--4fc08e0f235e.txt | ‘Amazing number’: Alex de Minaur hits new milestone on path to Shanghai quarters | Alex de\n",
            "saved [golf]: www.theguardian.com-collin-morikawa-denies-his-chaos-comments-inflamed-usa-fans--1d99c7bad2f8.txt | Collin Morikawa denies his ‘chaos’ comments inflamed USA fans at Ryder Cup | Ryder Cup 202\n",
            "saved [golf]: www.theguardian.com-swearing-booing-and-spitting-is-crowd-behaviour-out-of-contr-3a131717bcf5.txt | Swearing, booing and spitting: is crowd behaviour out of control? | Psychology | The Guard\n",
            "saved [golf]: www.theguardian.com-sports-quiz-of-the-week-ryder-cup-world-cup-winners-and-harr-ec4687193ccd.txt | Sports quiz of the week: Ryder Cup, World Cup winners and Harry Kane | Sport | The Guardia\n",
            "saved [golf]: www.theguardian.com-pga-of-america-president-belatedly-admits-us-ryder-cup-fans--30795edd2a44.txt | PGA of America president belatedly admits US Ryder Cup fans ‘crossed line’ with abuse | Ry\n",
            "saved [golf]: www.theguardian.com-what-can-americans-learn-from-the-uk-how-to-cope-with-nation-01b791aab9a6.txt | What can Americans learn from the UK? How to cope with national sporting despair | Sport |\n",
            "saved [formulaone]: www.theguardian.com-carlos-sainz-hits-out-at-f1-broadcasters-coverage-of-celebri-bc3442110a54.txt | Carlos Sainz hits out at F1 broadcasters’ coverage of ‘celebrities and girlfriends’ | Form\n",
            "saved [formulaone]: www.theguardian.com-stella-admits-mclaren-face-difficulties-managing-norris-and--6f552a9e5b73.txt | Stella admits McLaren ‘face difficulties’ managing Norris and Piastri in title run-in | Fo\n",
            "saved [formulaone]: www.theguardian.com-norris-rejects-piastri-s-complaints-after-mclaren-duo-clash--bbd578148cdc.txt | Norris rejects Piastri’s complaints after McLaren duo clash in Singapore | Formula One | T\n",
            "saved [formulaone]: www.theguardian.com-singapore-grand-prix-2025-russell-wins-as-verstappen-holds-o-306c11c1cfa6.txt | Singapore Grand Prix 2025: Russell wins as Verstappen holds off Norris for second – as it \n",
            "saved [formulaone]: www.theguardian.com-george-russell-wins-f1-singapore-gp-as-norris-nibbles-at-ang-0b4449be6727.txt | George Russell wins F1 Singapore GP as Norris nibbles at angry Piastri’s lead | Formula On\n",
            "saved [cycling]: www.theguardian.com-tadej-pogacar-caps-stunning-season-with-fifth-straight-il-lo-35631833dda8.txt | Tadej Pogacar caps stunning season with fifth straight Il Lombardia triumph | Cycling | Th\n",
            "saved [cycling]: www.theguardian.com-derek-gee-faces-30m-damages-claim-after-ending-israel-premie-276cc2daa284.txt | Derek Gee faces €30m damages claim after ending Israel-Premier Tech contract over ‘persona\n",
            "saved [cycling]: www.theguardian.com-i-m-the-total-opposite-to-cav-and-brad-geraint-thomas-on-how-a5188ec44344.txt | ‘I’m the total opposite to Cav and Brad’: Geraint Thomas on how a normal bloke won the Tou\n",
            "saved [cycling]: www.theguardian.com-israel-premier-tech-to-change-its-name-and-move-away-from-cu-85b48187d3bf.txt | Israel-Premier Tech to change its name and move away from ‘current identity’ | Cycling | T\n",
            "saved [cycling]: www.theguardian.com-unstoppable-tadej-pogacar-defends-men-s-road-race-world-titl-d377069a1f17.txt | Unstoppable Tadej Pogacar defends men’s road race world title in Rwanda | Cycling Road Wor\n",
            "saved [rugby-union]: www.theguardian.com-anthony-belleau-turns-the-screw-as-northampton-see-off-rival-d6258697986a.txt | Anthony Belleau turns the screw as Northampton see off rivals Leicester | Prem Rugby | The\n",
            "saved [rugby-union]: www.theguardian.com-exe-men-review-entertaining-rugby-drama-tackles-triumph-of-u-c0e096814a33.txt | Exe Men review – entertaining rugby drama tackles triumph of underdogs Exeter Chiefs | The\n",
            "saved [rugby-union]: www.theguardian.com-roebuck-hat-trick-sets-up-nine-try-rout-as-sale-pile-more-mi-6a3cc3e9b7ea.txt | Roebuck hat-trick sets up nine-try rout as Sale pile more misery on Newcastle | Prem Rugby\n",
            "saved [rugby-union]: www.theguardian.com-exeter-s-brown-bampoe-in-the-fast-lane-for-great-things-for--076e0d6c0074.txt | Exeter’s Brown-Bampoe in the fast lane for great things for club and country | Exeter | Th\n",
            "saved [rugby-union]: www.theguardian.com-northampton-welcome-back-lions-and-not-a-day-too-soon-as-lei-bf23596510f8.txt | Northampton welcome back Lions – and not a day too soon as Leicester loom | Prem Rugby | T\n",
            "saved [culture]: www.theguardian.com-diane-keaton-oscar-winning-star-of-annie-hall-and-the-godfat-e2b36c6197de.txt | Diane Keaton, Oscar-winning star of Annie Hall and The Godfather, dies aged 79 | Diane Kea\n",
            "saved [culture]: www.theguardian.com-rock-stars-would-be-like-yeah-bring-the-kid-in-cameron-crowe-65277b7b5c46.txt | ‘Rock stars would be like, Yeah, bring the kid in’: Cameron Crowe on his wild years as a t\n",
            "saved [culture]: www.theguardian.com-verging-on-unwatchable-guardian-writers-on-their-most-stress-f00c1525d9ba.txt | ‘Verging on unwatchable’: Guardian writers on their most stressful movies | Movies | The G\n",
            "saved [culture]: www.theguardian.com-i-m-going-to-write-about-all-of-it-author-chris-kraus-on-suc-1575e227be81.txt | ‘I’m going to write about all of it’: author Chris Kraus on success, drugs and I Love Dick\n",
            "saved [culture]: www.theguardian.com-emma-doran-when-i-was-growing-up-a-woman-s-biggest-complimen-ba847e640975.txt | Emma Doran: ‘When I was growing up, a woman’s biggest compliment would be that she was imm\n",
            "saved [film]: www.theguardian.com-diane-keaton-a-life-in-pictures-film-the-guardian-f01996aca99f.txt | Diane Keaton: a life in pictures | Film | The Guardian\n",
            "saved [film]: www.theguardian.com-the-freak-script-of-charlie-chaplin-s-unfinished-final-film--1e95d733afa5.txt | The Freak: script of Charlie Chaplin’s unfinished final film to be published | Charlie Cha\n",
            "saved [film]: www.theguardian.com-moss-and-freud-review-kate-meets-lucian-and-they-get-on-bril-e960e167c3fb.txt | Moss and Freud review – Kate meets Lucian and they get on brilliantly with absolutely no f\n",
            "saved [film]: www.theguardian.com-operation-pope-review-hard-bitten-thriller-about-a-true-life-82353edd7dd1.txt | Operation Pope review – hard-bitten thriller about a true-life papal assassination plot | \n",
            "saved [film]: www.theguardian.com-rebel-wilson-using-us-court-proceedings-to-harass-and-intimi-fbac922571b2.txt | Rebel Wilson using US court proceedings to ‘harass and intimidate’ star of The Deb, court \n",
            "saved [music]: www.theguardian.com-perfume-genius-i-really-like-body-hair-i-like-a-bush-i-didn--d2b4cdfa9096.txt | Perfume Genius: ‘I really like body hair! I like a bush. I didn’t even notice Jimmy Fallon\n",
            "saved [music]: www.theguardian.com-my-led-zeppelin-road-trip-was-counted-as-a-class-credit-came-57e14fa8ecdb.txt | ‘My Led Zeppelin road trip was counted as a class credit’: Cameron Crowe on the interview \n",
            "saved [music]: www.theguardian.com-my-cultural-awakening-kate-bush-helped-me-come-out-as-a-tran-76b8f6723166.txt | My cultural awakening: ‘Kate Bush helped me come out as a trans woman’ | Kate Bush | The G\n",
            "saved [music]: www.theguardian.com-taylor-swift-scores-second-biggest-uk-charts-opening-week-ev-367a4d91129a.txt | Taylor Swift scores second-biggest UK charts opening week ever with The Life of a Showgirl\n",
            "saved [music]: www.theguardian.com-moody-blues-singer-and-bassist-john-lodge-dies-aged-82-music-60a75e671b1b.txt | Moody Blues singer and bassist John Lodge dies aged 82 | Music | The Guardian\n",
            "saved [artanddesign]: www.theguardian.com-it-s-like-a-scene-from-a-movie-christian-barroso-s-best-phon-483cd3e08ea9.txt | ‘It’s like a scene from a movie’: Christian Barroso’s best phone picture | Photography | T\n",
            "saved [artanddesign]: www.theguardian.com-triple-trouble-fairey-hirst-invader-review-the-most-revoltin-81ecef32ee72.txt | Triple Trouble: Fairey, Hirst, Invader review – the most revolting visual soup imaginable \n",
            "saved [artanddesign]: www.theguardian.com-from-tron-ares-to-riot-women-your-complete-entertainment-gui-5016192644f2.txt | From Tron: Ares to Riot Women: your complete entertainment guide to the week ahead | Cultu\n",
            "saved [artanddesign]: www.theguardian.com-the-week-around-the-world-in-20-pictures-art-and-design-the--d40445a73114.txt | The week around the world in 20 pictures | Art and design | The Guardian\n",
            "saved [artanddesign]: www.theguardian.com-fill-the-frame-use-the-light-andrew-chapman-s-favourite-phot-1c75e29641fc.txt | Fill the frame, use the light: Andrew Chapman’s favourite photographs | Art and design | T\n",
            "saved [books]: www.theguardian.com-colm-t-ib-n-why-i-set-up-a-press-to-publish-nobel-winner-l-s-ab0df0756144.txt | Colm Tóibín: Why I set up a press to publish Nobel winner László Krasznahorkai | László Kr\n",
            "saved [books]: www.theguardian.com-the-best-recent-science-fiction-fantasy-and-horror-review-ro-ac4ee1120e64.txt | The best recent science fiction, fantasy and horror – review roundup | Science fiction boo\n",
            "saved [books]: www.theguardian.com-natalie-haynes-i-ll-never-read-anything-by-a-bront-again-boo-329556dce7c2.txt | Natalie Haynes: ‘I’ll never read anything by a Brontë again’ | Books | The Guardian\n",
            "saved [books]: www.theguardian.com-raise-your-soul-by-yanis-varoufakis-review-an-intimate-histo-477346e2f7e1.txt | Raise Your Soul by Yanis Varoufakis review – an intimate history of Greece | History books\n",
            "saved [books]: www.theguardian.com-l-szl-krasznahorkai-wins-the-nobel-prize-in-literature-2025--26dd2f601b97.txt | László Krasznahorkai wins the Nobel prize in literature 2025 | Nobel prize in literature |\n",
            "saved [tv-and-radio]: www.theguardian.com-the-intruder-review-the-daftest-thriller-of-the-entire-year--8c22248becc4.txt | The Intruder review – the daftest thriller of the entire year | Television | The Guardian\n",
            "saved [tv-and-radio]: www.theguardian.com-strictly-come-dancing-week-three-as-it-happened-strictly-com-194469bf7a08.txt | Strictly Come Dancing: week three – as it happened | Strictly Come Dancing | The Guardian\n",
            "saved [tv-and-radio]: www.theguardian.com-witches-of-essex-rylan-and-prof-alice-s-look-at-one-of-histo-ddfbf621f7da.txt | Witches of Essex: Rylan and Prof Alice’s look at one of history’s most shameful periods is\n",
            "saved [tv-and-radio]: www.theguardian.com-tv-tonight-a-french-psychological-thriller-about-a-perfect-n-2637ff45ea68.txt | TV tonight: a French psychological thriller about a ‘perfect’ nanny | Television & radio |\n",
            "saved [tv-and-radio]: www.theguardian.com-tech-terror-and-tom-hollander-niamh-algar-on-her-wild-new-tv-333cec4952a9.txt | Tech, terror and Tom Hollander: Niamh Algar on her wild new TV thriller | Television | The\n",
            "saved [lifestyle]: www.theguardian.com-charlie-higson-by-my-mid-20s-i-had-a-beer-gut-it-s-now-the-m-7d5424c959ff.txt | Charlie Higson: ‘By my mid-20s, I had a beer gut. It’s now the most substantial part of me\n",
            "saved [lifestyle]: www.theguardian.com-from-getting-kids-to-eat-veg-to-curbing-screen-time-the-pare-fe89707e351c.txt | From getting kids to eat veg to curbing screen time: the parenting hacks that actually wor\n",
            "saved [lifestyle]: www.theguardian.com-baggy-skinny-or-neither-why-goldilocks-jeans-are-having-a-mo-a49ab62cc2be.txt | Baggy, skinny … or neither? Why ‘Goldilocks’ jeans are having a moment | Jeans | The Guard\n",
            "saved [lifestyle]: www.theguardian.com-who-founded-the-royal-ballet-school-in-1926-the-saturday-qui-2b5b48d112e8.txt | Who founded the Royal Ballet School in 1926? The Saturday quiz | Quiz and trivia games | T\n",
            "saved [lifestyle]: www.theguardian.com-i-step-outside-into-a-cacophony-of-nature-an-off-grid-escape-d04712af17d2.txt | ‘I step outside into a cacophony of nature’: an off-grid escape in the west of England | E\n",
            "saved [family]: www.theguardian.com-why-do-onions-have-layers-and-do-sharks-swim-in-groups-the-k-b2ec57f73a8d.txt | Why do onions have layers and do sharks swim in groups? The kids’ quiz | Family | The Guar\n",
            "saved [family]: www.theguardian.com-blind-date-after-too-many-tepid-app-based-dates-outsourcing--fe9d1abbdc25.txt | Blind date: ‘After too many tepid, app-based dates, outsourcing my love life to a paper is\n",
            "saved [family]: www.theguardian.com-tim-dowling-i-m-under-an-azure-croatian-sky-fretting-about-t-8e93b5b8f545.txt | Tim Dowling: I’m under an azure Croatian sky – fretting about the roof back home | Life an\n",
            "saved [family]: www.theguardian.com-my-brother-and-sister-are-angry-at-my-parents-i-feel-caught--339c3bdca4c6.txt | My brother and sister are angry at my parents. I feel caught in the middle. What can I do?\n",
            "saved [family]: www.theguardian.com-my-girlfriend-is-supportive-as-a-partner-but-completely-baff-3f09ea553563.txt | My girlfriend is supportive as a partner, but completely baffled by my love of sports | Re\n",
            "saved [health]: www.theguardian.com-going-to-the-gym-was-too-much-effort-until-i-moved-into-one--8fabb4b13f7d.txt | Going to the gym was too much effort, until I moved into one | Fitness | The Guardian\n",
            "saved [health]: www.theguardian.com-the-armed-robber-who-went-straight-john-mcavoy-was-born-into-2aa263850a21.txt | The armed robber who went straight: John McAvoy was born into the criminal life. Here’s ho\n",
            "saved [health]: www.theguardian.com-look-out-for-number-one-selfish-self-help-books-are-booming--b8a5927f34c2.txt | Look out for number one! Selfish self-help books are booming – but will they improve your \n",
            "saved [health]: www.theguardian.com-can-t-sleep-turn-on-tune-in-and-drop-off-sleep-the-guardian-37aeb91614da.txt | Can’t sleep? Turn on, tune in and drop off | Sleep | The Guardian\n",
            "saved [health]: www.theguardian.com-typewriter-reveals-unexpected-genius-health-wellbeing-the-gu-2eb1396cc9a9.txt | Typewriter reveals unexpected genius | Health & wellbeing | The Guardian\n",
            "saved [inequality]: www.theguardian.com-worried-about-rising-bills-and-getting-by-keir-starmer-has-t-cc7a2779456d.txt | Worried about rising bills and getting by? Keir Starmer has the answer: try chewing a flag\n",
            "saved [inequality]: www.theguardian.com-lifetime-of-earnings-not-enough-for-uk-workers-to-join-wealt-198a9be56add.txt | Lifetime of earnings not enough for UK workers to join wealthiest 10%, report says | Inequ\n",
            "saved [inequality]: www.theguardian.com-vocational-training-needs-more-than-money-further-education--ce220d983cfd.txt | Vocational training needs more than money | Further education | The Guardian\n",
            "saved [inequality]: www.theguardian.com-one-rule-for-the-rich-the-salzburg-mansion-the-porsche-heir--fba56f492adb.txt | ‘One rule for the rich’: the Salzburg mansion, the Porsche heir and the writer Stefan Zwei\n",
            "saved [inequality]: www.theguardian.com-more-than-60-000-cancer-patients-in-england-not-getting-nece-662fb1a1c78a.txt | More than 60,000 cancer patients in England ‘not getting necessary radiotherapy’ | Cancer \n",
            "saved [obituaries]: www.theguardian.com-eric-potts-obituary-business-the-guardian-dd912d057836.txt | Eric Potts obituary | Business | The Guardian\n",
            "saved [obituaries]: www.theguardian.com-maureen-crill-obituary-nursing-the-guardian-0b72b5252f1d.txt | Maureen Crill obituary | Nursing | The Guardian\n",
            "saved [obituaries]: www.theguardian.com-mark-saunders-obituary-documentary-films-the-guardian-59b4365576ac.txt | Mark Saunders obituary | Documentary films | The Guardian\n",
            "saved [obituaries]: www.theguardian.com-michael-barnes-obituary-documentary-the-guardian-d19993d49edf.txt | Michael Barnes obituary | Documentary | The Guardian\n",
            "saved [obituaries]: www.theguardian.com-felicity-opp-obituary-movies-the-guardian-94a1b2d0ba1d.txt | Felicity Oppé obituary | Movies | The Guardian\n",
            "saved [travel]: www.theguardian.com-this-is-pretty-therapeutic-a-pottery-retreat-in-spain-s-alpu-54dc60b761d2.txt | ‘This is pretty therapeutic’: a pottery retreat in Spain’s Alpujarras | Learning holidays \n",
            "saved [travel]: www.theguardian.com-20-a-night-for-one-of-the-most-peaceful-locations-in-the-wor-29e32e4247fd.txt | ‘£20 a night for one of the most peaceful locations in the world’: readers’ favourite remo\n",
            "saved [travel]: www.theguardian.com-black-brummie-and-proud-a-walking-tour-of-the-real-handswort-f154cf45bebf.txt | Black, Brummie and proud: a walking tour of the real Handsworth | Birmingham holidays | Th\n",
            "saved [travel]: www.theguardian.com-sweat-dirt-and-grape-juice-it-s-incredibly-rewarding-volunte-9a04eeebd446.txt | ‘Sweat, dirt and grape juice – it’s incredibly rewarding’: volunteer harvesting on a viney\n",
            "saved [travel]: www.theguardian.com-it-s-more-than-a-pretty-backdrop-crime-writer-ann-cleeves-on-eff98fb9bd90.txt | ‘It’s more than a pretty backdrop’: crime writer Ann Cleeves on the magic of Orkney in Sco\n",
            "saved [fashion]: www.theguardian.com-bath-mats-candles-and-underpants-would-basquiat-have-loved-o-bf8e4073ed56.txt | Bath mats, candles and underpants: would Basquiat have loved or hated all the merch? | Jea\n",
            "saved [fashion]: www.theguardian.com-to-a-tee-what-to-wear-to-make-a-basic-t-shirt-look-less-basi-ae16cd79120b.txt | To a tee: what to wear to make a basic T-shirt look less basic | Fashion | The Guardian\n",
            "saved [fashion]: www.theguardian.com-we-had-to-kill-the-wag-nine-things-we-learned-from-victoria--2ba16036e38b.txt | ‘We had to kill the Wag’: nine things we learned from Victoria Beckham’s docuseries | Fash\n",
            "saved [fashion]: www.theguardian.com-jess-cartner-morley-ladylike-fashion-is-back-but-it-s-been-g-885dce0aa51b.txt | Jess Cartner-Morley: ladylike fashion is back, but it’s been given a modern twist | Fashio\n",
            "saved [fashion]: www.theguardian.com-sali-hughes-on-beauty-forget-harsh-treatments-to-get-rid-of--92ffd1b82980.txt | Sali Hughes on beauty: forget harsh treatments – to get rid of adult acne, show your skin \n",
            "saved [games]: www.theguardian.com-hack-of-age-verification-firm-may-have-exposed-70-000-discor-7b98e4c6183e.txt | Hack of age verification firm may have exposed 70,000 Discord users’ ID photos | Social me\n",
            "saved [games]: www.theguardian.com-the-non-profit-helping-people-from-all-over-the-world-to-bec-eba4c575c0de.txt | The non-profit helping people from all over the world to become successful game developers\n",
            "saved [games]: www.theguardian.com-what-the-xbox-game-pass-price-hike-says-about-the-rising-cos-5d2cd78257cc.txt | What the Xbox Game Pass price hike says about the rising cost of playing games | Games | T\n",
            "saved [games]: www.theguardian.com-cold-war-power-play-how-the-stasi-got-into-computer-games-ga-d26a77e8ac94.txt | Cold war power play: how the Stasi got into computer games | Games | The Guardian\n",
            "saved [games]: www.theguardian.com-proof-of-age-id-leaked-in-discord-data-breach-games-the-guar-1ccc4b39d0f5.txt | Proof-of-age ID leaked in Discord data breach | Games | The Guardian\n",
            "saved [stage]: www.theguardian.com-small-hotel-review-ralph-fiennes-fever-dream-leaves-you-with-5904ac30fa86.txt | Small Hotel review – Ralph Fiennes’ fever dream leaves you with major reservations | Theat\n",
            "saved [stage]: www.theguardian.com-charley-s-aunt-review-a-fresh-and-fun-glow-up-for-victorian--6574b22a4ccf.txt | Charley’s Aunt review – a fresh and fun glow-up for Victorian farce | Theatre | The Guardi\n",
            "saved [stage]: www.theguardian.com-my-right-foot-review-wryly-humorous-look-at-life-with-a-term-c0a3b35e6e8b.txt | My Right Foot review – wryly humorous look at life with a terminal illness | Theatre | The\n",
            "saved [stage]: www.theguardian.com-ian-judge-obituary-theatre-the-guardian-6fab1c33a28f.txt | Ian Judge obituary | Theatre | The Guardian\n",
            "saved [stage]: www.theguardian.com-bad-lads-review-brutality-shame-and-fear-as-horrors-of-youth-732901994b77.txt | Bad Lads review – brutality, shame and fear as horrors of youth detention centre are laid \n",
            "saved [crosswords]: www.theguardian.com-killer-sudoku-991-life-and-style-the-guardian-9bf7b1ea4751.txt | Killer sudoku 991 | Life and style | The Guardian\n",
            "saved [crosswords]: www.theguardian.com-quick-crossword-no-17-296-crosswords-the-guardian-d3a0214b2ce8.txt | Quick crossword No 17,296 | Crosswords | The Guardian\n",
            "saved [crosswords]: www.theguardian.com-quick-cryptic-crossword-no-80-crosswords-the-guardian-fd386f0aeb04.txt | Quick cryptic crossword No 80 | Crosswords | The Guardian\n",
            "saved [crosswords]: www.theguardian.com-prize-crossword-no-29-823-crosswords-the-guardian-37e3c9f89b27.txt | Prize crossword No 29,823 | Crosswords | The Guardian\n",
            "saved [crosswords]: www.theguardian.com-crossword-editor-s-desk-clues-for-michael-stipe-and-a-life-i-01b3103c9ac6.txt | Crossword editor’s desk: clues for Michael Stipe and a life in puzzles | Crosswords | The \n",
            "saved [commentisfree]: www.theguardian.com-why-is-this-fox-news-host-speculating-about-aoc-s-sex-life-a-a1eb7529979c.txt | Why is this Fox News host speculating about AOC’s sex life? | Arwa Mahdawi | The Guardian\n",
            "saved [commentisfree]: www.theguardian.com-pity-poor-trump-whose-nobel-hopes-were-dashed-by-common-sens-83e00f4d6157.txt | Pity poor Trump, whose Nobel hopes were dashed by common sense | Dave Schilling | The Guar\n",
            "saved [commentisfree]: www.theguardian.com-my-kids-gave-me-enough-material-to-write-tv-comedy-where-wil-1c76c9346d2b.txt | My kids gave me enough material to write TV comedy. Where will the jokes come from now the\n",
            "saved [commentisfree]: www.theguardian.com-why-should-you-be-labour-s-next-deputy-leader-guardian-reade-23286610a168.txt | Why should you be Labour’s next deputy leader? Guardian readers quiz the candidates | Brid\n",
            "saved [commentisfree]: www.theguardian.com-trump-s-strong-arming-of-netanyahu-led-to-a-deal-he-must-sus-e84683244b77.txt | Trump’s strong-arming of Netanyahu led to a deal. He must sustain that pressure | Mohamad \n",
            "saved [commentisfree]: www.theguardian.com-trump-wants-a-prize-for-the-gaza-deal-but-the-real-question--08d5454f881e.txt | Trump wants a prize for the Gaza deal. But the real question is, why didn’t he do it earli\n",
            "saved [commentisfree]: www.theguardian.com-peter-thiel-s-off-the-record-antichrist-lectures-reveal-more-318dae986ff5.txt | Peter Thiel’s off-the-record antichrist lectures reveal more about him than Armageddon | A\n",
            "saved [commentisfree]: www.theguardian.com-the-fledgling-un-tried-to-rein-in-mass-scale-misinformation--503ec319c92c.txt | The fledgling UN tried to rein in mass-scale misinformation. The world turned its back and\n",
            "saved [commentisfree]: www.theguardian.com-beware-netanyahu-he-is-a-master-of-self-interest-and-that-s--34b4a57d261d.txt | Beware Netanyahu: he is a master of self-interest – and that’s why he signed the Hamas cea\n",
            "saved [commentisfree]: www.theguardian.com-english-democracy-relies-on-local-councillors-so-why-are-so--7195d3698375.txt | English democracy relies on local councillors. So why are so many facing the axe? | Polly \n",
            "saved [commentisfree]: www.theguardian.com-stephen-miller-is-the-most-dangerous-man-in-the-trump-admini-5885d346054c.txt | Stephen Miller is the most dangerous man in the Trump administration | Judith Levine | The\n",
            "saved [commentisfree]: www.theguardian.com-who-will-run-against-trump-in-2028-please-step-forward-now-d-39f98cb37107.txt | Who will run against Trump in 2028? Please step forward now – don’t wait | David Kirp | Th\n",
            "saved [commentisfree]: www.theguardian.com-a-book-is-being-marketed-with-mayo-scented-ink-jealous-me-da-20f2f93baf3f.txt | A book is being marketed with mayo-scented ink. Jealous? Me? | David Barnett | The Guardia\n",
            "saved [commentisfree]: www.theguardian.com-we-all-know-brexit-s-to-blame-for-the-crisis-facing-uk-steel-e84bdaba3533.txt | We all know Brexit’s to blame for the crisis facing UK steel – it’s time for politicians t\n",
            "saved [commentisfree]: www.theguardian.com-the-guardian-view-on-trump-s-gaza-plan-the-bloodshed-must-en-02426d4ed87f.txt | The Guardian view on Trump’s Gaza plan: the bloodshed must end, but this proposal betrays \n",
            "saved [commentisfree]: www.theguardian.com-the-guardian-view-on-trump-s-argentina-bailout-it-s-a-politi-95762dda5574.txt | The Guardian view on Trump’s Argentina bailout: it’s a political play, not an economic pla\n",
            "saved [commentisfree]: www.theguardian.com-the-guardian-view-on-donald-trump-s-hate-for-opponents-pract-57911aae013d.txt | The Guardian view on Donald Trump’s hate for opponents: practising politics the wrong way \n",
            "saved [commentisfree]: www.theguardian.com-the-guardian-view-on-donald-trump-s-ukraine-strategy-talking-c805a582aef3.txt | The Guardian view on Donald Trump’s Ukraine strategy: talking tough and doing very little \n",
            "\n",
            "Summary\n",
            "{\n",
            "  \"saved_total\": 218,\n",
            "  \"errors_total\": 0,\n",
            "  \"max_articles\": 250,\n",
            "  \"max_per_feed\": null,\n",
            "  \"date_from\": \"2025-07-01\",\n",
            "  \"even_split\": true,\n",
            "  \"remainder_to\": \"commentisfree\",\n",
            "  \"feeds_count\": 42,\n",
            "  \"raw_dir\": \"/content/anti_echo/raw\",\n",
            "  \"index_path\": \"/content/anti_echo/feeds/index.json\",\n",
            "  \"feeds_state_local\": \"/content/anti_echo/feeds/feeds_state.json\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup 11 of N — package and push batch to HF, update registry on Git\n",
        "\n",
        "Purpose\n",
        "Package current Chroma collections into batch files, upload to HF under batches/{batch_id}/, then update artifacts/artifacts_registry.json in your GitHub repo. HF is the single source of truth. The registry keeps a chronological ledger for rebuilds.\n",
        "\n",
        "Why this matters\n",
        "Gives you versioned, reconstructable artifacts and a single source of truth for future runs and for a UI.\n",
        "\n",
        "Outputs\n",
        "topic_embeddings.npz, stance_embeddings.npz, metadata.jsonl, manifest.json uploaded to HF and registry updated on GitHub."
      ],
      "metadata": {
        "id": "fieZ0huen5Zg"
      },
      "id": "fieZ0huen5Zg"
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup 11 of N: package and push batch, update registry\n",
        "\n",
        "import os, json, time, uuid, warnings, logging, requests, base64\n",
        "from datetime import datetime, timezone\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from huggingface_hub import upload_file\n",
        "import chromadb\n",
        "\n",
        "logging.getLogger(\"chromadb\").setLevel(logging.ERROR)\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "PROJECT_ROOT = Path(\"/content/anti_echo\").resolve()\n",
        "BATCH_DIR = PROJECT_ROOT / CONFIG[\"batch\"][\"base_dir\"]\n",
        "HF_DATASET_ID = CONFIG[\"hf_dataset_id\"]\n",
        "REPO_OWNER = \"AHMerrill\"\n",
        "REPO_NAME = \"anti-echo-chamber\"\n",
        "BRANCH = \"main\"\n",
        "\n",
        "client = chromadb.PersistentClient(path=str(PROJECT_ROOT / CONFIG[\"chroma\"][\"dir\"]))\n",
        "topic_coll = client.get_collection(CONFIG[\"chroma_collections\"][\"topic\"])\n",
        "stance_coll = client.get_collection(CONFIG[\"chroma_collections\"][\"stance\"])\n",
        "\n",
        "timestamp = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
        "batch_id = f\"batch_{timestamp}_{uuid.uuid4().hex[:8]}\"\n",
        "batch_path = BATCH_DIR / batch_id\n",
        "batch_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Export all current vectors and aligned metadata\n",
        "topic_data = topic_coll.get(include=[\"embeddings\",\"metadatas\"])\n",
        "stance_data = stance_coll.get(include=[\"embeddings\",\"metadatas\"])\n",
        "\n",
        "topic_vecs = np.array(topic_data[\"embeddings\"], dtype=np.float16)\n",
        "stance_vecs = np.array(stance_data[\"embeddings\"], dtype=np.float16)\n",
        "meta_records = topic_data[\"metadatas\"]  # topic and stance share base ids in this scheme\n",
        "\n",
        "# File paths configured in CONFIG\n",
        "topic_npz = batch_path / CONFIG[\"batch\"][\"topic_file\"]\n",
        "stance_npz = batch_path / CONFIG[\"batch\"][\"stance_file\"]\n",
        "meta_path = batch_path / CONFIG[\"batch\"][\"metadata_file\"]\n",
        "manifest_path = batch_path / CONFIG[\"batch\"][\"manifest_name\"]\n",
        "\n",
        "# Write local artifacts\n",
        "np.savez_compressed(topic_npz, topic_vecs)\n",
        "np.savez_compressed(stance_npz, stance_vecs)\n",
        "with meta_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "    for m in meta_records:\n",
        "        json.dump(m, f)\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "manifest = {\n",
        "    \"batch_id\": batch_id,\n",
        "    \"created_at\": timestamp,\n",
        "    \"models\": CONFIG[\"embeddings\"],\n",
        "    \"counts\": {\"topic\": len(topic_vecs), \"stance\": len(stance_vecs)},\n",
        "    \"hf_dataset_id\": HF_DATASET_ID,\n",
        "    \"paths\": {\n",
        "        \"embeddings_topic\": f\"batches/{batch_id}/{topic_npz.name}\",\n",
        "        \"embeddings_stance\": f\"batches/{batch_id}/{stance_npz.name}\",\n",
        "        \"metadata\": f\"batches/{batch_id}/{meta_path.name}\",\n",
        "        \"manifest\": f\"batches/{batch_id}/{manifest_path.name}\",\n",
        "    }\n",
        "}\n",
        "manifest_path.write_text(json.dumps(manifest, indent=2), encoding=\"utf-8\")\n",
        "print(\"Manifest:\", json.dumps(manifest, indent=2))\n",
        "\n",
        "# Upload artifacts to HF dataset\n",
        "print(\"Uploading batch to HF...\")\n",
        "for fpath in [topic_npz, stance_npz, meta_path, manifest_path]:\n",
        "    upload_file(\n",
        "        path_or_fileobj=str(fpath),\n",
        "        path_in_repo=f\"batches/{batch_id}/{fpath.name}\",\n",
        "        repo_id=HF_DATASET_ID,\n",
        "        repo_type=\"dataset\",\n",
        "        token=os.environ[\"HF_TOKEN\"]\n",
        "    )\n",
        "print(\"HF batch upload complete\")\n",
        "\n",
        "# Update registry on GitHub\n",
        "REGISTRY_URL = f\"https://raw.githubusercontent.com/{REPO_OWNER}/{REPO_NAME}/{BRANCH}/artifacts/artifacts_registry.json\"\n",
        "try:\n",
        "    registry = requests.get(REGISTRY_URL, timeout=20).json()\n",
        "except Exception:\n",
        "    registry = {\"version\": 1, \"models\": {}, \"batches\": []}\n",
        "\n",
        "registry.setdefault(\"batches\", []).append(manifest)\n",
        "if isinstance(registry.get(\"version\"), int):\n",
        "    registry[\"version\"] += 1\n",
        "else:\n",
        "    registry[\"version\"] = 1\n",
        "\n",
        "new_registry_bytes = json.dumps(registry, indent=2).encode(\"utf-8\")\n",
        "url = f\"https://api.github.com/repos/{REPO_OWNER}/{REPO_NAME}/contents/artifacts/artifacts_registry.json\"\n",
        "headers = {\"Authorization\": f\"Bearer {os.environ['GITHUB_TOKEN']}\", \"Accept\": \"application/vnd.github+json\"}\n",
        "r = requests.get(url, headers=headers, timeout=20)\n",
        "sha = r.json().get(\"sha\") if r.status_code == 200 else None\n",
        "payload = {\n",
        "    \"message\": f\"Update artifacts registry {timestamp}\",\n",
        "    \"content\": base64.b64encode(new_registry_bytes).decode(),\n",
        "    \"branch\": BRANCH\n",
        "}\n",
        "if sha:\n",
        "    payload[\"sha\"] = sha\n",
        "resp = requests.put(url, headers=headers, json=payload, timeout=30)\n",
        "if resp.status_code not in (200,201):\n",
        "    raise RuntimeError(f\"GitHub registry push failed: {resp.status_code} {resp.text[:300]}\")\n",
        "print(\"Registry updated on GitHub\")\n"
      ],
      "metadata": {
        "id": "a3bxLgPWn8H2"
      },
      "id": "a3bxLgPWn8H2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup 12 of N — quick sanity query\n",
        "\n",
        "Purpose\n",
        "Verify both Chroma collections are populated and aligned. Useful when wiring a future UI that compares similar topics but dissimilar stances.\n",
        "\n",
        "Why this matters\n",
        "Catches empty collections or mismatched counts early.\n",
        "\n",
        "Outputs\n",
        "Counts for both collections and the collection names."
      ],
      "metadata": {
        "id": "D4Q0veK8W9xC"
      },
      "id": "D4Q0veK8W9xC"
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup 12 of N: quick sanity query\n",
        "\n",
        "import chromadb\n",
        "from pathlib import Path\n",
        "\n",
        "PROJECT_ROOT = Path(\"/content/anti_echo\").resolve()\n",
        "client = chromadb.PersistentClient(path=str(PROJECT_ROOT / CONFIG[\"chroma\"][\"dir\"]))\n",
        "topic_coll = client.get_collection(CONFIG[\"chroma_collections\"][\"topic\"])\n",
        "stance_coll = client.get_collection(CONFIG[\"chroma_collections\"][\"stance\"])\n",
        "\n",
        "print({\n",
        "    \"topic_count\": topic_coll.count(),\n",
        "    \"stance_count\": stance_coll.count(),\n",
        "    \"collections\": CONFIG[\"chroma_collections\"]\n",
        "})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l14DYviQcjSv",
        "outputId": "c3c87333-4bc9-4bb1-a2df-db1872d67a32"
      },
      "id": "l14DYviQcjSv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding model: sentence-transformers/all-MiniLM-L6-v2, dim=384, dtype=float16, device=cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2486 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upserted 871 topic embeddings to collection news_topic in 19.87s\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}