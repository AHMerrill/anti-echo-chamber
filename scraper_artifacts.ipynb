{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Setup 1 of N: environment and workspace\n",
        "\n",
        "Goal\n",
        "- Install core dependencies for scraping, embeddings, and Chroma\n",
        "- Create a clean workspace in `/content/anti_echo`\n",
        "- Print basic environment info so collaborators can debug quickly\n",
        "\n",
        "Notes\n",
        "- No Drive mount\n",
        "- Keep installs minimal and pinned where sensible\n"
      ],
      "metadata": {
        "id": "z3lyMabwG0TZ"
      },
      "id": "z3lyMabwG0TZ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup 1 of N: environment and workspace\n",
        "# Colab safe. No Drive mount.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import textwrap\n",
        "from pathlib import Path\n",
        "\n",
        "def pip_install(pkgs):\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + pkgs\n",
        "    print(\"Installing:\", \" \".join(pkgs))\n",
        "    subprocess.check_call(cmd)\n",
        "\n",
        "# Core deps\n",
        "pip_install([\n",
        "    \"feedparser==6.0.10\",\n",
        "    \"trafilatura>=1.6.2,<2.0\",\n",
        "    \"sentence-transformers>=2.6.1,<3.0\",\n",
        "    \"chromadb>=0.5.5,<0.6.0\",\n",
        "    \"huggingface_hub>=0.24.0,<0.28.0\",\n",
        "    \"pyyaml>=6.0.1,<7.0\",\n",
        "    \"numpy>=1.26.4,<3.0\",\n",
        "    \"tqdm>=4.66.0,<5.0\",\n",
        "    \"requests>=2.31.0,<3.0\",\n",
        "    \"rapidfuzz>=3.6.0,<4.0\"\n",
        "])\n",
        "\n",
        "# Optional but helpful\n",
        "try:\n",
        "    import torch\n",
        "except Exception:\n",
        "    pip_install([\"torch>=2.2.0,<3.0\"])\n",
        "\n",
        "# Workspace layout\n",
        "PROJECT_ROOT = Path(\"/content/anti_echo\").resolve()\n",
        "SUBDIRS = [\n",
        "    \"raw\",\n",
        "    \"batches\",\n",
        "    \"chroma_db\",\n",
        "    \"logs\",\n",
        "    \"feeds\",\n",
        "    \"tmp\"\n",
        "]\n",
        "\n",
        "for d in SUBDIRS:\n",
        "    (PROJECT_ROOT / d).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Environment tweaks\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "# Diagnostics\n",
        "import platform, json\n",
        "from importlib.metadata import version, PackageNotFoundError\n",
        "\n",
        "def v(name):\n",
        "    try:\n",
        "        return version(name)\n",
        "    except PackageNotFoundError:\n",
        "        return \"not-installed\"\n",
        "\n",
        "info = {\n",
        "    \"python\": sys.version.split()[0],\n",
        "    \"platform\": platform.platform(),\n",
        "    \"cuda_available\": False,\n",
        "    \"packages\": {\n",
        "        \"feedparser\": v(\"feedparser\"),\n",
        "        \"trafilatura\": v(\"trafilatura\"),\n",
        "        \"sentence-transformers\": v(\"sentence-transformers\"),\n",
        "        \"chromadb\": v(\"chromadb\"),\n",
        "        \"huggingface_hub\": v(\"huggingface-hub\"),\n",
        "        \"pyyaml\": v(\"PyYAML\"),\n",
        "        \"numpy\": v(\"numpy\"),\n",
        "        \"rapidfuzz\": v(\"rapidfuzz\"),\n",
        "        \"torch\": v(\"torch\"),\n",
        "        \"tqdm\": v(\"tqdm\"),\n",
        "        \"requests\": v(\"requests\"),\n",
        "    },\n",
        "    \"paths\": {\n",
        "        \"project_root\": str(PROJECT_ROOT),\n",
        "        \"raw\": str(PROJECT_ROOT / \"raw\"),\n",
        "        \"batches\": str(PROJECT_ROOT / \"batches\"),\n",
        "        \"chroma_db\": str(PROJECT_ROOT / \"chroma_db\"),\n",
        "        \"logs\": str(PROJECT_ROOT / \"logs\"),\n",
        "        \"feeds\": str(PROJECT_ROOT / \"feeds\"),\n",
        "        \"tmp\": str(PROJECT_ROOT / \"tmp\"),\n",
        "    }\n",
        "}\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    info[\"cuda_available\"] = bool(torch.cuda.is_available())\n",
        "    if info[\"cuda_available\"]:\n",
        "        info[\"cuda_device_name\"] = torch.cuda.get_device_name(0)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "print(json.dumps(info, indent=2))\n",
        "\n",
        "# Place a small README in the workspace for orientation\n",
        "workspace_readme = PROJECT_ROOT / \"README_WORKSPACE.txt\"\n",
        "if not workspace_readme.exists():\n",
        "    workspace_readme.write_text(textwrap.dedent(\"\"\"\n",
        "        anti echo chamber - Colab workspace\n",
        "        This directory is ephemeral per session.\n",
        "        Do not commit files from here directly.\n",
        "        Subdirs:\n",
        "          raw        - local scraped texts and meta for this session\n",
        "          batches    - locally packaged batches before HF upload\n",
        "          chroma_db  - local Chroma rebuild target\n",
        "          logs       - run logs\n",
        "          feeds      - runtime feed artifacts\n",
        "          tmp        - scratch space\n",
        "    \"\"\").strip() + \"\\n\", encoding=\"utf-8\")\n",
        "print(f\"Workspace ready at {PROJECT_ROOT}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zffpD2rNG0Ax",
        "outputId": "0660ffc5-ae11-485a-a0be-9fb8b2cd77b4"
      },
      "id": "zffpD2rNG0Ax",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing: feedparser==6.0.10 trafilatura>=1.6.2,<2.0 sentence-transformers>=2.6.1,<3.0 chromadb>=0.5.5,<0.6.0 huggingface_hub>=0.24.0,<0.28.0 pyyaml>=6.0.1,<7.0 numpy>=1.26.4,<3.0 tqdm>=4.66.0,<5.0 requests>=2.31.0,<3.0 rapidfuzz>=3.6.0,<4.0\n",
            "{\n",
            "  \"python\": \"3.12.11\",\n",
            "  \"platform\": \"Linux-6.6.97+-x86_64-with-glibc2.35\",\n",
            "  \"cuda_available\": true,\n",
            "  \"packages\": {\n",
            "    \"feedparser\": \"6.0.10\",\n",
            "    \"trafilatura\": \"1.12.2\",\n",
            "    \"sentence-transformers\": \"2.7.0\",\n",
            "    \"chromadb\": \"0.5.23\",\n",
            "    \"huggingface_hub\": \"0.27.1\",\n",
            "    \"pyyaml\": \"6.0.3\",\n",
            "    \"numpy\": \"2.0.2\",\n",
            "    \"rapidfuzz\": \"3.14.1\",\n",
            "    \"torch\": \"2.8.0+cu126\",\n",
            "    \"tqdm\": \"4.67.1\",\n",
            "    \"requests\": \"2.32.4\"\n",
            "  },\n",
            "  \"paths\": {\n",
            "    \"project_root\": \"/content/anti_echo\",\n",
            "    \"raw\": \"/content/anti_echo/raw\",\n",
            "    \"batches\": \"/content/anti_echo/batches\",\n",
            "    \"chroma_db\": \"/content/anti_echo/chroma_db\",\n",
            "    \"logs\": \"/content/anti_echo/logs\",\n",
            "    \"feeds\": \"/content/anti_echo/feeds\",\n",
            "    \"tmp\": \"/content/anti_echo/tmp\"\n",
            "  },\n",
            "  \"cuda_device_name\": \"NVIDIA A100-SXM4-40GB\"\n",
            "}\n",
            "Workspace ready at /content/anti_echo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup 2 of N: config and paths bootstrap (robust fetch)\n",
        "\n",
        "Goal\n",
        "- Load shared config from GitHub with fallback paths\n",
        "- Cache config locally for this session\n",
        "- Initialize runtime paths and print key settings\n",
        "\n",
        "Note\n",
        "- Tries multiple candidate filenames for `stance_axes` and `topic_labels` in case they are saved with .json or .yaml\n"
      ],
      "metadata": {
        "id": "rJEp6xNjHjAz"
      },
      "id": "rJEp6xNjHjAz"
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup 2 of N: config and paths bootstrap (robust fetch)\n",
        "\n",
        "import os\n",
        "import json\n",
        "import yaml\n",
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "PROJECT_ROOT = Path(\"/content/anti_echo\").resolve()\n",
        "CONFIG_CACHE = PROJECT_ROOT / \"config_cache\"\n",
        "CONFIG_CACHE.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "REPO_OWNER = \"AHMerrill\"\n",
        "REPO_NAME = \"anti-echo-chamber\"\n",
        "BRANCH = \"main\"\n",
        "\n",
        "def raw_url(path: str) -> str:\n",
        "    return f\"https://raw.githubusercontent.com/{REPO_OWNER}/{REPO_NAME}/{BRANCH}/{path.lstrip('/')}\"\n",
        "\n",
        "def fetch_text_first(paths):\n",
        "    last_err = None\n",
        "    tried = []\n",
        "    for p in paths:\n",
        "        url = raw_url(p)\n",
        "        tried.append(url)\n",
        "        try:\n",
        "            r = requests.get(url, timeout=20)\n",
        "            if r.status_code == 200 and r.text.strip():\n",
        "                return r.text, p, url\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "    msg = \"Could not fetch any of the candidate paths.\\nTried:\\n\" + \"\\n\".join(tried)\n",
        "    if last_err:\n",
        "        msg += f\"\\nLast error: {type(last_err).__name__}: {last_err}\"\n",
        "    raise RuntimeError(msg)\n",
        "\n",
        "# Candidate repo paths\n",
        "CFG_CANDIDATES = [\n",
        "    \"config/config.yaml\",\n",
        "    \"config/config.yml\",\n",
        "    \"config/config.json\",\n",
        "]\n",
        "STANCE_CANDIDATES = [\n",
        "    \"config/stance_axes.json\",\n",
        "    \"config/stance_axes.yaml\",\n",
        "    \"config/stance_axes.yml\",\n",
        "    \"config/stance_axes\",\n",
        "]\n",
        "TOPIC_CANDIDATES = [\n",
        "    \"config/topic_labels.json\",\n",
        "    \"config/topic_labels.yaml\",\n",
        "    \"config/topic_labels.yml\",\n",
        "    \"config/topic_labels\",\n",
        "]\n",
        "\n",
        "# Fetch config files\n",
        "cfg_txt, cfg_path, cfg_url = fetch_text_first(CFG_CANDIDATES)\n",
        "stance_txt, stance_path, stance_url = fetch_text_first(STANCE_CANDIDATES)\n",
        "topic_txt, topic_path, topic_url = fetch_text_first(TOPIC_CANDIDATES)\n",
        "\n",
        "# Cache copies\n",
        "(CONFIG_CACHE / Path(cfg_path).name).write_text(cfg_txt, encoding=\"utf-8\")\n",
        "(CONFIG_CACHE / (Path(stance_path).name if Path(stance_path).suffix else \"stance_axes.json\")).write_text(stance_txt, encoding=\"utf-8\")\n",
        "(CONFIG_CACHE / (Path(topic_path).name if Path(topic_path).suffix else \"topic_labels.json\")).write_text(topic_txt, encoding=\"utf-8\")\n",
        "\n",
        "# Parse helpers\n",
        "def parse_maybe_json_or_yaml(txt: str):\n",
        "    txt = txt.strip()\n",
        "    # try json first\n",
        "    try:\n",
        "        return json.loads(txt)\n",
        "    except Exception:\n",
        "        pass\n",
        "    # then yaml\n",
        "    try:\n",
        "        return yaml.safe_load(txt)\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Failed to parse as JSON or YAML: {e}\")\n",
        "\n",
        "# Parse into Python objects\n",
        "if cfg_path.endswith((\".yaml\", \".yml\")):\n",
        "    CONFIG = yaml.safe_load(cfg_txt)\n",
        "elif cfg_path.endswith(\".json\"):\n",
        "    CONFIG = json.loads(cfg_txt)\n",
        "else:\n",
        "    # default to YAML for config\n",
        "    CONFIG = yaml.safe_load(cfg_txt)\n",
        "\n",
        "STANCE_AXES = parse_maybe_json_or_yaml(stance_txt)\n",
        "TOPIC_LABELS = parse_maybe_json_or_yaml(topic_txt)\n",
        "\n",
        "# Validate minimum keys\n",
        "required_cfg_keys = [\"hf_dataset_id\", \"chroma_collections\", \"embeddings\", \"batch\", \"ids\", \"chroma\"]\n",
        "missing = [k for k in required_cfg_keys if k not in CONFIG]\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing required config keys: {missing}\")\n",
        "\n",
        "# Create runtime subdirs\n",
        "for key, path in {\n",
        "    \"raw\": \"raw\",\n",
        "    \"batches\": CONFIG[\"batch\"][\"base_dir\"],\n",
        "    \"chroma_db\": CONFIG[\"chroma\"][\"dir\"],\n",
        "    \"logs\": CONFIG.get(\"logging\", {}).get(\"save_dir\", \"logs\"),\n",
        "    \"tmp\": \"tmp\"\n",
        "}.items():\n",
        "    (PROJECT_ROOT / path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Print a concise summary\n",
        "summary = {\n",
        "    \"hf_dataset_id\": CONFIG[\"hf_dataset_id\"],\n",
        "    \"collections\": CONFIG[\"chroma_collections\"],\n",
        "    \"embeddings\": {\n",
        "        \"topic_model\": CONFIG[\"embeddings\"][\"topic_model\"],\n",
        "        \"stance_model\": CONFIG[\"embeddings\"][\"stance_model\"],\n",
        "        \"dim\": CONFIG[\"embeddings\"][\"dim\"],\n",
        "        \"dtype\": CONFIG[\"embeddings\"][\"dtype\"],\n",
        "        \"pooling\": CONFIG[\"embeddings\"][\"pooling\"],\n",
        "        \"chunk_tokens\": CONFIG[\"embeddings\"][\"chunk_tokens\"]\n",
        "    },\n",
        "    \"summarizer\": CONFIG.get(\"summarizer\", {}),\n",
        "    \"batch_files\": {\n",
        "        \"topic_file\": CONFIG[\"batch\"][\"topic_file\"],\n",
        "        \"stance_file\": CONFIG[\"batch\"][\"stance_file\"],\n",
        "        \"metadata_file\": CONFIG[\"batch\"][\"metadata_file\"],\n",
        "        \"manifest_name\": CONFIG[\"batch\"][\"manifest_name\"],\n",
        "        \"base_dir\": CONFIG[\"batch\"][\"base_dir\"]\n",
        "    },\n",
        "    \"id_policy\": CONFIG[\"ids\"],\n",
        "    \"paths\": {\n",
        "        \"project_root\": str(PROJECT_ROOT),\n",
        "        \"config_cache\": str(CONFIG_CACHE),\n",
        "        \"raw\": str(PROJECT_ROOT / \"raw\"),\n",
        "        \"batches\": str(PROJECT_ROOT / CONFIG[\"batch\"][\"base_dir\"]),\n",
        "        \"chroma_db\": str(PROJECT_ROOT / CONFIG[\"chroma\"][\"dir\"]),\n",
        "        \"logs\": str(PROJECT_ROOT / CONFIG.get(\"logging\", {}).get(\"save_dir\", \"logs\")),\n",
        "        \"tmp\": str(PROJECT_ROOT / \"tmp\")\n",
        "    },\n",
        "    \"loaded\": {\n",
        "        \"stance_axes_count\": len(STANCE_AXES) if isinstance(STANCE_AXES, (list, dict)) else \"unknown\",\n",
        "        \"topic_labels_count\": len(TOPIC_LABELS) if isinstance(TOPIC_LABELS, (list, dict)) else \"unknown\"\n",
        "    },\n",
        "    \"source_urls\": {\n",
        "        \"config\": cfg_url,\n",
        "        \"stance_axes\": stance_url,\n",
        "        \"topic_labels\": topic_url\n",
        "    }\n",
        "}\n",
        "\n",
        "print(json.dumps(summary, indent=2))\n",
        "\n",
        "# Make HF dataset id available to later cells\n",
        "os.environ[\"HF_DATASET_ID\"] = CONFIG[\"hf_dataset_id\"]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhAwiP7LHjgu",
        "outputId": "dfed6ce6-fe62-4b03-da57-ed7b0365d864"
      },
      "id": "vhAwiP7LHjgu",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"hf_dataset_id\": \"zanimal/anti-echo-artifacts\",\n",
            "  \"collections\": {\n",
            "    \"topic\": \"news_topic\",\n",
            "    \"stance\": \"news_stance\"\n",
            "  },\n",
            "  \"embeddings\": {\n",
            "    \"topic_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
            "    \"stance_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
            "    \"dim\": 384,\n",
            "    \"dtype\": \"float16\",\n",
            "    \"pooling\": \"mean\",\n",
            "    \"chunk_tokens\": 512\n",
            "  },\n",
            "  \"summarizer\": {\n",
            "    \"model\": \"facebook/bart-large-cnn\",\n",
            "    \"target_sentences\": 5,\n",
            "    \"truncation\": 2048\n",
            "  },\n",
            "  \"batch_files\": {\n",
            "    \"topic_file\": \"embeddings_topic.npz\",\n",
            "    \"stance_file\": \"embeddings_stance.npz\",\n",
            "    \"metadata_file\": \"metadata.jsonl\",\n",
            "    \"manifest_name\": \"manifest.json\",\n",
            "    \"base_dir\": \"batches\"\n",
            "  },\n",
            "  \"id_policy\": {\n",
            "    \"scheme\": \"domain-slug-sha12\",\n",
            "    \"hash\": \"sha256\",\n",
            "    \"normalize_whitespace\": true,\n",
            "    \"lowercase\": true\n",
            "  },\n",
            "  \"paths\": {\n",
            "    \"project_root\": \"/content/anti_echo\",\n",
            "    \"config_cache\": \"/content/anti_echo/config_cache\",\n",
            "    \"raw\": \"/content/anti_echo/raw\",\n",
            "    \"batches\": \"/content/anti_echo/batches\",\n",
            "    \"chroma_db\": \"/content/anti_echo/chroma_db\",\n",
            "    \"logs\": \"/content/anti_echo/logs\",\n",
            "    \"tmp\": \"/content/anti_echo/tmp\"\n",
            "  },\n",
            "  \"loaded\": {\n",
            "    \"stance_axes_count\": 49,\n",
            "    \"topic_labels_count\": 24\n",
            "  },\n",
            "  \"source_urls\": {\n",
            "    \"config\": \"https://raw.githubusercontent.com/AHMerrill/anti-echo-chamber/main/config/config.yaml\",\n",
            "    \"stance_axes\": \"https://raw.githubusercontent.com/AHMerrill/anti-echo-chamber/main/config/stance_axes.json\",\n",
            "    \"topic_labels\": \"https://raw.githubusercontent.com/AHMerrill/anti-echo-chamber/main/config/topic_labels.json\"\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup 3 of N: Hugging Face auth and registry pull\n",
        "\n",
        "Goal\n",
        "- Authenticate to Hugging Face with HF_TOKEN\n",
        "- Fetch the batch registry from GitHub\n",
        "- Validate the registry schema and summarize batches\n",
        "\n",
        "Notes\n",
        "- If HF_TOKEN is not set, you can still proceed to read public data but uploads will fail later\n",
        "- The registry lives at artifacts/artifacts_registry.json in your GitHub repo\n"
      ],
      "metadata": {
        "id": "7FuW4A7ELmrx"
      },
      "id": "7FuW4A7ELmrx"
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup 3A: load tokens once for the whole notebook (HF + GitHub)\n",
        "\n",
        "import os\n",
        "from getpass import getpass\n",
        "import requests\n",
        "\n",
        "def need(envvar, prompt):\n",
        "    if not os.environ.get(envvar, \"\").strip():\n",
        "        os.environ[envvar] = getpass(prompt)\n",
        "    print(f\"{envvar} loaded:\", bool(os.environ.get(envvar)))\n",
        "\n",
        "# Enter each once per Colab session\n",
        "need(\"HF_TOKEN\", \"Enter your Hugging Face token: \")\n",
        "need(\"GITHUB_TOKEN\", \"Enter your GitHub Personal Access Token: \")\n",
        "\n",
        "# Quick sanity checks (optional)\n",
        "try:\n",
        "    r = requests.get(\"https://api.github.com/user\",\n",
        "                     headers={\"Authorization\": f\"Bearer {os.environ['GITHUB_TOKEN']}\",\n",
        "                              \"Accept\": \"application/vnd.github+json\"},\n",
        "                     timeout=15)\n",
        "    print(\"GitHub auth status:\", r.status_code)\n",
        "except Exception as e:\n",
        "    print(\"GitHub auth check failed:\", type(e).__name__, e)\n",
        "\n",
        "try:\n",
        "    from huggingface_hub import whoami\n",
        "    me = whoami(token=os.environ[\"HF_TOKEN\"])\n",
        "    print(\"HF user:\", me.get(\"name\") or me.get(\"email\") or \"(ok)\")\n",
        "except Exception as e:\n",
        "    print(\"HF auth check failed:\", type(e).__name__, e)\n"
      ],
      "metadata": {
        "id": "D0Si5GInpuhV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f370b5a8-89f2-461b-822e-6f003c40c4b9"
      },
      "id": "D0Si5GInpuhV",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Hugging Face token: ··········\n",
            "HF_TOKEN loaded: True\n",
            "Enter your GitHub Personal Access Token: ··········\n",
            "GITHUB_TOKEN loaded: True\n",
            "GitHub auth status: 200\n",
            "HF user: zanimal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # --- temporary auth cell for Colab session ---\n",
        "# import os\n",
        "# from getpass import getpass\n",
        "\n",
        "# if \"HF_TOKEN\" not in os.environ or not os.environ[\"HF_TOKEN\"].strip():\n",
        "#     os.environ[\"HF_TOKEN\"] = getpass(\"Enter your Hugging Face token: \")\n",
        "\n",
        "# print(\"HF_TOKEN set in environment for this session (will reset when runtime restarts).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZlphFLgMJWk",
        "outputId": "9253db25-e85a-402d-e740-f1c3c595627f"
      },
      "id": "8ZlphFLgMJWk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Hugging Face token: ··········\n",
            "HF_TOKEN set in environment for this session (will reset when runtime restarts).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup 3 of N: Hugging Face auth and registry pull\n",
        "\n",
        "import os\n",
        "import json\n",
        "import requests\n",
        "from pathlib import Path\n",
        "from huggingface_hub import login, HfApi\n",
        "\n",
        "PROJECT_ROOT = Path(\"/content/anti_echo\").resolve()\n",
        "CACHE_DIR = PROJECT_ROOT / \"registry_cache\"\n",
        "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "REPO_OWNER = \"AHMerrill\"\n",
        "REPO_NAME = \"anti-echo-chamber\"\n",
        "BRANCH = \"main\"\n",
        "\n",
        "def raw_url(path: str) -> str:\n",
        "    return f\"https://raw.githubusercontent.com/{REPO_OWNER}/{REPO_NAME}/{BRANCH}/{path.lstrip('/')}\"\n",
        "\n",
        "# 1) HF auth\n",
        "HF_TOKEN = os.environ.get(\"HF_TOKEN\", \"\").strip()\n",
        "if HF_TOKEN:\n",
        "    try:\n",
        "        login(token=HF_TOKEN, add_to_git_credential=False)\n",
        "        print(\"Hugging Face login: OK\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: HF login failed: {type(e).__name__}: {e}\")\n",
        "else:\n",
        "    print(\"Warning: HF_TOKEN not set. You can read public artifacts but cannot upload.\")\n",
        "\n",
        "# 2) Validate the dataset exists\n",
        "HF_DATASET_ID = os.environ.get(\"HF_DATASET_ID\", \"\").strip()\n",
        "if not HF_DATASET_ID:\n",
        "    raise RuntimeError(\"HF_DATASET_ID not set in environment. It should have been set by Setup 2 from config.\")\n",
        "try:\n",
        "    api = HfApi()\n",
        "    ds_info = api.repo_info(HF_DATASET_ID, repo_type=\"dataset\")\n",
        "    print(f\"HF dataset found: {HF_DATASET_ID}\")\n",
        "except Exception as e:\n",
        "    print(f\"Warning: Could not verify HF dataset {HF_DATASET_ID}: {type(e).__name__}: {e}\")\n",
        "\n",
        "# 3) Pull registry from GitHub\n",
        "REGISTRY_URL = raw_url(\"artifacts/artifacts_registry.json\")\n",
        "r = requests.get(REGISTRY_URL, timeout=20)\n",
        "if r.status_code != 200:\n",
        "    raise RuntimeError(f\"Failed to fetch registry from {REGISTRY_URL}. Status {r.status_code}\")\n",
        "registry_txt = r.text\n",
        "(REGISTRY_CACHE_PATH := CACHE_DIR / \"artifacts_registry.json\").write_text(registry_txt, encoding=\"utf-8\")\n",
        "\n",
        "try:\n",
        "    REGISTRY = json.loads(registry_txt)\n",
        "except Exception as e:\n",
        "    raise ValueError(f\"Registry JSON parse failed: {e}\")\n",
        "\n",
        "# 4) Minimal schema checks and summary\n",
        "required_top = [\"version\", \"models\", \"batches\"]\n",
        "missing = [k for k in required_top if k not in REGISTRY]\n",
        "if missing:\n",
        "    raise ValueError(f\"Registry missing required keys: {missing}\")\n",
        "\n",
        "models_block = REGISTRY.get(\"models\", {})\n",
        "batches = REGISTRY.get(\"batches\", [])\n",
        "model_summary = {\n",
        "    \"topic\": models_block.get(\"topic\"),\n",
        "    \"stance\": models_block.get(\"stance\"),\n",
        "    \"dim\": models_block.get(\"dim\")\n",
        "}\n",
        "\n",
        "summary = {\n",
        "    \"registry_version\": REGISTRY.get(\"version\"),\n",
        "    \"models\": model_summary,\n",
        "    \"batch_count\": len(batches),\n",
        "}\n",
        "\n",
        "# Print concise summary\n",
        "print(json.dumps(summary, indent=2))\n",
        "\n",
        "# If batches exist, show a compact table\n",
        "if batches:\n",
        "    rows = []\n",
        "    for b in batches:\n",
        "        rows.append({\n",
        "            \"batch_id\": b.get(\"batch_id\"),\n",
        "            \"docs\": b.get(\"counts\", {}).get(\"docs\"),\n",
        "            \"created_at\": b.get(\"created_at\"),\n",
        "        })\n",
        "    # Keep it readable\n",
        "    print(\"Batches overview:\")\n",
        "    for row in rows:\n",
        "        print(f\"- {row['batch_id']} | docs={row['docs']} | created_at={row['created_at']}\")\n",
        "else:\n",
        "    print(\"No batches listed yet in artifacts_registry.json\")\n",
        "\n",
        "# Make available to later cells\n",
        "os.environ[\"REGISTRY_PATH\"] = str(REGISTRY_CACHE_PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8hxn43ELo1G",
        "outputId": "064c4417-7f26-4352-ebdf-d997ae8e653d"
      },
      "id": "w8hxn43ELo1G",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
            "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hugging Face login: OK\n",
            "HF dataset found: zanimal/anti-echo-artifacts\n",
            "{\n",
            "  \"registry_version\": 2,\n",
            "  \"models\": {\n",
            "    \"topic\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
            "    \"stance\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
            "    \"dim\": 384\n",
            "  },\n",
            "  \"batch_count\": 1\n",
            "}\n",
            "Batches overview:\n",
            "- batch_20251011T232938Z_283ca40f | docs=None | created_at=20251011T232938Z\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup 4 of N: Chroma rebuild or initialize\n",
        "\n",
        "Goal\n",
        "- Create a persistent Chroma client under `/content/anti_echo/chroma_db`\n",
        "- Ensure two collections exist: `news_topic` and `news_stance`\n",
        "- If batches are listed in the registry, download and ingest them in order\n",
        "- If no batches yet, initialize empty collections and print a clear summary\n",
        "\n",
        "Notes\n",
        "- Uses `artifacts/artifacts_registry.json` as the source of truth\n",
        "- Validates shapes and dims before inserting\n",
        "- Safe to re run\n"
      ],
      "metadata": {
        "id": "B8nvAQxhNAcH"
      },
      "id": "B8nvAQxhNAcH"
    },
    {
      "cell_type": "code",
      "source": [
        "# Hotfix: tolerant Chroma rebuild that accepts either 'hf_paths' or 'paths' in registry\n",
        "\n",
        "import os, json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from huggingface_hub import hf_hub_download\n",
        "import chromadb\n",
        "\n",
        "PROJECT_ROOT = Path(\"/content/anti_echo\").resolve()\n",
        "CHROMA_DIR = PROJECT_ROOT / \"chroma_db\"\n",
        "REGISTRY_PATH = Path(os.environ[\"REGISTRY_PATH\"])\n",
        "HF_DATASET_ID = os.environ[\"HF_DATASET_ID\"]\n",
        "\n",
        "COLL_TOPIC = CONFIG[\"chroma_collections\"][\"topic\"]\n",
        "COLL_STANCE = CONFIG[\"chroma_collections\"][\"stance\"]\n",
        "EMB_DIM = int(CONFIG[\"embeddings\"][\"dim\"])\n",
        "\n",
        "# Disable Chroma telemetry noise\n",
        "os.environ[\"CHROMA_TELEMETRY_ENABLED\"] = \"false\"\n",
        "os.environ[\"ANONYMIZED_TELEMETRY\"] = \"false\"\n",
        "\n",
        "def load_registry(path: Path):\n",
        "    return json.loads(path.read_text(encoding=\"utf-8\"))\n",
        "\n",
        "def read_metadata_jsonl(fp: Path):\n",
        "    ids, metas = [], []\n",
        "    with fp.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            obj = json.loads(line)\n",
        "            ids.append(obj[\"id\"])\n",
        "            metas.append(obj)\n",
        "    return ids, metas\n",
        "\n",
        "def load_npz_vectors(fp: Path, expected_dim: int):\n",
        "    arr = np.load(fp, allow_pickle=False)\n",
        "    if isinstance(arr, np.lib.npyio.NpzFile):\n",
        "        keys = list(arr.files)\n",
        "        arr = arr[keys[0]] if keys else None\n",
        "    if arr is None:\n",
        "        raise ValueError(f\"No array found in {fp.name}\")\n",
        "    vecs = np.asarray(arr)\n",
        "    if vecs.ndim != 2 or vecs.shape[1] != expected_dim:\n",
        "        raise ValueError(f\"Bad embedding shape in {fp.name}. Got {vecs.shape}, expected [N, {expected_dim}]\")\n",
        "    if not np.isfinite(vecs).all():\n",
        "        raise ValueError(f\"Non finite values in {fp.name}\")\n",
        "    return vecs\n",
        "\n",
        "def ensure_chroma():\n",
        "    client = chromadb.PersistentClient(path=str(CHROMA_DIR))\n",
        "    topic = client.get_or_create_collection(name=COLL_TOPIC, metadata={\"hnsw:space\": \"cosine\"})\n",
        "    stance = client.get_or_create_collection(name=COLL_STANCE, metadata={\"hnsw:space\": \"cosine\"})\n",
        "    return client, topic, stance\n",
        "\n",
        "def upsert_in_chunks(collection, ids, vectors, metadatas, chunk=2048):\n",
        "    n = len(ids)\n",
        "    for i in range(0, n, chunk):\n",
        "        j = min(i + chunk, n)\n",
        "        collection.upsert(\n",
        "            ids=ids[i:j],\n",
        "            embeddings=vectors[i:j].tolist(),\n",
        "            metadatas=metadatas[i:j],\n",
        "        )\n",
        "\n",
        "def ingest_one(path_block, topic_coll, stance_coll, batch_id=\"unknown\"):\n",
        "    t_rel = path_block.get(\"embeddings_topic\")\n",
        "    s_rel = path_block.get(\"embeddings_stance\")\n",
        "    m_rel = path_block.get(\"metadata\")\n",
        "    manifest_rel = path_block.get(\"manifest\")\n",
        "    if not all([t_rel, s_rel, m_rel, manifest_rel]):\n",
        "        raise ValueError(f\"Incomplete paths for batch {batch_id}\")\n",
        "\n",
        "    t_local = Path(hf_hub_download(repo_id=HF_DATASET_ID, repo_type=\"dataset\", filename=t_rel))\n",
        "    s_local = Path(hf_hub_download(repo_id=HF_DATASET_ID, repo_type=\"dataset\", filename=s_rel))\n",
        "    m_local = Path(hf_hub_download(repo_id=HF_DATASET_ID, repo_type=\"dataset\", filename=m_rel))\n",
        "    _ = Path(hf_hub_download(repo_id=HF_DATASET_ID, repo_type=\"dataset\", filename=manifest_rel))\n",
        "\n",
        "    ids, metas = read_metadata_jsonl(m_local)\n",
        "    t_vecs = load_npz_vectors(t_local, EMB_DIM)\n",
        "    s_vecs = load_npz_vectors(s_local, EMB_DIM)\n",
        "    if len(ids) != t_vecs.shape[0] or len(ids) != s_vecs.shape[0]:\n",
        "        raise ValueError(f\"Row count mismatch in batch {batch_id}\")\n",
        "\n",
        "    upsert_in_chunks(topic_coll, ids, t_vecs, metas)\n",
        "    upsert_in_chunks(stance_coll, ids, s_vecs, metas)\n",
        "    return len(ids)\n",
        "\n",
        "# Load and normalize batches\n",
        "REGISTRY = load_registry(REGISTRY_PATH)\n",
        "raw_batches = REGISTRY.get(\"batches\", [])\n",
        "norm = []\n",
        "for b in raw_batches:\n",
        "    p = b.get(\"hf_paths\") or b.get(\"paths\") or {}\n",
        "    if p:\n",
        "        norm.append({\"batch_id\": b.get(\"batch_id\", \"unknown\"), \"paths\": p})\n",
        "    else:\n",
        "        print(f\"Skipping batch with no paths: {b.get('batch_id')}\")\n",
        "\n",
        "client, topic_coll, stance_coll = ensure_chroma()\n",
        "\n",
        "total = 0\n",
        "for b in norm:\n",
        "    try:\n",
        "        n = ingest_one(b[\"paths\"], topic_coll, stance_coll, batch_id=b[\"batch_id\"])\n",
        "        print(f\"Ingested batch {b['batch_id']}: +{n} docs\")\n",
        "        total += n\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: failed to ingest {b['batch_id']}: {type(e).__name__}: {e}\")\n",
        "\n",
        "print(\"Chroma rebuild summary:\", {\n",
        "    \"topic_count\": topic_coll.count(),\n",
        "    \"stance_count\": stance_coll.count(),\n",
        "    \"docs_added\": total,\n",
        "    \"store\": str(CHROMA_DIR),\n",
        "})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "cFlG77sFNAzu",
        "outputId": "d181e7b8-1a99-4362-8774-997c2a5f62ec"
      },
      "id": "cFlG77sFNAzu",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "An instance of Chroma already exists for /content/anti_echo/chroma_db with different settings",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1632808827.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Skipping batch with no paths: {b.get('batch_id')}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m \u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_coll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstance_coll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_chroma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1632808827.py\u001b[0m in \u001b[0;36mensure_chroma\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mensure_chroma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchromadb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPersistentClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHROMA_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mtopic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_or_create_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCOLL_TOPIC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"hnsw:space\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"cosine\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_or_create_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCOLL_STANCE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"hnsw:space\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"cosine\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/chromadb/__init__.py\u001b[0m in \u001b[0;36mPersistentClient\u001b[0;34m(path, settings, tenant, database)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mdatabase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mClientCreator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtenant\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtenant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/chromadb/api/client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tenant, database, settings)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0msettings\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSettings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSettings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     ) -> None:\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtenant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtenant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatabase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/chromadb/api/shared_system_client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, settings)\u001b[0m\n\u001b[1;32m     17\u001b[0m     ) -> None:\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_identifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSharedSystemClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_identifier_from_settings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mSharedSystemClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_system_if_not_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_identifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/chromadb/api/shared_system_client.py\u001b[0m in \u001b[0;36m_create_system_if_not_exists\u001b[0;34m(cls, identifier, settings)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;31m# For now, the settings must match\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mprevious_system\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m     39\u001b[0m                     \u001b[0;34mf\"An instance of Chroma already exists for {identifier} with different settings\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 )\n",
            "\u001b[0;31mValueError\u001b[0m: An instance of Chroma already exists for /content/anti_echo/chroma_db with different settings"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup 5A: tunables and Guardian feeds\n",
        "\n",
        "Use this cell to:\n",
        "- Set how many total articles to scrape per run\n",
        "- Set optional per feed caps\n",
        "- Pick the date floor\n",
        "- Control even distribution across feeds (with remainder to a preferred feed)\n",
        "- Define the full Guardian feed list in one place\n",
        "\n",
        "Notes\n",
        "- The current default will scrape 30 total articles, evenly split across the feeds below, with any remainder to Comment is Free.\n",
        "- If MAX_ARTICLES is smaller than the number of feeds, many feeds will get 0 for that run. Increase MAX_ARTICLES to cover more feeds per run.\n",
        "- We will wire the scraper to read these values from the environment and JSON so you can change them here only.\n"
      ],
      "metadata": {
        "id": "AR-rVGInQqeZ"
      },
      "id": "AR-rVGInQqeZ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup 5A: tunables and Guardian feeds\n",
        "\n",
        "import os, json\n",
        "\n",
        "# ---- How many articles and distribution policy ----\n",
        "MAX_ARTICLES = 250              # total across all feeds this run\n",
        "MAX_PER_FEED = None            # None for no hard cap, or set an int (e.g., 3)\n",
        "DATE_FROM = \"2025-07-01\"       # ISO-8601 UTC lower bound; set None to ignore\n",
        "FORCE_REFETCH = False          # True to re-download even if cached\n",
        "EVEN_SPLIT = True              # True to evenly split MAX_ARTICLES across feeds\n",
        "QUOTA_REMAINDER_TO = \"commentisfree\"  # where to send the remainder from the even split\n",
        "\n",
        "# ---- Guardian feeds (edit here to add/remove) ----\n",
        "GUARDIAN_FEEDS = [\n",
        "    (\"world\",           \"https://www.theguardian.com/world/rss\"),\n",
        "    (\"uk-news\",         \"https://www.theguardian.com/uk-news/rss\"),\n",
        "    (\"us-news\",         \"https://www.theguardian.com/us-news/rss\"),\n",
        "    (\"politics\",        \"https://www.theguardian.com/politics/rss\"),\n",
        "    (\"europe\",          \"https://www.theguardian.com/world/europe/rss\"),\n",
        "    (\"americas\",        \"https://www.theguardian.com/world/americas/rss\"),\n",
        "    (\"asia\",            \"https://www.theguardian.com/world/asia/rss\"),\n",
        "    (\"australia-news\",  \"https://www.theguardian.com/australia-news/rss\"),\n",
        "    (\"business\",        \"https://www.theguardian.com/uk/business/rss\"),\n",
        "    (\"money\",           \"https://www.theguardian.com/uk/money/rss\"),\n",
        "    (\"technology\",      \"https://www.theguardian.com/uk/technology/rss\"),\n",
        "    (\"science\",         \"https://www.theguardian.com/science/rss\"),\n",
        "    (\"global-development\",\"https://www.theguardian.com/global-development/rss\"),\n",
        "    (\"environment\",     \"https://www.theguardian.com/uk/environment/rss\"),\n",
        "    (\"wildlife\",        \"https://www.theguardian.com/environment/wildlife/rss\"),\n",
        "    (\"pollution\",       \"https://www.theguardian.com/environment/pollution/rss\"),\n",
        "    (\"climate-crisis\",  \"https://www.theguardian.com/environment/climate-crisis/rss\"),\n",
        "    (\"sport\",           \"https://www.theguardian.com/uk/sport/rss\"),\n",
        "    (\"football\",        \"https://www.theguardian.com/football/rss\"),\n",
        "    (\"cricket\",         \"https://www.theguardian.com/sport/cricket/rss\"),\n",
        "    (\"tennis\",          \"https://www.theguardian.com/sport/tennis/rss\"),\n",
        "    (\"golf\",            \"https://www.theguardian.com/sport/golf/rss\"),\n",
        "    (\"formulaone\",      \"https://www.theguardian.com/sport/formulaone/rss\"),\n",
        "    (\"cycling\",         \"https://www.theguardian.com/sport/cycling/rss\"),\n",
        "    (\"rugby-union\",     \"https://www.theguardian.com/sport/rugby-union/rss\"),\n",
        "    (\"culture\",         \"https://www.theguardian.com/uk/culture/rss\"),\n",
        "    (\"film\",            \"https://www.theguardian.com/uk/film/rss\"),\n",
        "    (\"music\",           \"https://www.theguardian.com/music/rss\"),\n",
        "    (\"artanddesign\",    \"https://www.theguardian.com/artanddesign/rss\"),\n",
        "    (\"books\",           \"https://www.theguardian.com/books/rss\"),\n",
        "    (\"tv-and-radio\",    \"https://www.theguardian.com/uk/tv-and-radio/rss\"),\n",
        "    (\"lifestyle\",       \"https://www.theguardian.com/uk/lifeandstyle/rss\"),\n",
        "    (\"family\",          \"https://www.theguardian.com/lifeandstyle/family/rss\"),\n",
        "    (\"health\",          \"https://www.theguardian.com/lifeandstyle/health-and-wellbeing/rss\"),\n",
        "    (\"inequality\",      \"https://www.theguardian.com/inequality/rss\"),\n",
        "    (\"obituaries\",      \"https://www.theguardian.com/tone/obituaries/rss\"),\n",
        "    (\"travel\",          \"https://www.theguardian.com/uk/travel/rss\"),\n",
        "    (\"fashion\",         \"https://www.theguardian.com/fashion/rss\"),\n",
        "    (\"games\",           \"https://www.theguardian.com/games/rss\"),\n",
        "    (\"stage\",           \"https://www.theguardian.com/stage/rss\"),\n",
        "    (\"crosswords\",      \"https://www.theguardian.com/crosswords/rss\"),\n",
        "    (\"commentisfree\",   \"https://www.theguardian.com/commentisfree/rss\")  # opinion\n",
        "]\n",
        "\n",
        "# ---- Export to environment so the scraper can read without edits ----\n",
        "os.environ[\"MAX_ARTICLES\"] = str(MAX_ARTICLES)\n",
        "os.environ[\"MAX_PER_FEED\"] = \"\" if MAX_PER_FEED is None else str(MAX_PER_FEED)\n",
        "os.environ[\"DATE_FROM\"] = \"\" if DATE_FROM in (None, \"\") else DATE_FROM\n",
        "os.environ[\"FORCE_REFETCH\"] = \"true\" if FORCE_REFETCH else \"false\"\n",
        "os.environ[\"EVEN_SPLIT\"] = \"true\" if EVEN_SPLIT else \"false\"\n",
        "os.environ[\"QUOTA_REMAINDER_TO\"] = QUOTA_REMAINDER_TO\n",
        "\n",
        "# Serialize feeds to JSON as a list of [name, url]\n",
        "os.environ[\"GUARDIAN_FEEDS_JSON\"] = json.dumps(GUARDIAN_FEEDS)\n",
        "\n",
        "print(\"Tunables and Guardian feeds set.\")\n",
        "print(f\"Feeds configured: {len(GUARDIAN_FEEDS)}\")\n",
        "print(f\"MAX_ARTICLES={MAX_ARTICLES}, MAX_PER_FEED={MAX_PER_FEED}, DATE_FROM={DATE_FROM}, EVEN_SPLIT={EVEN_SPLIT}, REMAINDER_TO={QUOTA_REMAINDER_TO}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAULiUriQr9n",
        "outputId": "8da6f8c0-9c50-4ecf-ad13-9d496779bdb1"
      },
      "id": "wAULiUriQr9n",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tunables and Guardian feeds set.\n",
            "Feeds configured: 42\n",
            "MAX_ARTICLES=250, MAX_PER_FEED=None, DATE_FROM=2025-07-01, EVEN_SPLIT=True, REMAINDER_TO=commentisfree\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup 5B.0: Restore prior feed state\n",
        "\n",
        "Purpose\n",
        "- Rehydrate `feeds/feeds_state.json` and `feeds/index.json` so the scraper skips URLs already scraped in previous runs or by collaborators.\n",
        "\n",
        "Source of truth\n",
        "- Hugging Face dataset `feeds/feeds_state_latest.json` and `feeds/feed_index_latest.json`\n",
        "- Fallback to GitHub `feeds/feeds_state_latest.json` and `feeds/feed_index_latest.json` if HF is missing\n",
        "\n",
        "Run this once before 5B on a fresh runtime.\n"
      ],
      "metadata": {
        "id": "LSRhQ_EAnuCQ"
      },
      "id": "LSRhQ_EAnuCQ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup 5B.reconstruct: rebuild index.json and feeds_state.json from HF metadata\n",
        "\n",
        "import os, json, datetime as dt, re, hashlib\n",
        "from pathlib import Path\n",
        "from typing import Dict, List\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "PROJECT_ROOT = Path(\"/content/anti_echo\").resolve()\n",
        "FEEDS_DIR = PROJECT_ROOT / \"feeds\"\n",
        "FEEDS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "INDEX_PATH = FEEDS_DIR / \"index.json\"\n",
        "STATE_PATH = FEEDS_DIR / \"feeds_state.json\"\n",
        "\n",
        "# Helper to match earlier hashing in your scraper\n",
        "def sha256_text(txt: str) -> str:\n",
        "    return hashlib.sha256(re.sub(r\"\\s+\", \" \", txt.strip().lower()).encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "def now_utc_iso() -> str:\n",
        "    return dt.datetime.now(dt.timezone.utc).isoformat()\n",
        "\n",
        "# Load CONFIG, REGISTRY\n",
        "try:\n",
        "    CONFIG  # from Setup 2\n",
        "except NameError:\n",
        "    raise RuntimeError(\"CONFIG not found. Run Setup 2 first.\")\n",
        "\n",
        "HF_DATASET_ID = CONFIG[\"hf_dataset_id\"]\n",
        "\n",
        "# Get registry dict\n",
        "reg_path_env = os.environ.get(\"REGISTRY_PATH\", \"\")\n",
        "if reg_path_env and Path(reg_path_env).exists():\n",
        "    REGISTRY = json.loads(Path(reg_path_env).read_text(encoding=\"utf-8\"))\n",
        "else:\n",
        "    raise RuntimeError(\"REGISTRY_PATH not set or not found. Run Setup 3 to fetch the registry.\")\n",
        "\n",
        "# Collect metadata JSONL files to read\n",
        "# Prefer the registry batches. If you added a latest pointer, it will be used next time.\n",
        "batches = REGISTRY.get(\"batches\", [])\n",
        "if not batches:\n",
        "    raise RuntimeError(\"No batches found in registry. Cannot reconstruct without metadata JSONL.\")\n",
        "\n",
        "meta_files: List[Path] = []\n",
        "for b in batches:\n",
        "    meta_rel = b.get(\"hf_paths\", {}).get(\"metadata\")\n",
        "    if not meta_rel:\n",
        "        continue\n",
        "    try:\n",
        "        local = Path(hf_hub_download(repo_id=HF_DATASET_ID, repo_type=\"dataset\", filename=meta_rel))\n",
        "        meta_files.append(local)\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: could not download metadata for batch {b.get('batch_id')}: {type(e).__name__}: {e}\")\n",
        "\n",
        "if not meta_files:\n",
        "    raise RuntimeError(\"Found no downloadable metadata files in registry.\")\n",
        "\n",
        "# Build URL set and basic meta\n",
        "urls_seen = []\n",
        "url_hashes = []\n",
        "items = {}  # for index.json\n",
        "\n",
        "for fp in meta_files:\n",
        "    with fp.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            try:\n",
        "                obj = json.loads(line)\n",
        "            except Exception:\n",
        "                continue\n",
        "            u = obj.get(\"url\")\n",
        "            if not u:\n",
        "                continue\n",
        "            if u in items:\n",
        "                continue\n",
        "            items[u] = {\"status\": \"ok\", \"fetched_at\": now_utc_iso()}\n",
        "            urls_seen.append(u)\n",
        "            url_hashes.append(sha256_text(u)[:12])\n",
        "\n",
        "# Write index.json\n",
        "index_obj = {\n",
        "    \"last_updated\": now_utc_iso(),\n",
        "    \"items\": items\n",
        "}\n",
        "INDEX_PATH.write_text(json.dumps(index_obj, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "# Build feeds_state.json\n",
        "# If you have GUARDIAN_FEEDS env from 5A, use those feed names. Otherwise make a single bucket.\n",
        "try:\n",
        "    import json as _json\n",
        "    GUARDIAN_FEEDS = _json.loads(os.environ.get(\"GUARDIAN_FEEDS_JSON\", \"[]\"))\n",
        "    feed_names = [name for name, _ in GUARDIAN_FEEDS] if GUARDIAN_FEEDS else []\n",
        "except Exception:\n",
        "    feed_names = []\n",
        "\n",
        "if not feed_names:\n",
        "    feed_names = [\"theguardian_world\", \"theguardian_uk\", \"theguardian_politics\", \"theguardian_environment\", \"theguardian_commentisfree\"]\n",
        "\n",
        "# Fill each feed ring buffer up to its max with the last N url hashes\n",
        "# We do not know which feed a given URL came from, but dedupe works regardless because 5B checks index[\"items\"]\n",
        "feeds_block: Dict[str, Dict] = {}\n",
        "for name in feed_names:\n",
        "    maxlen = 1000 if \"commentisfree\" in name else 500\n",
        "    feeds_block[name] = {\n",
        "        \"feed_url\": None,\n",
        "        \"last_cursor_iso\": None,\n",
        "        \"recent_url_hashes\": url_hashes[-maxlen:],\n",
        "        \"recent_url_hashes_max\": maxlen,\n",
        "        \"last_run_at\": None,\n",
        "        \"last_run_by\": \"reconstruct\",\n",
        "        \"notes\": f\"Reconstructed ring buffer for {name}\"\n",
        "    }\n",
        "\n",
        "state_obj = {\n",
        "    \"version\": 1,\n",
        "    \"updated_at\": now_utc_iso(),\n",
        "    \"feeds\": feeds_block\n",
        "}\n",
        "STATE_PATH.write_text(json.dumps(state_obj, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "print(\"Reconstruction complete.\")\n",
        "print({\n",
        "    \"urls_in_index\": len(items),\n",
        "    \"feeds_in_state\": len(feeds_block),\n",
        "    \"example_url\": urls_seen[0] if urls_seen else None,\n",
        "    \"index_path\": str(INDEX_PATH),\n",
        "    \"feeds_state_path\": str(STATE_PATH),\n",
        "})\n"
      ],
      "metadata": {
        "id": "iB3izd-Rv1WV",
        "outputId": "21b67c06-22a1-45ed-81eb-128ea72b00ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "id": "iB3izd-Rv1WV",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Found no downloadable metadata files in registry.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-457354459.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmeta_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Found no downloadable metadata files in registry.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m# Build URL set and basic meta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no downloadable metadata files in registry."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup 5B.0: Restore prior feed state from HF (preferred) or GitHub (fallback)\n",
        "\n",
        "import os, json, shutil, requests\n",
        "from pathlib import Path\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "PROJECT_ROOT = Path(\"/content/anti_echo\").resolve()\n",
        "FEEDS_DIR = PROJECT_ROOT / \"feeds\"\n",
        "FEEDS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "STATE_PATH = FEEDS_DIR / \"feeds_state.json\"\n",
        "INDEX_PATH = FEEDS_DIR / \"index.json\"\n",
        "\n",
        "HF_DATASET_ID = os.environ.get(\"HF_DATASET_ID\", \"\").strip()\n",
        "REPO_OWNER = \"AHMerrill\"\n",
        "REPO_NAME = \"anti-echo-chamber\"\n",
        "BRANCH = \"main\"\n",
        "\n",
        "def restore_from_hf() -> bool:\n",
        "    try:\n",
        "        st_local = hf_hub_download(\n",
        "            repo_id=HF_DATASET_ID,\n",
        "            repo_type=\"dataset\",\n",
        "            filename=\"feeds/feeds_state_latest.json\"\n",
        "        )\n",
        "        ix_local = hf_hub_download(\n",
        "            repo_id=HF_DATASET_ID,\n",
        "            repo_type=\"dataset\",\n",
        "            filename=\"feeds/feed_index_latest.json\"\n",
        "        )\n",
        "        shutil.copy(st_local, STATE_PATH)\n",
        "        shutil.copy(ix_local, INDEX_PATH)\n",
        "        print(\"Restored feed state from HF: feeds_state_latest.json, feed_index_latest.json\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"HF restore not available: {type(e).__name__}: {e}\")\n",
        "        return False\n",
        "\n",
        "def restore_from_github() -> bool:\n",
        "    try:\n",
        "        base = f\"https://raw.githubusercontent.com/{REPO_OWNER}/{REPO_NAME}/{BRANCH}/feeds\"\n",
        "        urls = {\n",
        "            STATE_PATH: f\"{base}/feeds_state_latest.json\",\n",
        "            INDEX_PATH: f\"{base}/feed_index_latest.json\",\n",
        "        }\n",
        "        ok = False\n",
        "        for dst, url in urls.items():\n",
        "            r = requests.get(url, timeout=20)\n",
        "            if r.status_code == 200 and r.text.strip():\n",
        "                dst.write_text(r.text, encoding=\"utf-8\")\n",
        "                ok = True\n",
        "        if ok:\n",
        "            print(\"Restored feed state from GitHub latest files.\")\n",
        "        else:\n",
        "            print(\"GitHub latest feed state not found.\")\n",
        "        return ok\n",
        "    except Exception as e:\n",
        "        print(f\"GitHub restore failed: {type(e).__name__}: {e}\")\n",
        "        return False\n",
        "\n",
        "restored = False\n",
        "if HF_DATASET_ID:\n",
        "    restored = restore_from_hf()\n",
        "if not restored:\n",
        "    restored = restore_from_github()\n",
        "\n",
        "if not restored:\n",
        "    print(\"No prior feed state found. Starting fresh.\")\n",
        "else:\n",
        "    try:\n",
        "        fs = json.loads(STATE_PATH.read_text(encoding=\"utf-8\"))\n",
        "        ix = json.loads(INDEX_PATH.read_text(encoding=\"utf-8\"))\n",
        "        print(f\"feeds_state.json feeds={len(fs.get('feeds', {}))}, updated_at={fs.get('updated_at')}\")\n",
        "        print(f\"index.json items={len(ix.get('items', {}))}, last_updated={ix.get('last_updated')}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Sanity read failed: {e}\")\n"
      ],
      "metadata": {
        "id": "pV8CM5ZlnvVl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0aa21f6-7aa4-4170-88ca-8d52c96107d6"
      },
      "id": "pV8CM5ZlnvVl",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HF restore not available: EntryNotFoundError: 404 Client Error. (Request ID: Root=1-68eb32ef-5aa20d691554c8a514566010;5837e9de-b70c-4d5e-a8a4-714a1a262107)\n",
            "\n",
            "Entry Not Found for url: https://huggingface.co/datasets/zanimal/anti-echo-artifacts/resolve/main/feeds/feeds_state_latest.json.\n",
            "GitHub latest feed state not found.\n",
            "No prior feed state found. Starting fresh.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup 5B: Guardian scraper that reads tunables and evenly splits quotas\n",
        "\n",
        "What this cell does\n",
        "- Reads all tunables from the prior cell (env vars) and your `GUARDIAN_FEEDS` list\n",
        "- Computes per feed quotas by even split of `MAX_ARTICLES`, sending remainder to `QUOTA_REMAINDER_TO`\n",
        "- Respects `MAX_PER_FEED`, `DATE_FROM`, and `FORCE_REFETCH`\n",
        "- Saves `raw/{id}.txt` and `raw/{id}.meta.json`\n",
        "- Updates `feeds/index.json` and writes a local copy of `feeds_state.json` you can commit later\n",
        "\n",
        "Tip\n",
        "- Change counts, dates, or feeds only in Setup 5A. Re run this cell to apply.\n"
      ],
      "metadata": {
        "id": "7lHNRC8VZ28B"
      },
      "id": "7lHNRC8VZ28B"
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup 5B: Guardian scraper using tunables from 5A\n",
        "\n",
        "import os, re, json, hashlib, datetime as dt\n",
        "from pathlib import Path\n",
        "from urllib.parse import urlparse\n",
        "from email.utils import parsedate_to_datetime\n",
        "import feedparser, trafilatura\n",
        "\n",
        "PROJECT_ROOT = Path(\"/content/anti_echo\").resolve()\n",
        "RAW_DIR = PROJECT_ROOT / \"raw\"\n",
        "FEEDS_DIR = PROJECT_ROOT / \"feeds\"\n",
        "FEEDS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Load tunables from env set in 5A\n",
        "try:\n",
        "    GUARDIAN_FEEDS = json.loads(os.environ[\"GUARDIAN_FEEDS_JSON\"])\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\"GUARDIAN_FEEDS_JSON missing. Run Setup 5A first.\") from e\n",
        "\n",
        "MAX_ARTICLES = int(os.environ.get(\"MAX_ARTICLES\", \"30\"))\n",
        "MAX_PER_FEED = os.environ.get(\"MAX_PER_FEED\", \"\")\n",
        "MAX_PER_FEED = None if MAX_PER_FEED == \"\" else int(MAX_PER_FEED)\n",
        "DATE_FROM = os.environ.get(\"DATE_FROM\", \"\") or None\n",
        "FORCE_REFETCH = os.environ.get(\"FORCE_REFETCH\", \"false\").lower() == \"true\"\n",
        "EVEN_SPLIT = os.environ.get(\"EVEN_SPLIT\", \"true\").lower() == \"true\"\n",
        "QUOTA_REMAINDER_TO = os.environ.get(\"QUOTA_REMAINDER_TO\", \"commentisfree\")\n",
        "\n",
        "INDEX_PATH = FEEDS_DIR / \"index.json\"\n",
        "STATE_PATH = FEEDS_DIR / \"feeds_state.json\"\n",
        "\n",
        "def now_utc():\n",
        "    return dt.datetime.now(dt.timezone.utc).isoformat()\n",
        "\n",
        "def load_index_local():\n",
        "    if INDEX_PATH.exists():\n",
        "        try:\n",
        "            return json.loads(INDEX_PATH.read_text(encoding=\"utf-8\"))\n",
        "        except Exception:\n",
        "            pass\n",
        "    return {\"last_updated\": None, \"items\": {}}\n",
        "\n",
        "def save_index_local(idx):\n",
        "    idx[\"last_updated\"] = now_utc()\n",
        "    INDEX_PATH.write_text(json.dumps(idx, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "def load_state():\n",
        "    if STATE_PATH.exists():\n",
        "        try:\n",
        "            return json.loads(STATE_PATH.read_text(encoding=\"utf-8\"))\n",
        "        except Exception:\n",
        "            pass\n",
        "    return {\"version\": 1, \"updated_at\": None, \"feeds\": {}}\n",
        "\n",
        "index = load_index_local()\n",
        "feeds_state = load_state()\n",
        "\n",
        "def parse_entry_date(entry):\n",
        "    for attr in (\"published\", \"updated\"):\n",
        "        try:\n",
        "            val = getattr(entry, attr, None) or entry.get(attr)\n",
        "        except Exception:\n",
        "            val = None\n",
        "        if val:\n",
        "            try:\n",
        "                return parsedate_to_datetime(val)\n",
        "            except Exception:\n",
        "                pass\n",
        "    return None\n",
        "\n",
        "def in_date_range(d, lower_iso):\n",
        "    if not lower_iso:\n",
        "        return True\n",
        "    try:\n",
        "        floor = dt.datetime.fromisoformat(lower_iso).replace(tzinfo=dt.timezone.utc)\n",
        "    except Exception:\n",
        "        return True\n",
        "    if d is None:\n",
        "        return True\n",
        "    if d.tzinfo is None:\n",
        "        d = d.replace(tzinfo=dt.timezone.utc)\n",
        "    return d >= floor\n",
        "\n",
        "def normalize_text(txt: str) -> str:\n",
        "    return re.sub(r\"\\s+\", \" \", txt.strip().lower())\n",
        "\n",
        "def sha256_text(txt: str) -> str:\n",
        "    import hashlib\n",
        "    return hashlib.sha256(txt.encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "def slugify(text: str, maxlen: int = 60) -> str:\n",
        "    s = re.sub(r\"[^a-zA-Z0-9]+\", \"-\", text).strip(\"-\").lower()\n",
        "    return s[:maxlen] or \"untitled\"\n",
        "\n",
        "def get_title_from_html(html: str, fallback: str = \"Untitled\"):\n",
        "    m = re.search(r\"<title>(.*?)</title>\", html or \"\", flags=re.IGNORECASE | re.DOTALL)\n",
        "    if m:\n",
        "        return re.sub(r\"\\s+\", \" \", m.group(1)).strip()\n",
        "    return fallback\n",
        "\n",
        "def fetch_and_extract(url: str):\n",
        "    downloaded = trafilatura.fetch_url(url, no_ssl=False)\n",
        "    if not downloaded:\n",
        "        raise RuntimeError(\"failed to fetch\")\n",
        "    text = trafilatura.extract(downloaded, include_comments=False, include_tables=False) or \"\"\n",
        "    title = get_title_from_html(downloaded, \"Untitled\")\n",
        "    if not text.strip():\n",
        "        raise RuntimeError(\"no main text extracted\")\n",
        "    return {\"title\": title, \"text\": text}\n",
        "\n",
        "def save_article(url: str, title: str, text: str, source_name: str):\n",
        "    domain = urlparse(url).netloc\n",
        "    slug = slugify(title)\n",
        "    h = sha256_text(normalize_text(text))\n",
        "    art_id = f\"{domain}-{slug}-{h[:12]}\"\n",
        "    txt_path = RAW_DIR / f\"{art_id}.txt\"\n",
        "    meta_path = RAW_DIR / f\"{art_id}.meta.json\"\n",
        "    txt_path.write_text(text, encoding=\"utf-8\")\n",
        "    meta = {\n",
        "        \"id\": art_id,\n",
        "        \"url\": url,\n",
        "        \"title\": title,\n",
        "        \"source\": source_name,\n",
        "        \"section\": None,\n",
        "        \"domain\": domain,\n",
        "        \"published\": None,\n",
        "        \"sha256\": h,\n",
        "        \"chars\": len(text),\n",
        "        \"saved_at\": now_utc()\n",
        "    }\n",
        "    meta_path.write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
        "    return {\"id\": art_id, \"txt_path\": txt_path, \"meta_path\": meta_path}\n",
        "\n",
        "def already_cached(u: str) -> bool:\n",
        "    return (u in index[\"items\"]) and (index[\"items\"][u].get(\"status\") == \"ok\")\n",
        "\n",
        "def set_seen(u: str, status: str):\n",
        "    index[\"items\"][u] = {\"status\": status, \"fetched_at\": now_utc()}\n",
        "    save_index_local(index)\n",
        "\n",
        "# Compute quotas\n",
        "feed_names = [name for name, _ in GUARDIAN_FEEDS]\n",
        "num_feeds = len(feed_names)\n",
        "if num_feeds == 0:\n",
        "    raise RuntimeError(\"No Guardian feeds configured. Edit Setup 5A first.\")\n",
        "\n",
        "if EVEN_SPLIT:\n",
        "    base = MAX_ARTICLES // num_feeds\n",
        "    rem = MAX_ARTICLES % num_feeds\n",
        "    quotas = {name: base for name in feed_names}\n",
        "    if QUOTA_REMAINDER_TO in quotas:\n",
        "        quotas[QUOTA_REMAINDER_TO] += rem\n",
        "    else:\n",
        "        quotas[feed_names[0]] += rem\n",
        "else:\n",
        "    # If not even split, give everything to QUOTA_REMAINDER_TO\n",
        "    quotas = {name: 0 for name in feed_names}\n",
        "    quotas[QUOTA_REMAINDER_TO if QUOTA_REMAINDER_TO in feed_names else feed_names[0]] = MAX_ARTICLES\n",
        "\n",
        "# Apply per feed hard cap if set\n",
        "if isinstance(MAX_PER_FEED, int) and MAX_PER_FEED > 0:\n",
        "    for k in quotas:\n",
        "        quotas[k] = min(quotas[k], MAX_PER_FEED)\n",
        "\n",
        "print(\"Per feed quotas for this run:\")\n",
        "print(json.dumps(quotas, indent=2))\n",
        "\n",
        "saved_global = 0\n",
        "errors_global = 0\n",
        "globally_seen = set()\n",
        "\n",
        "# Ensure feeds_state has entries for these feeds\n",
        "fs = feeds_state.setdefault(\"feeds\", {})\n",
        "for name, feed_url in GUARDIAN_FEEDS:\n",
        "    if name not in fs:\n",
        "        fs[name] = {\n",
        "            \"feed_url\": feed_url,\n",
        "            \"last_cursor_iso\": None,\n",
        "            \"recent_url_hashes\": [],\n",
        "            \"recent_url_hashes_max\": 500 if name != \"commentisfree\" else 1000,\n",
        "            \"last_run_at\": None,\n",
        "            \"last_run_by\": \"colab\",\n",
        "            \"notes\": f\"Guardian {name}\"\n",
        "        }\n",
        "\n",
        "for name, feed_url in GUARDIAN_FEEDS:\n",
        "    if saved_global >= MAX_ARTICLES:\n",
        "        break\n",
        "    quota = quotas.get(name, 0)\n",
        "    if quota <= 0:\n",
        "        continue\n",
        "\n",
        "    saved_this_feed = 0\n",
        "    fp = feedparser.parse(feed_url)\n",
        "\n",
        "    # Collect and filter entries\n",
        "    items = []\n",
        "    for e in fp.entries:\n",
        "        url = getattr(e, \"link\", None)\n",
        "        if not url:\n",
        "            continue\n",
        "        pub = parse_entry_date(e)\n",
        "        if not in_date_range(pub, DATE_FROM):\n",
        "            continue\n",
        "        items.append({\"url\": url, \"published\": pub})\n",
        "\n",
        "    # Sort newest first and de-dupe\n",
        "    seen_urls = set()\n",
        "    uniq = []\n",
        "    for it in sorted(items, key=lambda x: (x[\"published\"] or dt.datetime.min), reverse=True):\n",
        "        if it[\"url\"] in seen_urls:\n",
        "            continue\n",
        "        seen_urls.add(it[\"url\"])\n",
        "        uniq.append(it)\n",
        "\n",
        "    for item in uniq:\n",
        "        if saved_global >= MAX_ARTICLES or saved_this_feed >= quota:\n",
        "            break\n",
        "\n",
        "        url = item[\"url\"]\n",
        "        if url in globally_seen:\n",
        "            continue\n",
        "        globally_seen.add(url)\n",
        "\n",
        "        if already_cached(url) and not FORCE_REFETCH:\n",
        "            print(f\"skip (cached) [{name}]: {url}\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            res = fetch_and_extract(url)\n",
        "            out = save_article(url, res[\"title\"], res[\"text\"], source_name=\"theguardian\")\n",
        "            set_seen(url, \"ok\")\n",
        "            saved_this_feed += 1\n",
        "            saved_global += 1\n",
        "            # Update ring buffer and timestamps\n",
        "            h = sha256_text(url)[:12]\n",
        "            ring = fs[name][\"recent_url_hashes\"]\n",
        "            ring.append(h)\n",
        "            maxlen = fs[name][\"recent_url_hashes_max\"]\n",
        "            if len(ring) > maxlen:\n",
        "                fs[name][\"recent_url_hashes\"] = ring[-maxlen:]\n",
        "            fs[name][\"last_run_at\"] = now_utc()\n",
        "            print(f\"saved [{name}]: {out['txt_path'].name} | {res['title'][:90]}\")\n",
        "        except Exception as exc:\n",
        "            print(f\"error [{name}]: {url} | {type(exc).__name__}: {str(exc)[:140]}\")\n",
        "            set_seen(url, \"error\")\n",
        "            errors_global += 1\n",
        "\n",
        "# Write back feeds_state.json locally so you can commit later\n",
        "feeds_state[\"updated_at\"] = now_utc()\n",
        "STATE_PATH.write_text(json.dumps(feeds_state, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "print(\"\\nSummary\")\n",
        "print(json.dumps({\n",
        "    \"saved_total\": saved_global,\n",
        "    \"errors_total\": errors_global,\n",
        "    \"max_articles\": MAX_ARTICLES,\n",
        "    \"max_per_feed\": MAX_PER_FEED,\n",
        "    \"date_from\": DATE_FROM,\n",
        "    \"even_split\": EVEN_SPLIT,\n",
        "    \"remainder_to\": QUOTA_REMAINDER_TO,\n",
        "    \"feeds_count\": len(GUARDIAN_FEEDS),\n",
        "    \"raw_dir\": str(RAW_DIR),\n",
        "    \"index_path\": str(INDEX_PATH),\n",
        "    \"feeds_state_local\": str(STATE_PATH)\n",
        "}, indent=2))\n"
      ],
      "metadata": {
        "id": "uHrefNpJZ4L7",
        "outputId": "a89c6c1c-e21b-482c-9311-8bd56de611ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "uHrefNpJZ4L7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Per feed quotas for this run:\n",
            "{\n",
            "  \"world\": 5,\n",
            "  \"uk-news\": 5,\n",
            "  \"us-news\": 5,\n",
            "  \"politics\": 5,\n",
            "  \"europe\": 5,\n",
            "  \"americas\": 5,\n",
            "  \"asia\": 5,\n",
            "  \"australia-news\": 5,\n",
            "  \"business\": 5,\n",
            "  \"money\": 5,\n",
            "  \"technology\": 5,\n",
            "  \"science\": 5,\n",
            "  \"global-development\": 5,\n",
            "  \"environment\": 5,\n",
            "  \"wildlife\": 5,\n",
            "  \"pollution\": 5,\n",
            "  \"climate-crisis\": 5,\n",
            "  \"sport\": 5,\n",
            "  \"football\": 5,\n",
            "  \"cricket\": 5,\n",
            "  \"tennis\": 5,\n",
            "  \"golf\": 5,\n",
            "  \"formulaone\": 5,\n",
            "  \"cycling\": 5,\n",
            "  \"rugby-union\": 5,\n",
            "  \"culture\": 5,\n",
            "  \"film\": 5,\n",
            "  \"music\": 5,\n",
            "  \"artanddesign\": 5,\n",
            "  \"books\": 5,\n",
            "  \"tv-and-radio\": 5,\n",
            "  \"lifestyle\": 5,\n",
            "  \"family\": 5,\n",
            "  \"health\": 5,\n",
            "  \"inequality\": 5,\n",
            "  \"obituaries\": 5,\n",
            "  \"travel\": 5,\n",
            "  \"fashion\": 5,\n",
            "  \"games\": 5,\n",
            "  \"stage\": 5,\n",
            "  \"crosswords\": 5,\n",
            "  \"commentisfree\": 45\n",
            "}\n",
            "saved [world]: www.theguardian.com-egypt-confirms-international-leaders-summit-on-monday-to-dis-0efa690f2f6e.txt | Egypt confirms international leaders’ summit on Monday to discuss Gaza ceasefire - live | \n",
            "saved [world]: www.theguardian.com-trump-says-military-members-will-be-paid-despite-government--69a05b3f6a9f.txt | Trump says military members will be paid despite government shutdown | US federal governme\n",
            "saved [world]: www.theguardian.com-chimamanda-ngozi-adichie-says-she-is-terrified-her-sons-will-7fa91e986bc1.txt | Chimamanda Ngozi Adichie says she is terrified her sons will ‘join manosphere’ | Chimamand\n",
            "saved [world]: www.theguardian.com-six-people-dead-with-multiple-injured-in-three-shootings-at--5f3eed5f0c5a.txt | Six people dead with multiple injured in three shootings at Mississippi homecoming weekend\n",
            "saved [world]: www.theguardian.com-no-survivors-in-tennessee-explosives-factory-blast-officials-c35ecb4e3c8c.txt | No survivors in Tennessee explosives factory blast, officials say | Tennessee | The Guardi\n",
            "saved [uk-news]: www.theguardian.com-two-men-arrested-after-lostprophets-singer-ian-watkins-dies--d313ec049af9.txt | Two men arrested after Lostprophets singer Ian Watkins dies in prison attack | UK news | T\n",
            "saved [uk-news]: www.theguardian.com-snp-backs-swinney-s-clear-strategy-for-new-independence-refe-1e83bf492422.txt | SNP backs Swinney’s ‘clear’ strategy for new independence referendum | Scottish National p\n",
            "saved [uk-news]: www.theguardian.com-thousands-gather-in-london-in-march-for-lasting-peace-in-gaz-62b3c83abc3e.txt | Thousands gather in London in march for ‘lasting peace’ in Gaza | Israel-Gaza war | The Gu\n",
            "saved [uk-news]: www.theguardian.com-tony-blair-met-jeffrey-epstein-in-no-10-on-advice-of-peter-m-885a8c846dd5.txt | Tony Blair met Jeffrey Epstein in No 10 on advice of Peter Mandelson, documents reveal | T\n",
            "saved [uk-news]: www.theguardian.com-revealed-labour-run-council-using-legal-loophole-to-serve-fa-c900b7ea898d.txt | Revealed: Labour-run council using legal loophole to serve families with no-fault eviction\n",
            "saved [us-news]: www.theguardian.com-national-guard-begins-memphis-patrols-as-senators-in-illinoi-5a397dac782a.txt | National guard begins Memphis patrols as senators in Illinois are turned away from Ice fac\n",
            "saved [us-news]: www.theguardian.com-the-peerless-a-ja-wilson-may-already-be-the-wnba-s-greatest--62527310cfc0.txt | The peerless A’ja Wilson may already be the WNBA’s greatest ever player | Las Vegas Aces |\n",
            "saved [us-news]: www.theguardian.com-before-trump-dreamers-were-shielded-from-deportation-here-s--6a0f39d2dbc5.txt | Before Trump, ‘Dreamers’ were shielded from deportation. Here’s what’s changed | US immigr\n",
            "saved [us-news]: www.theguardian.com-a-new-frontier-why-aussies-are-coming-to-mls-for-soccer-oppo-9a1c25d98332.txt | A new frontier: why Aussies are coming to MLS for soccer opportunity | Australia | The Gua\n",
            "saved [us-news]: www.theguardian.com-joe-biden-receiving-radiation-therapy-for-prostate-cancer-jo-03f66a833b14.txt | Joe Biden receiving radiation therapy for prostate cancer | Joe Biden | The Guardian\n",
            "saved [politics]: www.theguardian.com-madeline-horwath-on-st-george-returning-to-england-cartoon-m-83632ba81f7d.txt | Madeline Horwath on St George returning to England – cartoon | Madeline Horwath | The Guar\n",
            "saved [politics]: www.theguardian.com-don-t-fall-for-the-authoritarian-hype-reform-and-the-hard-ri-d6f6a528f5f1.txt | Don’t fall for the authoritarian hype – Reform and the hard right can be stopped in their \n",
            "saved [politics]: www.theguardian.com-tony-blair-and-nick-clegg-hosted-dinner-giving-tech-bosses-a-b6165dce5782.txt | Tony Blair and Nick Clegg hosted dinner giving tech bosses access to UK minister | Tony Bl\n",
            "saved [politics]: www.theguardian.com-rachel-reeves-v-the-obr-chancellor-aims-to-loosen-the-watchd-5730d48a7d0e.txt | Rachel Reeves v the OBR: chancellor aims to loosen the watchdog’s grip | Economics | The G\n",
            "saved [politics]: www.theguardian.com-rachel-reeves-looks-for-extra-headroom-in-budget-to-insulate-4fabf4fe5c92.txt | Rachel Reeves looks for extra headroom in budget to insulate UK economy against bond marke\n",
            "saved [americas]: www.theguardian.com-who-is-mar-a-corina-machado-venezuela-s-iron-lady-and-new-no-9b835d3874e0.txt | Who is María Corina Machado,'Venezuela’s Iron Lady' and new Nobel peace prize winner? – vi\n",
            "saved [americas]: www.theguardian.com-venezuelan-politician-mar-a-corina-machado-wins-nobel-peace--485310474908.txt | Venezuelan politician María Corina Machado wins Nobel peace prize | Nobel peace prize | Th\n",
            "saved [americas]: www.theguardian.com-peru-lawmakers-vote-to-oust-president-dina-boluarte-over-cri-c39ff21a0824.txt | Peru lawmakers vote to oust president Dina Boluarte over crime crisis | Peru | The Guardia\n",
            "saved [americas]: www.theguardian.com-john-candy-i-like-me-review-starry-but-treacly-tribute-to-co-bf60a515b8c8.txt | John Candy: I Like Me review – starry but treacly tribute to comedy legend | Movies | The \n",
            "saved [americas]: www.theguardian.com-president-petro-accuses-us-of-killing-colombians-in-attacks--56467e8e9ad3.txt | President Petro accuses US of killing Colombians in attacks on ‘narco-boats’ | Colombia | \n",
            "saved [asia]: www.theguardian.com-china-issues-rewards-for-information-about-taiwan-military-s-35444eeb3e8c.txt | China issues rewards for information about Taiwan military’s ‘psychological warfare unit’ \n",
            "saved [asia]: www.theguardian.com-to-the-men-who-ran-the-world-i-was-just-a-photo-op-malala-yo-951355fae79d.txt | ‘To the men who ran the world, I was just a photo op’: Malala Yousafzai on growing up, get\n",
            "saved [asia]: www.theguardian.com-unease-at-slow-pace-of-change-in-nepal-one-month-on-from-gen-8e90cfc36802.txt | Unease at slow pace of change in Nepal one month on from gen Z protests | Nepal | The Guar\n",
            "saved [asia]: www.theguardian.com-seven-people-killed-after-twin-earthquakes-off-coast-of-phil-4c06d8035124.txt | Seven people killed after twin earthquakes off coast of Philippines | Earthquakes | The Gu\n",
            "saved [asia]: www.theguardian.com-weather-tracker-south-east-china-swelters-in-summer-like-hea-68c59aaa0bc9.txt | Weather tracker: South-east China swelters in summer-like heat | China | The Guardian\n",
            "saved [australia-news]: www.theguardian.com-australia-news-live-pro-palestine-rallies-to-continue-amid-o-a667330657c5.txt | Australia news live: pro-Palestine rallies to continue amid Opera House ban and ceasefire \n",
            "saved [australia-news]: www.theguardian.com-as-the-liberal-party-s-key-demographic-shuffles-off-how-can--eab7de08c3b4.txt | As the Liberal party’s key demographic shuffles off, how can Sussan Ley appeal to Australi\n",
            "saved [australia-news]: www.theguardian.com-adopting-a-she-ll-be-right-attitude-to-australian-politics-m-9f62d9265fe6.txt | Adopting a ‘she’ll be right’ attitude to Australian politics may be seductive, but it cert\n",
            "saved [australia-news]: www.theguardian.com-fossicking-for-a-fortune-the-price-of-gold-is-sky-high-and-p-c7f56823c4dd.txt | Fossicking for a fortune: the price of gold is sky-high, and prospectors in Victoria hope \n",
            "saved [australia-news]: www.theguardian.com-the-moment-i-knew-he-was-so-open-and-vulnerable-even-as-a-bl-d8194367a165.txt | The moment I knew: he was so open and vulnerable, even as a bloke’s bloke | Life and style\n",
            "saved [business]: www.theguardian.com-trump-is-obsessed-with-seeming-pro-worker-but-his-actions-su-f3b2f79a0a67.txt | Trump is ‘obsessed’ with seeming pro-worker – but his actions suggest otherwise | Donald T\n",
            "saved [business]: www.theguardian.com-widow-of-man-conned-out-of-pension-savings-to-lose-half-the--e55645a012a1.txt | Widow of man conned out of pension savings to lose half the compensation to tax | HMRC | T\n",
            "saved [business]: www.theguardian.com-do-we-need-imax-70mm-vistavision-all-i-need-to-watch-movies--3a4727c736b9.txt | Do we need Imax? 70mm? VistaVision? All I need to watch movies at the cinema is darkness a\n",
            "saved [business]: www.theguardian.com-london-has-turned-into-something-crazy-is-the-city-in-the-gr-50008635b0f5.txt | ‘London has turned into something crazy’: is the city in the grip of a crime wave? | Londo\n",
            "saved [business]: www.theguardian.com-fifa-seeks-advice-over-banning-league-games-staged-overseas--59111dfd8b60.txt | Fifa seeks advice over banning league games staged overseas amid regulations redraft | Fif\n",
            "saved [money]: www.theguardian.com-hits-the-nose-like-wasabi-the-best-and-worst-supermarket-eng-7978994d7512.txt | ‘Hits the nose like wasabi’: the best (and worst) supermarket English mustard, tasted and \n",
            "saved [money]: www.theguardian.com-what-price-loyalty-uk-supermarket-cards-rated-shopping-the-g-bbda9ae90172.txt | What price loyalty? UK supermarket cards rated | Shopping | The Guardian\n",
            "saved [money]: www.theguardian.com-tiktok-influencers-fuelling-parallel-market-for-unlicensed-w-2c0acede5735.txt | TikTok influencers fuelling parallel market for unlicensed weight-loss drug | Consumer aff\n",
            "saved [money]: www.theguardian.com-britons-travelling-to-eu-to-undergo-new-biometric-checks-at--6287d2ebac6b.txt | Britons travelling to EU to undergo new biometric checks at border from Sunday | European \n",
            "saved [money]: www.theguardian.com-50-men-s-autumn-wardrobe-updates-for-under-150-some-are-even-159dfe69dc3e.txt | 50 men’s autumn wardrobe updates for under £150 (some are even free) | Men's fashion | The\n",
            "saved [technology]: www.theguardian.com-using-a-swearword-in-your-google-search-can-stop-the-ai-answ-fc679e0534fb.txt | Using a swearword in your Google search can stop the AI answer. But should you? | Artifici\n",
            "saved [technology]: www.theguardian.com-inside-tech-billionaire-peter-thiel-s-off-the-record-lecture-8c1fa5790b8a.txt | Inside tech billionaire Peter Thiel’s off-the-record lectures about the antichrist | US ne\n",
            "saved [technology]: www.theguardian.com-it-s-sam-altman-the-man-who-stole-the-rights-from-copyright--341c19061de3.txt | It’s Sam Altman: the man who stole the rights from copyright. If he’s the future, can we g\n",
            "saved [technology]: www.theguardian.com-little-lungs-are-paying-1-6m-claimants-head-to-high-court-as-d922a3bcb371.txt | ‘Little lungs are paying’: 1.6m claimants head to high court as carmakers finally face pun\n",
            "saved [technology]: www.theguardian.com-meet-anamanaguchi-the-band-behind-the-last-scott-pilgrim-vid-05f8c2717b6c.txt | Meet Anamanaguchi, the band behind the last Scott Pilgrim video game’s soundtrack – and th\n",
            "saved [science]: www.theguardian.com-drummond-rennie-obituary-science-the-guardian-20bd0c8534ce.txt | Drummond Rennie obituary | Science | The Guardian\n",
            "saved [science]: www.theguardian.com-grisly-recording-reveals-bat-catching-killing-and-eating-rob-05d386541784.txt | Grisly recording reveals bat catching, killing and eating robin mid-flight | Animal behavi\n",
            "saved [science]: www.theguardian.com-dogs-name-toys-while-elephants-name-each-other-animal-langua-ba0531b5feac.txt | Dogs name toys while elephants name each other. Animal language is more complex than we im\n",
            "saved [science]: www.theguardian.com-all-the-news-and-science-from-the-2025-nobel-prizes-podcast--6b3ed95dad81.txt | All the news and science from the 2025 Nobel prizes – podcast | Science | The Guardian\n",
            "saved [science]: www.theguardian.com-nobel-prize-in-chemistry-awarded-to-scientists-for-work-on-h-1570a11f7cfe.txt | Nobel prize in chemistry awarded to scientists for work on ‘Hermione’s handbag’ | Nobel pr\n",
            "saved [global-development]: www.theguardian.com-torture-blackmail-extortion-the-dangers-of-queer-online-dati-30a32a8839bc.txt | Torture, blackmail, extortion: the dangers of queer online dating in Ghana | Global develo\n",
            "saved [global-development]: www.theguardian.com-just-money-with-no-strings-attached-how-direct-cash-transfer-45456966d70a.txt | ‘Just money, with no strings attached’: how direct cash transfers are giving women in rura\n",
            "saved [global-development]: www.theguardian.com-us-undermining-global-health-by-threatening-to-strip-funding-6e391d47249c.txt | US ‘undermining global health’ by threatening to strip funding from aid projects that do n\n",
            "saved [global-development]: www.theguardian.com-i-have-searched-and-searched-for-help-the-sudanese-women-lef-b3aa6375c351.txt | ‘I have searched and searched for help’: the Sudanese women left alone to live hand to mou\n",
            "saved [global-development]: www.theguardian.com-fast-fashion-recycling-how-the-castoff-capital-of-the-world--3fbd2486cecb.txt | Fast-fashion recycling: how ‘the castoff capital of the world’ is making Indian factory wo\n",
            "saved [environment]: www.theguardian.com-number-of-wild-bee-species-at-risk-of-extinction-in-europe-d-7aa6acb043f9.txt | Number of wild bee species at risk of extinction in Europe doubles in 10 years | Bees | Th\n",
            "saved [environment]: www.theguardian.com-country-diary-panic-stress-glory-my-day-scaling-the-inaccess-9352c67e1c1a.txt | Country diary: Panic, stress, glory – my day scaling the Inaccessible Pinnacle | Mountaine\n",
            "saved [environment]: www.theguardian.com-baby-numbats-spotted-at-two-wildlife-sanctuaries-in-hopeful--93f9fb1e5ed6.txt | Baby numbats spotted at two wildlife sanctuaries in hopeful sign for one of Australia’s ra\n",
            "saved [environment]: www.theguardian.com-orange-bellied-parrots-have-swelled-back-from-imminent-extin-6254634456a8.txt | Orange-bellied parrots have swelled back from imminent extinction – but now they face a ne\n",
            "saved [environment]: www.theguardian.com-baby-giant-tortoises-thrive-in-seychelles-after-first-succes-b9b63448e6fb.txt | Baby giant tortoises thrive in Seychelles after first successful artificial incubation | C\n",
            "saved [wildlife]: www.theguardian.com-here-s-to-the-birdwatchers-optimistic-slightly-eccentric-cus-171d793f1527.txt | Here’s to the birdwatchers! Optimistic, slightly eccentric custodians of wonder and joy an\n",
            "saved [wildlife]: www.theguardian.com-and-then-there-were-none-australia-s-only-shrew-declared-ext-f537fdf4933e.txt | And then there were none: Australia’s only shrew declared extinct | John Woinarski for the\n",
            "saved [wildlife]: www.theguardian.com-crocodile-made-famous-by-steve-irwin-wrongfully-arrested-and-a28168757125.txt | Crocodile made famous by Steve Irwin ‘wrongfully arrested’ and should be returned to wild,\n",
            "saved [wildlife]: www.theguardian.com-more-than-half-of-world-s-bird-species-in-decline-as-leaders-e01518d9c81f.txt | More than half of world’s bird species in decline, as leaders meet on extinction crisis | \n",
            "saved [wildlife]: www.theguardian.com-week-in-wildlife-a-bumpy-snailfish-a-slow-loris-and-a-whistl-d28cda0f8e72.txt | Week in wildlife: a bumpy snailfish, a slow loris and a whistle pig | Environment | The Gu\n",
            "saved [pollution]: www.theguardian.com-millions-in-england-face-higher-water-bills-after-regulator--14d176706e91.txt | Millions in England face higher water bills after regulator backs more price rises | Water\n",
            "saved [pollution]: www.theguardian.com-thousands-take-legal-action-over-widespread-pollution-of-thr-2f56c9401116.txt | Thousands take legal action over ‘widespread pollution’ of three UK rivers | Rivers | The \n",
            "saved [pollution]: www.theguardian.com-uk-plastic-waste-exports-to-developing-countries-rose-84-in--6b64f8be1495.txt | UK plastic waste exports to developing countries rose 84% in a year, data shows | Plastics\n",
            "saved [pollution]: www.theguardian.com-thames-water-removes-100-tonne-fatberg-from-west-london-sewe-6e4d199cb1c0.txt | Thames Water removes 100-tonne fatberg from west London sewer | Environment | The Guardian\n",
            "saved [pollution]: www.theguardian.com-carbon-offsets-fail-to-cut-global-heating-due-to-intractable-0fef0f7b4b5a.txt | Carbon offsets fail to cut global heating due to ‘intractable’ systemic problems, study sa\n",
            "saved [climate-crisis]: www.theguardian.com-with-most-mps-ignorant-of-the-urgency-how-can-the-uk-ever-re-eff8058f92b2.txt | With most MPs ignorant of the urgency, how can the UK ever reach net zero? | Climate crisi\n",
            "saved [climate-crisis]: www.theguardian.com-prince-william-to-attend-cop30-un-climate-summit-in-brazil-c-9d8e93f8aebd.txt | Prince William to attend Cop30 UN climate summit in Brazil | Cop30 | The Guardian\n",
            "saved [climate-crisis]: www.theguardian.com-americans-are-dying-from-extreme-heat-autopsy-reports-don-t--6c6982019ab1.txt | Americans are dying from extreme heat. Autopsy reports don’t show the full story | Extreme\n",
            "saved [climate-crisis]: www.theguardian.com-more-than-40-trump-administration-picks-tied-directly-to-oil-3032eea563fb.txt | More than 40 Trump administration picks tied directly to oil, gas and coal, analysis shows\n",
            "saved [climate-crisis]: www.theguardian.com-has-finding-an-affordable-home-in-australia-just-gotten-hard-455bbd71753e.txt | Has finding an affordable home in Australia just gotten harder? | Fiona Katauskas | The Gu\n",
            "saved [sport]: www.theguardian.com-world-cup-qualifying-roundup-neves-stuns-republic-of-ireland-499bfc0c420c.txt | World Cup qualifying roundup: Neves stuns Republic of Ireland with late goal | World Cup 2\n",
            "saved [sport]: www.theguardian.com-rangers-could-turn-to-r-hl-after-steven-gerrard-rules-out-re-42237c8bcc59.txt | Rangers could turn to Röhl after Steven Gerrard rules out return as manager | Steven Gerra\n",
            "saved [sport]: www.theguardian.com-hull-kr-secure-treble-with-dominant-display-against-wigan-in-c3f6a953f14d.txt | Hull KR secure treble with dominant display against Wigan in Super League Grand Final | Su\n",
            "saved [sport]: www.theguardian.com-aston-villa-must-stop-crying-foul-and-focus-on-the-europa-le-e3f53a617444.txt | Aston Villa must stop crying foul and focus on the Europa League instead | Aston Villa | T\n",
            "saved [sport]: www.theguardian.com-former-arsenal-midfielder-jack-wilshere-in-the-frame-for-lut-a4c919b67e0a.txt | Former Arsenal midfielder Jack Wilshere in the frame for Luton job | Soccer | The Guardian\n",
            "saved [football]: www.theguardian.com-erling-haaland-hat-trick-helps-norway-sink-israel-against-ba-ac0afb3fe6b8.txt | Erling Haaland hat-trick helps Norway sink Israel against backdrop of protests | World Cup\n",
            "saved [football]: www.theguardian.com-ross-wilson-ends-newcastle-sporting-director-saga-in-switch--75f933684acd.txt | Ross Wilson ends Newcastle sporting director saga in switch from Forest | Newcastle United\n",
            "saved [football]: www.theguardian.com-denver-summit-s-nick-cushing-on-building-an-nwsl-club-from-s-4fbcad300c5d.txt | Denver Summit’s Nick Cushing on building an NWSL club from scratch | Women's football | Th\n",
            "saved [football]: www.theguardian.com-tottenham-s-burst-of-positivity-under-martin-ho-is-boost-for-1e5ff5cf99e0.txt | Tottenham’s burst of positivity under Martin Ho is boost for WSL as a whole | Tottenham Ho\n",
            "saved [football]: www.theguardian.com-clive-tyldesley-i-ve-only-been-drunk-twice-and-once-was-with-363556646e53.txt | Clive Tyldesley: ‘I’ve only been drunk twice and once was with the England women’s team’ |\n",
            "saved [cricket]: www.theguardian.com-england-ease-to-89-run-win-over-sri-lanka-women-s-cricket-wo-e8122bd9597c.txt | England ease to 89-run win over Sri Lanka: Women’s Cricket World Cup – as it happened | Wo\n",
            "saved [cricket]: www.theguardian.com-nat-sciver-brunt-shines-as-england-recover-to-sweep-aside-sr-1f41f385c083.txt | Nat Sciver-Brunt shines as England recover to sweep aside Sri Lanka | Women's Cricket Worl\n",
            "saved [cricket]: www.theguardian.com-winter-is-coming-england-s-cricketers-fly-out-for-long-tour--a16abdc3b98c.txt | Winter is coming: England’s cricketers fly out for long tour that will decide Ashes and Wo\n",
            "saved [cricket]: www.theguardian.com-harry-brook-says-pat-cummins-absence-could-boost-england-s-a-c2a44967739f.txt | Harry Brook says Pat Cummins’ absence could boost England’s Ashes hopes | Ashes 2025 - 26 \n",
            "saved [cricket]: www.theguardian.com-sports-quiz-of-the-week-cole-palmer-women-s-cricket-world-cu-30b2db1a8ff7.txt | Sports quiz of the week: Cole Palmer, Women’s Cricket World Cup and Cape Verde | Sport | T\n",
            "saved [tennis]: www.theguardian.com-world-no-204-vacherot-topples-djokovic-to-set-up-cousin-v-co-89b5dd9e242c.txt | World No 204 Vacherot topples Djokovic to set up cousin v cousin Shanghai final | Tennis |\n",
            "saved [tennis]: www.theguardian.com-all-fluffed-up-why-modern-balls-are-sparking-injury-worries--9526e45cd6e7.txt | All fluffed up: why modern balls are sparking injury worries and frustration in tennis | T\n",
            "saved [tennis]: www.theguardian.com-we-rewatched-an-ajax-match-what-really-happens-when-tennis-a-ffb782b64ce4.txt | ‘We rewatched an Ajax match’: what really happens when tennis anti-dopers call | Tennis | \n",
            "saved [tennis]: www.theguardian.com-novak-djokovic-battles-into-shanghai-semis-as-vacherot-s-dre-837c3606f39f.txt | Novak Djokovic battles into Shanghai semis as Vacherot’s dream run continues | Tennis | Th\n",
            "saved [tennis]: www.theguardian.com-amazing-number-alex-de-minaur-hits-new-milestone-on-path-to--4fc08e0f235e.txt | ‘Amazing number’: Alex de Minaur hits new milestone on path to Shanghai quarters | Alex de\n",
            "saved [golf]: www.theguardian.com-collin-morikawa-denies-his-chaos-comments-inflamed-usa-fans--1d99c7bad2f8.txt | Collin Morikawa denies his ‘chaos’ comments inflamed USA fans at Ryder Cup | Ryder Cup 202\n",
            "saved [golf]: www.theguardian.com-swearing-booing-and-spitting-is-crowd-behaviour-out-of-contr-3a131717bcf5.txt | Swearing, booing and spitting: is crowd behaviour out of control? | Psychology | The Guard\n",
            "saved [golf]: www.theguardian.com-sports-quiz-of-the-week-ryder-cup-world-cup-winners-and-harr-ec4687193ccd.txt | Sports quiz of the week: Ryder Cup, World Cup winners and Harry Kane | Sport | The Guardia\n",
            "saved [golf]: www.theguardian.com-pga-of-america-president-belatedly-admits-us-ryder-cup-fans--30795edd2a44.txt | PGA of America president belatedly admits US Ryder Cup fans ‘crossed line’ with abuse | Ry\n",
            "saved [golf]: www.theguardian.com-what-can-americans-learn-from-the-uk-how-to-cope-with-nation-01b791aab9a6.txt | What can Americans learn from the UK? How to cope with national sporting despair | Sport |\n",
            "saved [formulaone]: www.theguardian.com-carlos-sainz-hits-out-at-f1-broadcasters-coverage-of-celebri-bc3442110a54.txt | Carlos Sainz hits out at F1 broadcasters’ coverage of ‘celebrities and girlfriends’ | Form\n",
            "saved [formulaone]: www.theguardian.com-stella-admits-mclaren-face-difficulties-managing-norris-and--6f552a9e5b73.txt | Stella admits McLaren ‘face difficulties’ managing Norris and Piastri in title run-in | Fo\n",
            "saved [formulaone]: www.theguardian.com-norris-rejects-piastri-s-complaints-after-mclaren-duo-clash--bbd578148cdc.txt | Norris rejects Piastri’s complaints after McLaren duo clash in Singapore | Formula One | T\n",
            "saved [formulaone]: www.theguardian.com-singapore-grand-prix-2025-russell-wins-as-verstappen-holds-o-306c11c1cfa6.txt | Singapore Grand Prix 2025: Russell wins as Verstappen holds off Norris for second – as it \n",
            "saved [formulaone]: www.theguardian.com-george-russell-wins-f1-singapore-gp-as-norris-nibbles-at-ang-0b4449be6727.txt | George Russell wins F1 Singapore GP as Norris nibbles at angry Piastri’s lead | Formula On\n",
            "saved [cycling]: www.theguardian.com-tadej-pogacar-caps-stunning-season-with-fifth-straight-il-lo-35631833dda8.txt | Tadej Pogacar caps stunning season with fifth straight Il Lombardia triumph | Cycling | Th\n",
            "saved [cycling]: www.theguardian.com-derek-gee-faces-30m-damages-claim-after-ending-israel-premie-276cc2daa284.txt | Derek Gee faces €30m damages claim after ending Israel-Premier Tech contract over ‘persona\n",
            "saved [cycling]: www.theguardian.com-i-m-the-total-opposite-to-cav-and-brad-geraint-thomas-on-how-a5188ec44344.txt | ‘I’m the total opposite to Cav and Brad’: Geraint Thomas on how a normal bloke won the Tou\n",
            "saved [cycling]: www.theguardian.com-israel-premier-tech-to-change-its-name-and-move-away-from-cu-85b48187d3bf.txt | Israel-Premier Tech to change its name and move away from ‘current identity’ | Cycling | T\n",
            "saved [cycling]: www.theguardian.com-unstoppable-tadej-pogacar-defends-men-s-road-race-world-titl-d377069a1f17.txt | Unstoppable Tadej Pogacar defends men’s road race world title in Rwanda | Cycling Road Wor\n",
            "saved [rugby-union]: www.theguardian.com-anthony-belleau-turns-the-screw-as-northampton-see-off-rival-d6258697986a.txt | Anthony Belleau turns the screw as Northampton see off rivals Leicester | Prem Rugby | The\n",
            "saved [rugby-union]: www.theguardian.com-exe-men-review-entertaining-rugby-drama-tackles-triumph-of-u-c0e096814a33.txt | Exe Men review – entertaining rugby drama tackles triumph of underdogs Exeter Chiefs | The\n",
            "saved [rugby-union]: www.theguardian.com-roebuck-hat-trick-sets-up-nine-try-rout-as-sale-pile-more-mi-6a3cc3e9b7ea.txt | Roebuck hat-trick sets up nine-try rout as Sale pile more misery on Newcastle | Prem Rugby\n",
            "saved [rugby-union]: www.theguardian.com-exeter-s-brown-bampoe-in-the-fast-lane-for-great-things-for--076e0d6c0074.txt | Exeter’s Brown-Bampoe in the fast lane for great things for club and country | Exeter | Th\n",
            "saved [rugby-union]: www.theguardian.com-northampton-welcome-back-lions-and-not-a-day-too-soon-as-lei-bf23596510f8.txt | Northampton welcome back Lions – and not a day too soon as Leicester loom | Prem Rugby | T\n",
            "saved [culture]: www.theguardian.com-diane-keaton-oscar-winning-star-of-annie-hall-and-the-godfat-e2b36c6197de.txt | Diane Keaton, Oscar-winning star of Annie Hall and The Godfather, dies aged 79 | Diane Kea\n",
            "saved [culture]: www.theguardian.com-rock-stars-would-be-like-yeah-bring-the-kid-in-cameron-crowe-65277b7b5c46.txt | ‘Rock stars would be like, Yeah, bring the kid in’: Cameron Crowe on his wild years as a t\n",
            "saved [culture]: www.theguardian.com-verging-on-unwatchable-guardian-writers-on-their-most-stress-f00c1525d9ba.txt | ‘Verging on unwatchable’: Guardian writers on their most stressful movies | Movies | The G\n",
            "saved [culture]: www.theguardian.com-i-m-going-to-write-about-all-of-it-author-chris-kraus-on-suc-1575e227be81.txt | ‘I’m going to write about all of it’: author Chris Kraus on success, drugs and I Love Dick\n",
            "saved [culture]: www.theguardian.com-emma-doran-when-i-was-growing-up-a-woman-s-biggest-complimen-ba847e640975.txt | Emma Doran: ‘When I was growing up, a woman’s biggest compliment would be that she was imm\n",
            "saved [film]: www.theguardian.com-diane-keaton-a-life-in-pictures-film-the-guardian-f01996aca99f.txt | Diane Keaton: a life in pictures | Film | The Guardian\n",
            "saved [film]: www.theguardian.com-the-freak-script-of-charlie-chaplin-s-unfinished-final-film--1e95d733afa5.txt | The Freak: script of Charlie Chaplin’s unfinished final film to be published | Charlie Cha\n",
            "saved [film]: www.theguardian.com-moss-and-freud-review-kate-meets-lucian-and-they-get-on-bril-e960e167c3fb.txt | Moss and Freud review – Kate meets Lucian and they get on brilliantly with absolutely no f\n",
            "saved [film]: www.theguardian.com-operation-pope-review-hard-bitten-thriller-about-a-true-life-82353edd7dd1.txt | Operation Pope review – hard-bitten thriller about a true-life papal assassination plot | \n",
            "saved [film]: www.theguardian.com-rebel-wilson-using-us-court-proceedings-to-harass-and-intimi-fbac922571b2.txt | Rebel Wilson using US court proceedings to ‘harass and intimidate’ star of The Deb, court \n",
            "saved [music]: www.theguardian.com-perfume-genius-i-really-like-body-hair-i-like-a-bush-i-didn--d2b4cdfa9096.txt | Perfume Genius: ‘I really like body hair! I like a bush. I didn’t even notice Jimmy Fallon\n",
            "saved [music]: www.theguardian.com-my-led-zeppelin-road-trip-was-counted-as-a-class-credit-came-57e14fa8ecdb.txt | ‘My Led Zeppelin road trip was counted as a class credit’: Cameron Crowe on the interview \n",
            "saved [music]: www.theguardian.com-my-cultural-awakening-kate-bush-helped-me-come-out-as-a-tran-76b8f6723166.txt | My cultural awakening: ‘Kate Bush helped me come out as a trans woman’ | Kate Bush | The G\n",
            "saved [music]: www.theguardian.com-taylor-swift-scores-second-biggest-uk-charts-opening-week-ev-367a4d91129a.txt | Taylor Swift scores second-biggest UK charts opening week ever with The Life of a Showgirl\n",
            "saved [music]: www.theguardian.com-moody-blues-singer-and-bassist-john-lodge-dies-aged-82-music-60a75e671b1b.txt | Moody Blues singer and bassist John Lodge dies aged 82 | Music | The Guardian\n",
            "saved [artanddesign]: www.theguardian.com-it-s-like-a-scene-from-a-movie-christian-barroso-s-best-phon-483cd3e08ea9.txt | ‘It’s like a scene from a movie’: Christian Barroso’s best phone picture | Photography | T\n",
            "saved [artanddesign]: www.theguardian.com-triple-trouble-fairey-hirst-invader-review-the-most-revoltin-81ecef32ee72.txt | Triple Trouble: Fairey, Hirst, Invader review – the most revolting visual soup imaginable \n",
            "saved [artanddesign]: www.theguardian.com-from-tron-ares-to-riot-women-your-complete-entertainment-gui-5016192644f2.txt | From Tron: Ares to Riot Women: your complete entertainment guide to the week ahead | Cultu\n",
            "saved [artanddesign]: www.theguardian.com-the-week-around-the-world-in-20-pictures-art-and-design-the--d40445a73114.txt | The week around the world in 20 pictures | Art and design | The Guardian\n",
            "saved [artanddesign]: www.theguardian.com-fill-the-frame-use-the-light-andrew-chapman-s-favourite-phot-1c75e29641fc.txt | Fill the frame, use the light: Andrew Chapman’s favourite photographs | Art and design | T\n",
            "saved [books]: www.theguardian.com-colm-t-ib-n-why-i-set-up-a-press-to-publish-nobel-winner-l-s-ab0df0756144.txt | Colm Tóibín: Why I set up a press to publish Nobel winner László Krasznahorkai | László Kr\n",
            "saved [books]: www.theguardian.com-the-best-recent-science-fiction-fantasy-and-horror-review-ro-ac4ee1120e64.txt | The best recent science fiction, fantasy and horror – review roundup | Science fiction boo\n",
            "saved [books]: www.theguardian.com-natalie-haynes-i-ll-never-read-anything-by-a-bront-again-boo-329556dce7c2.txt | Natalie Haynes: ‘I’ll never read anything by a Brontë again’ | Books | The Guardian\n",
            "saved [books]: www.theguardian.com-raise-your-soul-by-yanis-varoufakis-review-an-intimate-histo-477346e2f7e1.txt | Raise Your Soul by Yanis Varoufakis review – an intimate history of Greece | History books\n",
            "saved [books]: www.theguardian.com-l-szl-krasznahorkai-wins-the-nobel-prize-in-literature-2025--26dd2f601b97.txt | László Krasznahorkai wins the Nobel prize in literature 2025 | Nobel prize in literature |\n",
            "saved [tv-and-radio]: www.theguardian.com-the-intruder-review-the-daftest-thriller-of-the-entire-year--8c22248becc4.txt | The Intruder review – the daftest thriller of the entire year | Television | The Guardian\n",
            "saved [tv-and-radio]: www.theguardian.com-strictly-come-dancing-week-three-as-it-happened-strictly-com-194469bf7a08.txt | Strictly Come Dancing: week three – as it happened | Strictly Come Dancing | The Guardian\n",
            "saved [tv-and-radio]: www.theguardian.com-witches-of-essex-rylan-and-prof-alice-s-look-at-one-of-histo-ddfbf621f7da.txt | Witches of Essex: Rylan and Prof Alice’s look at one of history’s most shameful periods is\n",
            "saved [tv-and-radio]: www.theguardian.com-tv-tonight-a-french-psychological-thriller-about-a-perfect-n-2637ff45ea68.txt | TV tonight: a French psychological thriller about a ‘perfect’ nanny | Television & radio |\n",
            "saved [tv-and-radio]: www.theguardian.com-tech-terror-and-tom-hollander-niamh-algar-on-her-wild-new-tv-333cec4952a9.txt | Tech, terror and Tom Hollander: Niamh Algar on her wild new TV thriller | Television | The\n",
            "saved [lifestyle]: www.theguardian.com-charlie-higson-by-my-mid-20s-i-had-a-beer-gut-it-s-now-the-m-7d5424c959ff.txt | Charlie Higson: ‘By my mid-20s, I had a beer gut. It’s now the most substantial part of me\n",
            "saved [lifestyle]: www.theguardian.com-from-getting-kids-to-eat-veg-to-curbing-screen-time-the-pare-fe89707e351c.txt | From getting kids to eat veg to curbing screen time: the parenting hacks that actually wor\n",
            "saved [lifestyle]: www.theguardian.com-baggy-skinny-or-neither-why-goldilocks-jeans-are-having-a-mo-a49ab62cc2be.txt | Baggy, skinny … or neither? Why ‘Goldilocks’ jeans are having a moment | Jeans | The Guard\n",
            "saved [lifestyle]: www.theguardian.com-who-founded-the-royal-ballet-school-in-1926-the-saturday-qui-2b5b48d112e8.txt | Who founded the Royal Ballet School in 1926? The Saturday quiz | Quiz and trivia games | T\n",
            "saved [lifestyle]: www.theguardian.com-i-step-outside-into-a-cacophony-of-nature-an-off-grid-escape-d04712af17d2.txt | ‘I step outside into a cacophony of nature’: an off-grid escape in the west of England | E\n",
            "saved [family]: www.theguardian.com-why-do-onions-have-layers-and-do-sharks-swim-in-groups-the-k-b2ec57f73a8d.txt | Why do onions have layers and do sharks swim in groups? The kids’ quiz | Family | The Guar\n",
            "saved [family]: www.theguardian.com-blind-date-after-too-many-tepid-app-based-dates-outsourcing--fe9d1abbdc25.txt | Blind date: ‘After too many tepid, app-based dates, outsourcing my love life to a paper is\n",
            "saved [family]: www.theguardian.com-tim-dowling-i-m-under-an-azure-croatian-sky-fretting-about-t-8e93b5b8f545.txt | Tim Dowling: I’m under an azure Croatian sky – fretting about the roof back home | Life an\n",
            "saved [family]: www.theguardian.com-my-brother-and-sister-are-angry-at-my-parents-i-feel-caught--339c3bdca4c6.txt | My brother and sister are angry at my parents. I feel caught in the middle. What can I do?\n",
            "saved [family]: www.theguardian.com-my-girlfriend-is-supportive-as-a-partner-but-completely-baff-3f09ea553563.txt | My girlfriend is supportive as a partner, but completely baffled by my love of sports | Re\n",
            "saved [health]: www.theguardian.com-going-to-the-gym-was-too-much-effort-until-i-moved-into-one--8fabb4b13f7d.txt | Going to the gym was too much effort, until I moved into one | Fitness | The Guardian\n",
            "saved [health]: www.theguardian.com-the-armed-robber-who-went-straight-john-mcavoy-was-born-into-2aa263850a21.txt | The armed robber who went straight: John McAvoy was born into the criminal life. Here’s ho\n",
            "saved [health]: www.theguardian.com-look-out-for-number-one-selfish-self-help-books-are-booming--b8a5927f34c2.txt | Look out for number one! Selfish self-help books are booming – but will they improve your \n",
            "saved [health]: www.theguardian.com-can-t-sleep-turn-on-tune-in-and-drop-off-sleep-the-guardian-37aeb91614da.txt | Can’t sleep? Turn on, tune in and drop off | Sleep | The Guardian\n",
            "saved [health]: www.theguardian.com-typewriter-reveals-unexpected-genius-health-wellbeing-the-gu-2eb1396cc9a9.txt | Typewriter reveals unexpected genius | Health & wellbeing | The Guardian\n",
            "saved [inequality]: www.theguardian.com-worried-about-rising-bills-and-getting-by-keir-starmer-has-t-cc7a2779456d.txt | Worried about rising bills and getting by? Keir Starmer has the answer: try chewing a flag\n",
            "saved [inequality]: www.theguardian.com-lifetime-of-earnings-not-enough-for-uk-workers-to-join-wealt-198a9be56add.txt | Lifetime of earnings not enough for UK workers to join wealthiest 10%, report says | Inequ\n",
            "saved [inequality]: www.theguardian.com-vocational-training-needs-more-than-money-further-education--ce220d983cfd.txt | Vocational training needs more than money | Further education | The Guardian\n",
            "saved [inequality]: www.theguardian.com-one-rule-for-the-rich-the-salzburg-mansion-the-porsche-heir--fba56f492adb.txt | ‘One rule for the rich’: the Salzburg mansion, the Porsche heir and the writer Stefan Zwei\n",
            "saved [inequality]: www.theguardian.com-more-than-60-000-cancer-patients-in-england-not-getting-nece-662fb1a1c78a.txt | More than 60,000 cancer patients in England ‘not getting necessary radiotherapy’ | Cancer \n",
            "saved [obituaries]: www.theguardian.com-eric-potts-obituary-business-the-guardian-dd912d057836.txt | Eric Potts obituary | Business | The Guardian\n",
            "saved [obituaries]: www.theguardian.com-maureen-crill-obituary-nursing-the-guardian-0b72b5252f1d.txt | Maureen Crill obituary | Nursing | The Guardian\n",
            "saved [obituaries]: www.theguardian.com-mark-saunders-obituary-documentary-films-the-guardian-59b4365576ac.txt | Mark Saunders obituary | Documentary films | The Guardian\n",
            "saved [obituaries]: www.theguardian.com-michael-barnes-obituary-documentary-the-guardian-d19993d49edf.txt | Michael Barnes obituary | Documentary | The Guardian\n",
            "saved [obituaries]: www.theguardian.com-felicity-opp-obituary-movies-the-guardian-94a1b2d0ba1d.txt | Felicity Oppé obituary | Movies | The Guardian\n",
            "saved [travel]: www.theguardian.com-this-is-pretty-therapeutic-a-pottery-retreat-in-spain-s-alpu-54dc60b761d2.txt | ‘This is pretty therapeutic’: a pottery retreat in Spain’s Alpujarras | Learning holidays \n",
            "saved [travel]: www.theguardian.com-20-a-night-for-one-of-the-most-peaceful-locations-in-the-wor-29e32e4247fd.txt | ‘£20 a night for one of the most peaceful locations in the world’: readers’ favourite remo\n",
            "saved [travel]: www.theguardian.com-black-brummie-and-proud-a-walking-tour-of-the-real-handswort-f154cf45bebf.txt | Black, Brummie and proud: a walking tour of the real Handsworth | Birmingham holidays | Th\n",
            "saved [travel]: www.theguardian.com-sweat-dirt-and-grape-juice-it-s-incredibly-rewarding-volunte-9a04eeebd446.txt | ‘Sweat, dirt and grape juice – it’s incredibly rewarding’: volunteer harvesting on a viney\n",
            "saved [travel]: www.theguardian.com-it-s-more-than-a-pretty-backdrop-crime-writer-ann-cleeves-on-eff98fb9bd90.txt | ‘It’s more than a pretty backdrop’: crime writer Ann Cleeves on the magic of Orkney in Sco\n",
            "saved [fashion]: www.theguardian.com-bath-mats-candles-and-underpants-would-basquiat-have-loved-o-bf8e4073ed56.txt | Bath mats, candles and underpants: would Basquiat have loved or hated all the merch? | Jea\n",
            "saved [fashion]: www.theguardian.com-to-a-tee-what-to-wear-to-make-a-basic-t-shirt-look-less-basi-ae16cd79120b.txt | To a tee: what to wear to make a basic T-shirt look less basic | Fashion | The Guardian\n",
            "saved [fashion]: www.theguardian.com-we-had-to-kill-the-wag-nine-things-we-learned-from-victoria--2ba16036e38b.txt | ‘We had to kill the Wag’: nine things we learned from Victoria Beckham’s docuseries | Fash\n",
            "saved [fashion]: www.theguardian.com-jess-cartner-morley-ladylike-fashion-is-back-but-it-s-been-g-885dce0aa51b.txt | Jess Cartner-Morley: ladylike fashion is back, but it’s been given a modern twist | Fashio\n",
            "saved [fashion]: www.theguardian.com-sali-hughes-on-beauty-forget-harsh-treatments-to-get-rid-of--92ffd1b82980.txt | Sali Hughes on beauty: forget harsh treatments – to get rid of adult acne, show your skin \n",
            "saved [games]: www.theguardian.com-hack-of-age-verification-firm-may-have-exposed-70-000-discor-7b98e4c6183e.txt | Hack of age verification firm may have exposed 70,000 Discord users’ ID photos | Social me\n",
            "saved [games]: www.theguardian.com-the-non-profit-helping-people-from-all-over-the-world-to-bec-eba4c575c0de.txt | The non-profit helping people from all over the world to become successful game developers\n",
            "saved [games]: www.theguardian.com-what-the-xbox-game-pass-price-hike-says-about-the-rising-cos-5d2cd78257cc.txt | What the Xbox Game Pass price hike says about the rising cost of playing games | Games | T\n",
            "saved [games]: www.theguardian.com-cold-war-power-play-how-the-stasi-got-into-computer-games-ga-d26a77e8ac94.txt | Cold war power play: how the Stasi got into computer games | Games | The Guardian\n",
            "saved [games]: www.theguardian.com-proof-of-age-id-leaked-in-discord-data-breach-games-the-guar-1ccc4b39d0f5.txt | Proof-of-age ID leaked in Discord data breach | Games | The Guardian\n",
            "saved [stage]: www.theguardian.com-small-hotel-review-ralph-fiennes-fever-dream-leaves-you-with-5904ac30fa86.txt | Small Hotel review – Ralph Fiennes’ fever dream leaves you with major reservations | Theat\n",
            "saved [stage]: www.theguardian.com-charley-s-aunt-review-a-fresh-and-fun-glow-up-for-victorian--6574b22a4ccf.txt | Charley’s Aunt review – a fresh and fun glow-up for Victorian farce | Theatre | The Guardi\n",
            "saved [stage]: www.theguardian.com-my-right-foot-review-wryly-humorous-look-at-life-with-a-term-c0a3b35e6e8b.txt | My Right Foot review – wryly humorous look at life with a terminal illness | Theatre | The\n",
            "saved [stage]: www.theguardian.com-ian-judge-obituary-theatre-the-guardian-6fab1c33a28f.txt | Ian Judge obituary | Theatre | The Guardian\n",
            "saved [stage]: www.theguardian.com-bad-lads-review-brutality-shame-and-fear-as-horrors-of-youth-732901994b77.txt | Bad Lads review – brutality, shame and fear as horrors of youth detention centre are laid \n",
            "saved [crosswords]: www.theguardian.com-killer-sudoku-991-life-and-style-the-guardian-9bf7b1ea4751.txt | Killer sudoku 991 | Life and style | The Guardian\n",
            "saved [crosswords]: www.theguardian.com-quick-crossword-no-17-296-crosswords-the-guardian-d3a0214b2ce8.txt | Quick crossword No 17,296 | Crosswords | The Guardian\n",
            "saved [crosswords]: www.theguardian.com-quick-cryptic-crossword-no-80-crosswords-the-guardian-fd386f0aeb04.txt | Quick cryptic crossword No 80 | Crosswords | The Guardian\n",
            "saved [crosswords]: www.theguardian.com-prize-crossword-no-29-823-crosswords-the-guardian-37e3c9f89b27.txt | Prize crossword No 29,823 | Crosswords | The Guardian\n",
            "saved [crosswords]: www.theguardian.com-crossword-editor-s-desk-clues-for-michael-stipe-and-a-life-i-01b3103c9ac6.txt | Crossword editor’s desk: clues for Michael Stipe and a life in puzzles | Crosswords | The \n",
            "saved [commentisfree]: www.theguardian.com-why-is-this-fox-news-host-speculating-about-aoc-s-sex-life-a-a1eb7529979c.txt | Why is this Fox News host speculating about AOC’s sex life? | Arwa Mahdawi | The Guardian\n",
            "saved [commentisfree]: www.theguardian.com-pity-poor-trump-whose-nobel-hopes-were-dashed-by-common-sens-83e00f4d6157.txt | Pity poor Trump, whose Nobel hopes were dashed by common sense | Dave Schilling | The Guar\n",
            "saved [commentisfree]: www.theguardian.com-my-kids-gave-me-enough-material-to-write-tv-comedy-where-wil-1c76c9346d2b.txt | My kids gave me enough material to write TV comedy. Where will the jokes come from now the\n",
            "saved [commentisfree]: www.theguardian.com-why-should-you-be-labour-s-next-deputy-leader-guardian-reade-23286610a168.txt | Why should you be Labour’s next deputy leader? Guardian readers quiz the candidates | Brid\n",
            "saved [commentisfree]: www.theguardian.com-trump-s-strong-arming-of-netanyahu-led-to-a-deal-he-must-sus-e84683244b77.txt | Trump’s strong-arming of Netanyahu led to a deal. He must sustain that pressure | Mohamad \n",
            "saved [commentisfree]: www.theguardian.com-trump-wants-a-prize-for-the-gaza-deal-but-the-real-question--08d5454f881e.txt | Trump wants a prize for the Gaza deal. But the real question is, why didn’t he do it earli\n",
            "saved [commentisfree]: www.theguardian.com-peter-thiel-s-off-the-record-antichrist-lectures-reveal-more-318dae986ff5.txt | Peter Thiel’s off-the-record antichrist lectures reveal more about him than Armageddon | A\n",
            "saved [commentisfree]: www.theguardian.com-the-fledgling-un-tried-to-rein-in-mass-scale-misinformation--503ec319c92c.txt | The fledgling UN tried to rein in mass-scale misinformation. The world turned its back and\n",
            "saved [commentisfree]: www.theguardian.com-beware-netanyahu-he-is-a-master-of-self-interest-and-that-s--34b4a57d261d.txt | Beware Netanyahu: he is a master of self-interest – and that’s why he signed the Hamas cea\n",
            "saved [commentisfree]: www.theguardian.com-english-democracy-relies-on-local-councillors-so-why-are-so--7195d3698375.txt | English democracy relies on local councillors. So why are so many facing the axe? | Polly \n",
            "saved [commentisfree]: www.theguardian.com-stephen-miller-is-the-most-dangerous-man-in-the-trump-admini-5885d346054c.txt | Stephen Miller is the most dangerous man in the Trump administration | Judith Levine | The\n",
            "saved [commentisfree]: www.theguardian.com-who-will-run-against-trump-in-2028-please-step-forward-now-d-39f98cb37107.txt | Who will run against Trump in 2028? Please step forward now – don’t wait | David Kirp | Th\n",
            "saved [commentisfree]: www.theguardian.com-a-book-is-being-marketed-with-mayo-scented-ink-jealous-me-da-20f2f93baf3f.txt | A book is being marketed with mayo-scented ink. Jealous? Me? | David Barnett | The Guardia\n",
            "saved [commentisfree]: www.theguardian.com-we-all-know-brexit-s-to-blame-for-the-crisis-facing-uk-steel-e84bdaba3533.txt | We all know Brexit’s to blame for the crisis facing UK steel – it’s time for politicians t\n",
            "saved [commentisfree]: www.theguardian.com-the-guardian-view-on-trump-s-gaza-plan-the-bloodshed-must-en-02426d4ed87f.txt | The Guardian view on Trump’s Gaza plan: the bloodshed must end, but this proposal betrays \n",
            "saved [commentisfree]: www.theguardian.com-the-guardian-view-on-trump-s-argentina-bailout-it-s-a-politi-95762dda5574.txt | The Guardian view on Trump’s Argentina bailout: it’s a political play, not an economic pla\n",
            "saved [commentisfree]: www.theguardian.com-the-guardian-view-on-donald-trump-s-hate-for-opponents-pract-57911aae013d.txt | The Guardian view on Donald Trump’s hate for opponents: practising politics the wrong way \n",
            "saved [commentisfree]: www.theguardian.com-the-guardian-view-on-donald-trump-s-ukraine-strategy-talking-c805a582aef3.txt | The Guardian view on Donald Trump’s Ukraine strategy: talking tough and doing very little \n",
            "\n",
            "Summary\n",
            "{\n",
            "  \"saved_total\": 218,\n",
            "  \"errors_total\": 0,\n",
            "  \"max_articles\": 250,\n",
            "  \"max_per_feed\": null,\n",
            "  \"date_from\": \"2025-07-01\",\n",
            "  \"even_split\": true,\n",
            "  \"remainder_to\": \"commentisfree\",\n",
            "  \"feeds_count\": 42,\n",
            "  \"raw_dir\": \"/content/anti_echo/raw\",\n",
            "  \"index_path\": \"/content/anti_echo/feeds/index.json\",\n",
            "  \"feeds_state_local\": \"/content/anti_echo/feeds/feeds_state.json\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup 5C: Persist feed state to HF and GitHub\n",
        "\n",
        "Purpose\n",
        "- Snapshot `feeds/feeds_state.json` and `feeds/index.json` after each scrape.\n",
        "- Upload to HF as timestamped copies and as `*_latest.json`.\n",
        "- Commit to GitHub under `feeds/` as both timestamped and `*_latest.json`.\n",
        "\n",
        "Requires\n",
        "- `HF_TOKEN` loaded earlier\n",
        "- `GITHUB_TOKEN` present (fine grained or classic, with contents read/write)\n"
      ],
      "metadata": {
        "id": "fieZ0huen5Zg"
      },
      "id": "fieZ0huen5Zg"
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup 5C: Persist feed state to HF dataset and GitHub\n",
        "\n",
        "import os, json, base64\n",
        "from datetime import datetime, timezone\n",
        "from pathlib import Path\n",
        "from getpass import getpass\n",
        "from huggingface_hub import upload_file\n",
        "import requests\n",
        "\n",
        "PROJECT_ROOT = Path(\"/content/anti_echo\").resolve()\n",
        "FEEDS_DIR = PROJECT_ROOT / \"feeds\"\n",
        "STATE_PATH = FEEDS_DIR / \"feeds_state.json\"\n",
        "INDEX_PATH = FEEDS_DIR / \"index.json\"\n",
        "\n",
        "# Inputs\n",
        "HF_DATASET_ID = os.environ.get(\"HF_DATASET_ID\", \"\").strip()\n",
        "if not HF_DATASET_ID:\n",
        "    raise RuntimeError(\"HF_DATASET_ID not set. Run Setup 2 earlier.\")\n",
        "\n",
        "if \"HF_TOKEN\" not in os.environ or not os.environ[\"HF_TOKEN\"].strip():\n",
        "    os.environ[\"HF_TOKEN\"] = getpass(\"Enter your Hugging Face token: \")\n",
        "\n",
        "if \"GITHUB_TOKEN\" not in os.environ or not os.environ[\"GITHUB_TOKEN\"].strip():\n",
        "    os.environ[\"GITHUB_TOKEN\"] = getpass(\"Enter your GitHub token: \")\n",
        "\n",
        "GITHUB_TOKEN = os.environ[\"GITHUB_TOKEN\"].strip()\n",
        "HF_TOKEN = os.environ[\"HF_TOKEN\"].strip()\n",
        "\n",
        "REPO_OWNER = \"AHMerrill\"\n",
        "REPO_NAME = \"anti-echo-chamber\"\n",
        "BRANCH = \"main\"\n",
        "\n",
        "# Validate files exist\n",
        "if not STATE_PATH.exists() or not INDEX_PATH.exists():\n",
        "    raise FileNotFoundError(\"Expected feeds_state.json and index.json to exist. Run 5B first.\")\n",
        "\n",
        "# Timestamped names and latest pointers\n",
        "ts = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
        "hf_files = [\n",
        "    (STATE_PATH, f\"feeds/feeds_state_{ts}.json\"),\n",
        "    (INDEX_PATH, f\"feeds/feed_index_{ts}.json\"),\n",
        "    (STATE_PATH, \"feeds/feeds_state_latest.json\"),\n",
        "    (INDEX_PATH, \"feeds/feed_index_latest.json\"),\n",
        "]\n",
        "\n",
        "print(\"Uploading feed state to Hugging Face dataset...\")\n",
        "for local, remote in hf_files:\n",
        "    upload_file(\n",
        "        path_or_fileobj=str(local),\n",
        "        path_in_repo=remote,\n",
        "        repo_id=HF_DATASET_ID,\n",
        "        repo_type=\"dataset\",\n",
        "        token=HF_TOKEN,\n",
        "    )\n",
        "print(\"HF upload complete.\")\n",
        "\n",
        "# GitHub commit helper\n",
        "def gh_put_file(local_path: Path, repo_path: str, message: str):\n",
        "    url = f\"https://api.github.com/repos/{REPO_OWNER}/{REPO_NAME}/contents/{repo_path}\"\n",
        "    headers = {\"Authorization\": f\"Bearer {GITHUB_TOKEN}\", \"Accept\": \"application/vnd.github+json\"}\n",
        "    content = local_path.read_bytes()\n",
        "    # get existing sha if present\n",
        "    r = requests.get(url, headers=headers, timeout=20)\n",
        "    sha = r.json().get(\"sha\") if r.status_code == 200 else None\n",
        "    payload = {\n",
        "        \"message\": message,\n",
        "        \"content\": base64.b64encode(content).decode(\"utf-8\"),\n",
        "        \"branch\": BRANCH,\n",
        "    }\n",
        "    if sha:\n",
        "        payload[\"sha\"] = sha\n",
        "    resp = requests.put(url, headers=headers, json=payload, timeout=30)\n",
        "    if resp.status_code not in (200, 201):\n",
        "        raise RuntimeError(f\"GitHub push failed for {repo_path}: {resp.status_code} {resp.text[:300]}\")\n",
        "\n",
        "print(\"Committing feed state to GitHub...\")\n",
        "commit_msg = f\"Update feed state and index - {ts}\"\n",
        "# commit both timestamped history copies and latest pointers\n",
        "gh_files = [\n",
        "    (STATE_PATH, f\"feeds/feeds_state_{ts}.json\"),\n",
        "    (INDEX_PATH, f\"feeds/feed_index_{ts}.json\"),\n",
        "    (STATE_PATH, \"feeds/feeds_state_latest.json\"),\n",
        "    (INDEX_PATH, \"feeds/feed_index_latest.json\"),\n",
        "]\n",
        "for local, repo_path in gh_files:\n",
        "    gh_put_file(local, repo_path, commit_msg)\n",
        "\n",
        "print(\"GitHub commit complete.\")\n",
        "\n",
        "# Small echo summary\n",
        "fs = json.loads(STATE_PATH.read_text(encoding=\"utf-8\"))\n",
        "ix = json.loads(INDEX_PATH.read_text(encoding=\"utf-8\"))\n",
        "print({\n",
        "    \"feeds_state_feeds\": len(fs.get(\"feeds\", {})),\n",
        "    \"index_items\": len(ix.get(\"items\", {})),\n",
        "    \"timestamp\": ts,\n",
        "    \"hf_dataset\": HF_DATASET_ID,\n",
        "    \"github_repo\": f\"{REPO_OWNER}/{REPO_NAME}\",\n",
        "})\n"
      ],
      "metadata": {
        "id": "a3bxLgPWn8H2"
      },
      "id": "a3bxLgPWn8H2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup 6A and 6B: Multi-topic and Multi-stance Embeddings\n",
        "\n",
        "This section implements the **core embedding stage** for the Anti Echo Chamber system.\n",
        "\n",
        "All prior setups (1–5B) have prepared:\n",
        "- Local workspace at `/content/anti_echo`\n",
        "- Raw article text files under `/content/anti_echo/raw`\n",
        "- Metadata `.meta.json` sidecars for each article\n",
        "- Config (`CONFIG`) loaded from the repo with model, dim, dtype, and chunk parameters\n",
        "- Chroma collections (`topic_coll`, `stance_coll`) initialized by Setup 4\n",
        "\n",
        "These two cells will:\n",
        "1. **Generate topic embeddings** (`Setup 6A`) — modeling what each article is about.\n",
        "2. **Generate stance embeddings** (`Setup 6B`) — modeling how each article argues.\n",
        "\n",
        "Both results are stored in the active Chroma collections.\n",
        "\n",
        "---\n",
        "\n",
        "### **6A. Multi-topic Embeddings**\n",
        "- **Model:** `sentence-transformers/all-MiniLM-L6-v2`\n",
        "- **Dim:** 384  \n",
        "- **Dtype:** `float16`\n",
        "- **Pooling:** Mean pooled over 512-token chunks\n",
        "- **Segmentation:** Sentence clustering (1–8 topics per article)\n",
        "- **Storage:** `topic_coll` (Chroma collection for topics)\n",
        "- **Output IDs:** `article_id::topic::k`\n",
        "- **Metadata Includes:** article ID, title, url, source, topic index, model info\n",
        "\n",
        "Each article’s text is segmented into multiple topical clusters.\n",
        "Every cluster is chunked and embedded separately to support multi-theme representation.\n",
        "\n",
        "---\n",
        "\n",
        "### **6B. Multi-stance Embeddings**\n",
        "- **Summarizer:** `facebook/bart-large-cnn`\n",
        "- **Embedding Model:** `sentence-transformers/all-MiniLM-L6-v2`\n",
        "- **Dim:** 384  \n",
        "- **Dtype:** `float16`\n",
        "- **Storage:** `stance_coll` (Chroma collection for stances)\n",
        "- **Output IDs:** `article_id::stance::k`\n",
        "- **Metadata Includes:** article ID, title, url, source, stance summary, model info\n",
        "\n",
        "Each article’s full text is summarized to capture its dominant argument or position.\n",
        "The summary is embedded to allow contrasting stance comparisons later.\n",
        "\n",
        "---\n",
        "\n",
        "### **After this step**\n",
        "Running both 6A and 6B will populate your local Chroma database (`/content/anti_echo/chroma_db`)\n",
        "with two complementary vector spaces:\n",
        "\n",
        "| Space | Captures | Purpose |\n",
        "|-------|-----------|----------|\n",
        "| **Topic Space** | Core subject matter of each article | Enables topical retrieval |\n",
        "| **Stance Space** | Underlying tone or argument | Enables stance contrast search |\n",
        "\n",
        "This brings the pipeline up to the point where retrieval and RAG-style contrastive queries\n",
        "can be implemented in later stages (Setup 7+).\n"
      ],
      "metadata": {
        "id": "D4Q0veK8W9xC"
      },
      "id": "D4Q0veK8W9xC"
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup 6A: Multi-topic embeddings (final robust version)\n",
        "# Handles NLTK punkt_tab, long text truncation, metadata sanitization, and single-sentence cases\n",
        "\n",
        "import json, time\n",
        "import numpy as np\n",
        "import torch\n",
        "import nltk\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from pathlib import Path\n",
        "\n",
        "# --- Ensure required NLTK data (punkt_tab fix) ---\n",
        "for pkg in [\"punkt\", \"punkt_tab\"]:\n",
        "    try:\n",
        "        nltk.data.find(f\"tokenizers/{pkg}\")\n",
        "    except LookupError:\n",
        "        nltk.download(pkg)\n",
        "\n",
        "# --- Environment and Config ---\n",
        "RAW_DIR = Path(\"/content/anti_echo/raw\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "topic_model_name = CONFIG[\"embeddings\"][\"topic_model\"]\n",
        "topic_dim = int(CONFIG[\"embeddings\"][\"dim\"])\n",
        "topic_dtype = CONFIG[\"embeddings\"][\"dtype\"]\n",
        "chunk_tokens = int(CONFIG[\"embeddings\"][\"chunk_tokens\"])\n",
        "coll_topic = CONFIG[\"chroma_collections\"][\"topic\"]\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(topic_model_name, use_fast=True)\n",
        "embedder = SentenceTransformer(topic_model_name, device=device)\n",
        "\n",
        "print(f\"Embedding model: {topic_model_name}, dim={topic_dim}, dtype={topic_dtype}, device={device}\")\n",
        "\n",
        "# --- Helper functions ---\n",
        "\n",
        "def sent_split(text):\n",
        "    return [s.strip() for s in nltk.sent_tokenize(text) if s.strip()]\n",
        "\n",
        "def encode_batch(texts, batch=16):\n",
        "    return embedder.encode(\n",
        "        texts,\n",
        "        convert_to_numpy=True,\n",
        "        batch_size=batch,\n",
        "        show_progress_bar=False\n",
        "    )\n",
        "\n",
        "def chunk_by_tokens(text, max_tokens=512, overlap=64):\n",
        "    ids = tokenizer(text, add_special_tokens=False, return_attention_mask=False)[\"input_ids\"]\n",
        "    step = max_tokens - overlap\n",
        "    chunks = []\n",
        "    for i in range(0, len(ids), step):\n",
        "        j = min(i + max_tokens, len(ids))\n",
        "        piece = tokenizer.decode(ids[i:j], skip_special_tokens=True)\n",
        "        if piece.strip():\n",
        "            chunks.append(piece)\n",
        "    return chunks\n",
        "\n",
        "def sanitize_metadata(meta: dict) -> dict:\n",
        "    clean = {}\n",
        "    for k, v in meta.items():\n",
        "        if isinstance(v, (str, int, float, bool)):\n",
        "            clean[k] = v\n",
        "        elif v is None:\n",
        "            clean[k] = \"\"\n",
        "        else:\n",
        "            try:\n",
        "                clean[k] = str(v)\n",
        "            except Exception:\n",
        "                clean[k] = \"\"\n",
        "    return clean\n",
        "\n",
        "def topic_vectors_for_article(text):\n",
        "    sents = sent_split(text)\n",
        "    if not sents:\n",
        "        return []\n",
        "    # --- handle single-sentence case ---\n",
        "    if len(sents) < 2:\n",
        "        v = encode_batch([\" \".join(sents)])[0]\n",
        "        return [v.astype(np.float16) if topic_dtype == \"float16\" else v.astype(np.float32)]\n",
        "\n",
        "    emb = encode_batch(sents)\n",
        "    k = min(max(1, len(sents)//8), 8)\n",
        "    labels = AgglomerativeClustering(n_clusters=k).fit_predict(emb)\n",
        "    segs = []\n",
        "    for lab in sorted(set(labels)):\n",
        "        segs.append(\" \".join([s for s, l in zip(sents, labels) if l == lab]))\n",
        "    vecs = []\n",
        "    for seg in segs:\n",
        "        # Truncate overly long segments to 512 tokens to avoid model warning\n",
        "        ids = tokenizer(seg, add_special_tokens=False)[\"input_ids\"][:512]\n",
        "        seg_trunc = tokenizer.decode(ids, skip_special_tokens=True)\n",
        "        chunks = chunk_by_tokens(seg_trunc, chunk_tokens, 64)\n",
        "        if not chunks:\n",
        "            continue\n",
        "        chunk_embs = encode_batch(chunks)\n",
        "        pooled = chunk_embs.mean(axis=0)\n",
        "        vecs.append(\n",
        "            pooled.astype(np.float16) if topic_dtype == \"float16\" else pooled.astype(np.float32)\n",
        "        )\n",
        "    return vecs\n",
        "\n",
        "# --- Execution ---\n",
        "start = time.time()\n",
        "count = 0\n",
        "\n",
        "for txt_path in RAW_DIR.glob(\"*.txt\"):\n",
        "    meta_path = txt_path.with_suffix(\".meta.json\")\n",
        "    if not meta_path.exists():\n",
        "        continue\n",
        "    meta = json.loads(meta_path.read_text(encoding=\"utf-8\"))\n",
        "    text = txt_path.read_text(encoding=\"utf-8\").strip()\n",
        "    if not text:\n",
        "        continue\n",
        "\n",
        "    vecs = topic_vectors_for_article(text)\n",
        "    if not vecs:\n",
        "        continue\n",
        "\n",
        "    ids = [f\"{meta['id']}::topic::{i}\" for i in range(len(vecs))]\n",
        "    metas = [sanitize_metadata({**meta, \"topic_index\": i, \"topic_model\": topic_model_name}) for i in range(len(vecs))]\n",
        "    upsert_in_chunks(topic_coll, ids, np.vstack(vecs), metas)\n",
        "    count += len(vecs)\n",
        "\n",
        "print(f\"Upserted {count} topic embeddings to collection {coll_topic} in {round(time.time()-start,2)}s\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l14DYviQcjSv",
        "outputId": "c3c87333-4bc9-4bb1-a2df-db1872d67a32"
      },
      "id": "l14DYviQcjSv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding model: sentence-transformers/all-MiniLM-L6-v2, dim=384, dtype=float16, device=cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2486 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upserted 871 topic embeddings to collection news_topic in 19.87s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup 6B: Multi-stance embeddings (final robust version)\n",
        "# Handles metadata sanitization, summarization length safety, and errors\n",
        "\n",
        "import json, time\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pathlib import Path\n",
        "\n",
        "RAW_DIR = Path(\"/content/anti_echo/raw\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "stance_model_name = CONFIG[\"embeddings\"][\"stance_model\"]\n",
        "stance_dim = int(CONFIG[\"embeddings\"][\"dim\"])\n",
        "stance_dtype = CONFIG[\"embeddings\"][\"dtype\"]\n",
        "# Choose summarizer dynamically based on environment\n",
        "if torch.cuda.is_available():\n",
        "    summarizer_name = CONFIG[\"summarizer\"][\"model\"]  # usually facebook/bart-large-cnn\n",
        "    print(\"GPU detected — using full summarizer:\", summarizer_name)\n",
        "else:\n",
        "    summarizer_name = \"sshleifer/distilbart-cnn-12-6\"\n",
        "    print(\"No GPU detected — using smaller summarizer for CPU mode:\", summarizer_name)\n",
        "coll_stance = CONFIG[\"chroma_collections\"][\"stance\"]\n",
        "\n",
        "tok_sum = AutoTokenizer.from_pretrained(summarizer_name)\n",
        "model_sum = AutoModelForSeq2SeqLM.from_pretrained(summarizer_name).to(device)\n",
        "embedder = SentenceTransformer(stance_model_name, device=device)\n",
        "\n",
        "print(f\"Summarizer: {summarizer_name}\\nEmbedder: {stance_model_name}, dim={stance_dim}, dtype={stance_dtype}, device={device}\")\n",
        "\n",
        "# --- Helper functions ---\n",
        "\n",
        "def sanitize_metadata(meta: dict) -> dict:\n",
        "    clean = {}\n",
        "    for k, v in meta.items():\n",
        "        if isinstance(v, (str, int, float, bool)):\n",
        "            clean[k] = v\n",
        "        elif v is None:\n",
        "            clean[k] = \"\"\n",
        "        else:\n",
        "            try:\n",
        "                clean[k] = str(v)\n",
        "            except Exception:\n",
        "                clean[k] = \"\"\n",
        "    return clean\n",
        "\n",
        "def summarize_text(text: str, max_in=1024, max_out=150) -> str:\n",
        "    \"\"\"Summarize text safely, truncating long input.\"\"\"\n",
        "    inputs = tok_sum(\n",
        "        [text],\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=max_in,\n",
        "    ).to(device)\n",
        "    with torch.no_grad():\n",
        "        out = model_sum.generate(\n",
        "            **inputs,\n",
        "            max_length=max_out,\n",
        "            num_beams=4,\n",
        "            early_stopping=True,\n",
        "        )\n",
        "    return tok_sum.batch_decode(out, skip_special_tokens=True)[0].strip()\n",
        "\n",
        "# --- Execution ---\n",
        "\n",
        "start = time.time()\n",
        "count = 0\n",
        "skipped = 0\n",
        "\n",
        "for txt_path in RAW_DIR.glob(\"*.txt\"):\n",
        "    meta_path = txt_path.with_suffix(\".meta.json\")\n",
        "    if not meta_path.exists():\n",
        "        continue\n",
        "    meta = json.loads(meta_path.read_text(encoding=\"utf-8\"))\n",
        "    text = txt_path.read_text(encoding=\"utf-8\").strip()\n",
        "    if not text:\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        summary = summarize_text(text)\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: summarization failed for {meta.get('id')}: {e}\")\n",
        "        skipped += 1\n",
        "        continue\n",
        "\n",
        "    if not summary:\n",
        "        skipped += 1\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        vec = embedder.encode([summary], convert_to_numpy=True, normalize_embeddings=False)[0]\n",
        "        vec = vec.astype(np.float16) if stance_dtype == \"float16\" else vec.astype(np.float32)\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: embedding failed for {meta.get('id')}: {e}\")\n",
        "        skipped += 1\n",
        "        continue\n",
        "\n",
        "    ids = [f\"{meta['id']}::stance::0\"]\n",
        "    metas = [\n",
        "        sanitize_metadata({\n",
        "            **meta,\n",
        "            \"stance_summary\": summary,\n",
        "            \"stance_model\": stance_model_name,\n",
        "            \"summary_model\": summarizer_name,\n",
        "        })\n",
        "    ]\n",
        "    upsert_in_chunks(stance_coll, ids, np.vstack([vec]), metas)\n",
        "    count += 1\n",
        "\n",
        "elapsed = round(time.time() - start, 2)\n",
        "print(f\"Upserted {count} stance embeddings, skipped {skipped}, to collection {coll_stance} in {elapsed}s\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2B7ckwnajcd7",
        "outputId": "18cfe87d-9a02-4bfe-a7a9-6736c5f7c5d8"
      },
      "id": "2B7ckwnajcd7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summarizer: facebook/bart-large-cnn\n",
            "Embedder: sentence-transformers/all-MiniLM-L6-v2, dim=384, dtype=float16, device=cuda\n",
            "Upserted 218 stance embeddings, skipped 0, to collection news_stance in 207.16s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setup 7: Batch Packaging and Checkpoint Push**\n",
        "\n",
        "Now that all topic and stance embeddings are complete,  \n",
        "we package the results into versioned batch artifacts and push them to the cloud.\n",
        "\n",
        "**Steps:**\n",
        "1. Collect embeddings and metadata from local ChromaDB.  \n",
        "2. Save artifacts to `/content/anti_echo/batches/{batch_id}/`:\n",
        "   - `topic_embeddings.npz`\n",
        "   - `stance_embeddings.npz`\n",
        "   - `metadata.jsonl`\n",
        "   - `manifest.json`\n",
        "3. Upload all artifacts to the Hugging Face dataset (`zanimal/anti-echo-artifacts`).  \n",
        "4. Update and commit `artifacts_registry.json` on GitHub (Setup 7B).\n",
        "\n",
        "This ensures Chroma can be fully rebuilt later from Hugging Face data,  \n",
        "and the project registry remains synchronized between GitHub and HF.\n"
      ],
      "metadata": {
        "id": "dWdvJ_SwnCvW"
      },
      "id": "dWdvJ_SwnCvW"
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup 7A: Package embeddings + push to Hugging Face\n",
        "# Fixed for Chroma >=0.5.5 (no 'ids' in include list)\n",
        "# Silences Chroma telemetry warnings\n",
        "\n",
        "import os, json, time, uuid, warnings, logging\n",
        "from datetime import datetime, timezone\n",
        "import numpy as np\n",
        "from huggingface_hub import HfApi, upload_file\n",
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "# --- Silence Chroma telemetry noise ---\n",
        "logging.getLogger(\"chromadb\").setLevel(logging.ERROR)\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "PROJECT_ROOT = Path(\"/content/anti_echo\").resolve()\n",
        "BATCH_DIR = PROJECT_ROOT / CONFIG[\"batch\"][\"base_dir\"]\n",
        "HF_DATASET_ID = CONFIG[\"hf_dataset_id\"]\n",
        "REGISTRY_URL = f\"https://raw.githubusercontent.com/{REPO_OWNER}/{REPO_NAME}/{BRANCH}/artifacts/artifacts_registry.json\"\n",
        "\n",
        "client, topic_coll, stance_coll = ensure_chroma()\n",
        "\n",
        "timestamp = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
        "batch_id = f\"batch_{timestamp}_{uuid.uuid4().hex[:8]}\"\n",
        "batch_path = BATCH_DIR / batch_id\n",
        "batch_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Packaging new batch: {batch_id}\")\n",
        "\n",
        "# --- Export from Chroma ---\n",
        "topic_data = topic_coll.get(include=[\"embeddings\", \"metadatas\"])\n",
        "stance_data = stance_coll.get(include=[\"embeddings\", \"metadatas\"])\n",
        "\n",
        "topic_vecs = np.array(topic_data[\"embeddings\"], dtype=np.float16)\n",
        "stance_vecs = np.array(stance_data[\"embeddings\"], dtype=np.float16)\n",
        "meta_records = topic_data[\"metadatas\"]\n",
        "\n",
        "meta_path = batch_path / CONFIG[\"batch\"][\"metadata_file\"]\n",
        "topic_npz = batch_path / CONFIG[\"batch\"][\"topic_file\"]\n",
        "stance_npz = batch_path / CONFIG[\"batch\"][\"stance_file\"]\n",
        "manifest_path = batch_path / CONFIG[\"batch\"][\"manifest_name\"]\n",
        "\n",
        "# --- Write local files ---\n",
        "print(\"Saving batch artifacts...\")\n",
        "np.savez_compressed(topic_npz, topic_vecs)\n",
        "np.savez_compressed(stance_npz, stance_vecs)\n",
        "\n",
        "with meta_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "    for m in meta_records:\n",
        "        json.dump(m, f)\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "manifest = {\n",
        "    \"batch_id\": batch_id,\n",
        "    \"created_at\": timestamp,\n",
        "    \"models\": CONFIG[\"embeddings\"],\n",
        "    \"counts\": {\n",
        "        \"topic\": len(topic_vecs),\n",
        "        \"stance\": len(stance_vecs)\n",
        "    },\n",
        "    \"hf_dataset_id\": HF_DATASET_ID,\n",
        "    \"paths\": {\n",
        "        \"embeddings_topic\": f\"batches/{batch_id}/{topic_npz.name}\",\n",
        "        \"embeddings_stance\": f\"batches/{batch_id}/{stance_npz.name}\",\n",
        "        \"metadata\": f\"batches/{batch_id}/{meta_path.name}\",\n",
        "        \"manifest\": f\"batches/{batch_id}/{manifest_path.name}\"\n",
        "    }\n",
        "}\n",
        "manifest_path.write_text(json.dumps(manifest, indent=2), encoding=\"utf-8\")\n",
        "print(json.dumps(manifest, indent=2))\n",
        "\n",
        "# --- Upload to Hugging Face ---\n",
        "api = HfApi()\n",
        "print(f\"Uploading batch {batch_id} to Hugging Face dataset: {HF_DATASET_ID}\")\n",
        "\n",
        "for fpath in [topic_npz, stance_npz, meta_path, manifest_path]:\n",
        "    rel = f\"batches/{batch_id}/{fpath.name}\"\n",
        "    upload_file(\n",
        "        path_or_fileobj=str(fpath),\n",
        "        path_in_repo=rel,\n",
        "        repo_id=HF_DATASET_ID,\n",
        "        repo_type=\"dataset\",\n",
        "        token=os.environ[\"HF_TOKEN\"]\n",
        "    )\n",
        "print(\"✅ Upload complete.\")\n",
        "\n",
        "# --- Update GitHub registry (local only for now) ---\n",
        "try:\n",
        "    reg_resp = requests.get(REGISTRY_URL, timeout=20)\n",
        "    registry = reg_resp.json()\n",
        "except Exception:\n",
        "    registry = {\"version\": 1, \"models\": {}, \"batches\": []}\n",
        "\n",
        "registry.setdefault(\"batches\", []).append(manifest)\n",
        "new_registry_path = PROJECT_ROOT / \"artifacts_registry_updated.json\"\n",
        "new_registry_path.write_text(json.dumps(registry, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "print(f\"Registry updated locally: {new_registry_path}\")\n",
        "print(\"Ready for Setup 7B (auto GitHub push).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659,
          "referenced_widgets": [
            "95d915bbf06c4a4f9bf843926421c9da",
            "47a3b02ab542471f86e52e9bbf0b728b",
            "4d1509c4e225423f89831e3044513d1f",
            "a2ea9d7c7b3141a384db1f8628902f7b",
            "6252a2945d164e269755c13267a30ec4",
            "3f261394cea7465183fee018ce8423d2",
            "2a3eff212bc24900a97ad98d6619c16e",
            "0981aac2f5734bef9368c02b88230191",
            "21c6b68098b140c9918037a9c8604c2f",
            "f9da71d4008d4963b1c05eeef3f5a25b",
            "d524f84626334ed79df87715de8ae97e"
          ]
        },
        "id": "z9rMDj-ZmpQh",
        "outputId": "9ff326cf-b6ea-402f-9042-29de8bb18008"
      },
      "id": "z9rMDj-ZmpQh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event CollectionGetEvent: capture() takes 1 positional argument but 3 were given\n",
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event CollectionGetEvent: capture() takes 1 positional argument but 3 were given\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Packaging new batch: batch_20251011T232938Z_283ca40f\n",
            "Saving batch artifacts...\n",
            "{\n",
            "  \"batch_id\": \"batch_20251011T232938Z_283ca40f\",\n",
            "  \"created_at\": \"20251011T232938Z\",\n",
            "  \"models\": {\n",
            "    \"topic_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
            "    \"stance_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
            "    \"dim\": 384,\n",
            "    \"dtype\": \"float16\",\n",
            "    \"pooling\": \"mean\",\n",
            "    \"chunk_tokens\": 512\n",
            "  },\n",
            "  \"counts\": {\n",
            "    \"topic\": 0,\n",
            "    \"stance\": 0\n",
            "  },\n",
            "  \"hf_dataset_id\": \"zanimal/anti-echo-artifacts\",\n",
            "  \"paths\": {\n",
            "    \"embeddings_topic\": \"batches/batch_20251011T232938Z_283ca40f/embeddings_topic.npz\",\n",
            "    \"embeddings_stance\": \"batches/batch_20251011T232938Z_283ca40f/embeddings_stance.npz\",\n",
            "    \"metadata\": \"batches/batch_20251011T232938Z_283ca40f/metadata.jsonl\",\n",
            "    \"manifest\": \"batches/batch_20251011T232938Z_283ca40f/manifest.json\"\n",
            "  }\n",
            "}\n",
            "Uploading batch batch_20251011T232938Z_283ca40f to Hugging Face dataset: zanimal/anti-echo-artifacts\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "embeddings_topic.npz:   0%|          | 0.00/204 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "95d915bbf06c4a4f9bf843926421c9da"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Upload complete.\n",
            "Registry updated locally: /content/anti_echo/artifacts_registry_updated.json\n",
            "Ready for Setup 7B (auto GitHub push).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup 7B: Auto-push updated registry JSON to GitHub\n",
        "# Works for fine-grained or classic tokens\n",
        "# Requires: artifacts_registry_updated.json from Setup 7A\n",
        "\n",
        "import base64\n",
        "import json\n",
        "import os\n",
        "import requests\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from getpass import getpass\n",
        "\n",
        "# --- Basic config ---\n",
        "REPO_OWNER = \"AHMerrill\"\n",
        "REPO_NAME = \"anti-echo-chamber\"\n",
        "BRANCH = \"main\"\n",
        "FILE_PATH = \"artifacts/artifacts_registry.json\"\n",
        "LOCAL_REGISTRY = Path(\"/content/anti_echo/artifacts_registry_updated.json\")\n",
        "\n",
        "# --- Get token securely ---\n",
        "if \"GITHUB_TOKEN\" not in os.environ or not os.environ[\"GITHUB_TOKEN\"].strip():\n",
        "    os.environ[\"GITHUB_TOKEN\"] = getpass(\"Enter your GitHub Personal Access Token: \")\n",
        "\n",
        "TOKEN = os.environ[\"GITHUB_TOKEN\"].strip()\n",
        "if not TOKEN:\n",
        "    raise RuntimeError(\"GitHub token not provided. Please rerun and paste it when prompted.\")\n",
        "\n",
        "print(f\"Pushing updated registry to {REPO_OWNER}/{REPO_NAME}:{BRANCH}/{FILE_PATH}\")\n",
        "\n",
        "# --- Load the local registry ---\n",
        "if not LOCAL_REGISTRY.exists():\n",
        "    raise FileNotFoundError(f\"Local registry not found: {LOCAL_REGISTRY}\")\n",
        "\n",
        "registry = json.loads(LOCAL_REGISTRY.read_text(encoding=\"utf-8\"))\n",
        "\n",
        "# --- Version bump ---\n",
        "if \"version\" not in registry or not isinstance(registry[\"version\"], int):\n",
        "    registry[\"version\"] = 1\n",
        "else:\n",
        "    registry[\"version\"] += 1\n",
        "\n",
        "# --- Prepare updated JSON content ---\n",
        "new_content = json.dumps(registry, indent=2)\n",
        "message = f\"Update artifacts registry v{registry['version']} - {datetime.utcnow().isoformat()}\"\n",
        "\n",
        "# --- Get current file SHA (for GitHub API) ---\n",
        "url = f\"https://api.github.com/repos/{REPO_OWNER}/{REPO_NAME}/contents/{FILE_PATH}\"\n",
        "headers = {\"Authorization\": f\"Bearer {TOKEN}\", \"Accept\": \"application/vnd.github+json\"}\n",
        "r = requests.get(url, headers=headers, timeout=20)\n",
        "\n",
        "if r.status_code == 200:\n",
        "    sha = r.json()[\"sha\"]\n",
        "    print(f\"Existing registry found (SHA={sha[:8]}...), updating.\")\n",
        "else:\n",
        "    sha = None\n",
        "    print(\"No existing registry found; creating new file.\")\n",
        "\n",
        "# --- Commit payload ---\n",
        "payload = {\n",
        "    \"message\": message,\n",
        "    \"content\": base64.b64encode(new_content.encode(\"utf-8\")).decode(\"utf-8\"),\n",
        "    \"branch\": BRANCH,\n",
        "}\n",
        "if sha:\n",
        "    payload[\"sha\"] = sha\n",
        "\n",
        "# --- Push to GitHub ---\n",
        "resp = requests.put(url, headers=headers, json=payload, timeout=30)\n",
        "\n",
        "if resp.status_code in (200, 201):\n",
        "    commit_url = resp.json().get(\"commit\", {}).get(\"html_url\")\n",
        "    print(f\"✅ Successfully pushed updated registry to GitHub.\")\n",
        "    print(f\"Commit URL: {commit_url}\")\n",
        "else:\n",
        "    print(f\"❌ GitHub push failed ({resp.status_code}).\")\n",
        "    print(resp.text[:500])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiTmVNxkoySg",
        "outputId": "c7d094fe-630a-45dd-d584-078ab77b226b"
      },
      "id": "AiTmVNxkoySg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your GitHub Personal Access Token: ··········\n",
            "Pushing updated registry to AHMerrill/anti-echo-chamber:main/artifacts/artifacts_registry.json\n",
            "Existing registry found (SHA=5dc1537a...), updating.\n",
            "✅ Successfully pushed updated registry to GitHub.\n",
            "Commit URL: https://github.com/AHMerrill/anti-echo-chamber/commit/9f5d1ee5c245e67f77947058fad9ecfe3d0cc517\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "95d915bbf06c4a4f9bf843926421c9da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_47a3b02ab542471f86e52e9bbf0b728b",
              "IPY_MODEL_4d1509c4e225423f89831e3044513d1f",
              "IPY_MODEL_a2ea9d7c7b3141a384db1f8628902f7b"
            ],
            "layout": "IPY_MODEL_6252a2945d164e269755c13267a30ec4"
          }
        },
        "47a3b02ab542471f86e52e9bbf0b728b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f261394cea7465183fee018ce8423d2",
            "placeholder": "​",
            "style": "IPY_MODEL_2a3eff212bc24900a97ad98d6619c16e",
            "value": "embeddings_topic.npz: 100%"
          }
        },
        "4d1509c4e225423f89831e3044513d1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0981aac2f5734bef9368c02b88230191",
            "max": 204,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_21c6b68098b140c9918037a9c8604c2f",
            "value": 204
          }
        },
        "a2ea9d7c7b3141a384db1f8628902f7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9da71d4008d4963b1c05eeef3f5a25b",
            "placeholder": "​",
            "style": "IPY_MODEL_d524f84626334ed79df87715de8ae97e",
            "value": " 204/204 [00:00&lt;00:00, 827B/s]"
          }
        },
        "6252a2945d164e269755c13267a30ec4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f261394cea7465183fee018ce8423d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a3eff212bc24900a97ad98d6619c16e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0981aac2f5734bef9368c02b88230191": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21c6b68098b140c9918037a9c8604c2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f9da71d4008d4963b1c05eeef3f5a25b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d524f84626334ed79df87715de8ae97e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}